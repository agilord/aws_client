// ignore_for_file: deprecated_member_use_from_same_package
// ignore_for_file: unintended_html_in_doc_comment
// ignore_for_file: unused_element
// ignore_for_file: unused_field
// ignore_for_file: unused_import
// ignore_for_file: unused_local_variable
// ignore_for_file: unused_shown_name

import 'dart:convert';
import 'dart:typed_data';

import '../../shared/shared.dart' as _s;
import '../../shared/shared.dart'
    show
        rfc822ToJson,
        iso8601ToJson,
        unixTimestampToJson,
        nonNullableTimeStampFromJson,
        timeStampFromJson;

export '../../shared/shared.dart' show AwsClientCredentials;

/// AWS Elemental MediaConvert
class MediaConvert {
  final _s.RestJsonProtocol _protocol;
  MediaConvert({
    required String region,
    _s.AwsClientCredentials? credentials,
    _s.AwsClientCredentialsProvider? credentialsProvider,
    _s.Client? client,
    String? endpointUrl,
  }) : _protocol = _s.RestJsonProtocol(
          client: client,
          service: _s.ServiceMetadata(
            endpointPrefix: 'mediaconvert',
            signingName: 'mediaconvert',
          ),
          region: region,
          credentials: credentials,
          credentialsProvider: credentialsProvider,
          endpointUrl: endpointUrl,
        );

  /// Closes the internal HTTP client if none was provided at creation.
  /// If a client was passed as a constructor argument, this becomes a noop.
  ///
  /// It's important to close all clients when it's done being used; failing to
  /// do so can cause the Dart process to hang.
  void close() {
    _protocol.close();
  }

  /// Associates an AWS Certificate Manager (ACM) Amazon Resource Name (ARN)
  /// with AWS Elemental MediaConvert.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [arn] :
  /// The ARN of the ACM certificate that you want to associate with your
  /// MediaConvert resource.
  Future<void> associateCertificate({
    required String arn,
  }) async {
    final $payload = <String, dynamic>{
      'arn': arn,
    };
    final response = await _protocol.send(
      payload: $payload,
      method: 'POST',
      requestUri: '/2017-08-29/certificates',
      exceptionFnMap: _exceptionFns,
    );
  }

  /// Permanently cancel a job. Once you have canceled a job, you can't start it
  /// again.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [id] :
  /// The Job ID of the job to be cancelled.
  Future<void> cancelJob({
    required String id,
  }) async {
    final response = await _protocol.send(
      payload: null,
      method: 'DELETE',
      requestUri: '/2017-08-29/jobs/${Uri.encodeComponent(id)}',
      exceptionFnMap: _exceptionFns,
    );
  }

  /// Create a new transcoding job. For information about jobs and job settings,
  /// see the User Guide at
  /// http://docs.aws.amazon.com/mediaconvert/latest/ug/what-is.html
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [role] :
  /// Required. The IAM role you use for creating this job. For details about
  /// permissions, see the User Guide topic at the User Guide at
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/iam-role.html.
  ///
  /// Parameter [settings] :
  /// JobSettings contains all the transcode settings for a job.
  ///
  /// Parameter [accelerationSettings] :
  /// Optional. Accelerated transcoding can significantly speed up jobs with
  /// long, visually complex content. Outputs that use this feature incur
  /// pro-tier pricing. For information about feature limitations, see the AWS
  /// Elemental MediaConvert User Guide.
  ///
  /// Parameter [billingTagsSource] :
  /// Optional. Choose a tag type that AWS Billing and Cost Management will use
  /// to sort your AWS Elemental MediaConvert costs on any billing report that
  /// you set up. Any transcoding outputs that don't have an associated tag will
  /// appear in your billing report unsorted. If you don't choose a valid value
  /// for this field, your job outputs will appear on the billing report
  /// unsorted.
  ///
  /// Parameter [clientRequestToken] :
  /// Prevent duplicate jobs from being created and ensure idempotency for your
  /// requests. A client request token can be any string that includes up to 64
  /// ASCII characters. If you reuse a client request token within one minute of
  /// a successful request, the API returns the job details of the original
  /// request instead. For more information see
  /// https://docs.aws.amazon.com/mediaconvert/latest/apireference/idempotency.html.
  ///
  /// Parameter [hopDestinations] :
  /// Optional. Use queue hopping to avoid overly long waits in the backlog of
  /// the queue that you submit your job to. Specify an alternate queue and the
  /// maximum time that your job will wait in the initial queue before hopping.
  /// For more information about this feature, see the AWS Elemental
  /// MediaConvert User Guide.
  ///
  /// Parameter [jobTemplate] :
  /// Optional. When you create a job, you can either specify a job template or
  /// specify the transcoding settings individually.
  ///
  /// Parameter [priority] :
  /// Optional. Specify the relative priority for this job. In any given queue,
  /// the service begins processing the job with the highest value first. When
  /// more than one job has the same priority, the service begins processing the
  /// job that you submitted first. If you don't specify a priority, the service
  /// uses the default value 0.
  ///
  /// Parameter [queue] :
  /// Optional. When you create a job, you can specify a queue to send it to. If
  /// you don't specify, the job will go to the default queue. For more about
  /// queues, see the User Guide topic at
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/what-is.html.
  ///
  /// Parameter [simulateReservedQueue] :
  /// Optional. Enable this setting when you run a test job to estimate how many
  /// reserved transcoding slots (RTS) you need. When this is enabled,
  /// MediaConvert runs your job from an on-demand queue with similar
  /// performance to what you will see with one RTS in a reserved queue. This
  /// setting is disabled by default.
  ///
  /// Parameter [statusUpdateInterval] :
  /// Optional. Specify how often MediaConvert sends STATUS_UPDATE events to
  /// Amazon CloudWatch Events. Set the interval, in seconds, between status
  /// updates. MediaConvert sends an update at this interval from the time the
  /// service begins processing your job to the time it completes the transcode
  /// or encounters an error.
  ///
  /// Parameter [tags] :
  /// Optional. The tags that you want to add to the resource. You can tag
  /// resources with a key-value pair or with only a key.  Use standard AWS tags
  /// on your job for automatic integration with AWS services and for custom
  /// integrations and workflows.
  ///
  /// Parameter [userMetadata] :
  /// Optional. User-defined metadata that you want to associate with an
  /// MediaConvert job. You specify metadata in key/value pairs.  Use only for
  /// existing integrations or workflows that rely on job metadata tags.
  /// Otherwise, we recommend that you use standard AWS tags.
  Future<CreateJobResponse> createJob({
    required String role,
    required JobSettings settings,
    AccelerationSettings? accelerationSettings,
    BillingTagsSource? billingTagsSource,
    String? clientRequestToken,
    List<HopDestination>? hopDestinations,
    String? jobTemplate,
    int? priority,
    String? queue,
    SimulateReservedQueue? simulateReservedQueue,
    StatusUpdateInterval? statusUpdateInterval,
    Map<String, String>? tags,
    Map<String, String>? userMetadata,
  }) async {
    _s.validateNumRange(
      'priority',
      priority,
      -50,
      50,
    );
    final $payload = <String, dynamic>{
      'role': role,
      'settings': settings,
      if (accelerationSettings != null)
        'accelerationSettings': accelerationSettings,
      if (billingTagsSource != null)
        'billingTagsSource': billingTagsSource.value,
      'clientRequestToken': clientRequestToken ?? _s.generateIdempotencyToken(),
      if (hopDestinations != null) 'hopDestinations': hopDestinations,
      if (jobTemplate != null) 'jobTemplate': jobTemplate,
      if (priority != null) 'priority': priority,
      if (queue != null) 'queue': queue,
      if (simulateReservedQueue != null)
        'simulateReservedQueue': simulateReservedQueue.value,
      if (statusUpdateInterval != null)
        'statusUpdateInterval': statusUpdateInterval.value,
      if (tags != null) 'tags': tags,
      if (userMetadata != null) 'userMetadata': userMetadata,
    };
    final response = await _protocol.send(
      payload: $payload,
      method: 'POST',
      requestUri: '/2017-08-29/jobs',
      exceptionFnMap: _exceptionFns,
    );
    return CreateJobResponse.fromJson(response);
  }

  /// Create a new job template. For information about job templates see the
  /// User Guide at
  /// http://docs.aws.amazon.com/mediaconvert/latest/ug/what-is.html
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [name] :
  /// The name of the job template you are creating.
  ///
  /// Parameter [settings] :
  /// JobTemplateSettings contains all the transcode settings saved in the
  /// template that will be applied to jobs created from it.
  ///
  /// Parameter [accelerationSettings] :
  /// Accelerated transcoding can significantly speed up jobs with long,
  /// visually complex content. Outputs that use this feature incur pro-tier
  /// pricing. For information about feature limitations, see the AWS Elemental
  /// MediaConvert User Guide.
  ///
  /// Parameter [category] :
  /// Optional. A category for the job template you are creating
  ///
  /// Parameter [description] :
  /// Optional. A description of the job template you are creating.
  ///
  /// Parameter [hopDestinations] :
  /// Optional. Use queue hopping to avoid overly long waits in the backlog of
  /// the queue that you submit your job to. Specify an alternate queue and the
  /// maximum time that your job will wait in the initial queue before hopping.
  /// For more information about this feature, see the AWS Elemental
  /// MediaConvert User Guide.
  ///
  /// Parameter [priority] :
  /// Specify the relative priority for this job. In any given queue, the
  /// service begins processing the job with the highest value first. When more
  /// than one job has the same priority, the service begins processing the job
  /// that you submitted first. If you don't specify a priority, the service
  /// uses the default value 0.
  ///
  /// Parameter [queue] :
  /// Optional. The queue that jobs created from this template are assigned to.
  /// If you don't specify this, jobs will go to the default queue.
  ///
  /// Parameter [statusUpdateInterval] :
  /// Specify how often MediaConvert sends STATUS_UPDATE events to Amazon
  /// CloudWatch Events. Set the interval, in seconds, between status updates.
  /// MediaConvert sends an update at this interval from the time the service
  /// begins processing your job to the time it completes the transcode or
  /// encounters an error.
  ///
  /// Parameter [tags] :
  /// The tags that you want to add to the resource. You can tag resources with
  /// a key-value pair or with only a key.
  Future<CreateJobTemplateResponse> createJobTemplate({
    required String name,
    required JobTemplateSettings settings,
    AccelerationSettings? accelerationSettings,
    String? category,
    String? description,
    List<HopDestination>? hopDestinations,
    int? priority,
    String? queue,
    StatusUpdateInterval? statusUpdateInterval,
    Map<String, String>? tags,
  }) async {
    _s.validateNumRange(
      'priority',
      priority,
      -50,
      50,
    );
    final $payload = <String, dynamic>{
      'name': name,
      'settings': settings,
      if (accelerationSettings != null)
        'accelerationSettings': accelerationSettings,
      if (category != null) 'category': category,
      if (description != null) 'description': description,
      if (hopDestinations != null) 'hopDestinations': hopDestinations,
      if (priority != null) 'priority': priority,
      if (queue != null) 'queue': queue,
      if (statusUpdateInterval != null)
        'statusUpdateInterval': statusUpdateInterval.value,
      if (tags != null) 'tags': tags,
    };
    final response = await _protocol.send(
      payload: $payload,
      method: 'POST',
      requestUri: '/2017-08-29/jobTemplates',
      exceptionFnMap: _exceptionFns,
    );
    return CreateJobTemplateResponse.fromJson(response);
  }

  /// Create a new preset. For information about job templates see the User
  /// Guide at http://docs.aws.amazon.com/mediaconvert/latest/ug/what-is.html
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [name] :
  /// The name of the preset you are creating.
  ///
  /// Parameter [settings] :
  /// Settings for preset
  ///
  /// Parameter [category] :
  /// Optional. A category for the preset you are creating.
  ///
  /// Parameter [description] :
  /// Optional. A description of the preset you are creating.
  ///
  /// Parameter [tags] :
  /// The tags that you want to add to the resource. You can tag resources with
  /// a key-value pair or with only a key.
  Future<CreatePresetResponse> createPreset({
    required String name,
    required PresetSettings settings,
    String? category,
    String? description,
    Map<String, String>? tags,
  }) async {
    final $payload = <String, dynamic>{
      'name': name,
      'settings': settings,
      if (category != null) 'category': category,
      if (description != null) 'description': description,
      if (tags != null) 'tags': tags,
    };
    final response = await _protocol.send(
      payload: $payload,
      method: 'POST',
      requestUri: '/2017-08-29/presets',
      exceptionFnMap: _exceptionFns,
    );
    return CreatePresetResponse.fromJson(response);
  }

  /// Create a new transcoding queue. For information about queues, see Working
  /// With Queues in the User Guide at
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/working-with-queues.html
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [name] :
  /// The name of the queue that you are creating.
  ///
  /// Parameter [description] :
  /// Optional. A description of the queue that you are creating.
  ///
  /// Parameter [pricingPlan] :
  /// Specifies whether the pricing plan for the queue is on-demand or reserved.
  /// For on-demand, you pay per minute, billed in increments of .01 minute. For
  /// reserved, you pay for the transcoding capacity of the entire queue,
  /// regardless of how much or how little you use it. Reserved pricing requires
  /// a 12-month commitment. When you use the API to create a queue, the default
  /// is on-demand.
  ///
  /// Parameter [reservationPlanSettings] :
  /// Details about the pricing plan for your reserved queue. Required for
  /// reserved queues and not applicable to on-demand queues.
  ///
  /// Parameter [status] :
  /// Initial state of the queue. If you create a paused queue, then jobs in
  /// that queue won't begin.
  ///
  /// Parameter [tags] :
  /// The tags that you want to add to the resource. You can tag resources with
  /// a key-value pair or with only a key.
  Future<CreateQueueResponse> createQueue({
    required String name,
    String? description,
    PricingPlan? pricingPlan,
    ReservationPlanSettings? reservationPlanSettings,
    QueueStatus? status,
    Map<String, String>? tags,
  }) async {
    final $payload = <String, dynamic>{
      'name': name,
      if (description != null) 'description': description,
      if (pricingPlan != null) 'pricingPlan': pricingPlan.value,
      if (reservationPlanSettings != null)
        'reservationPlanSettings': reservationPlanSettings,
      if (status != null) 'status': status.value,
      if (tags != null) 'tags': tags,
    };
    final response = await _protocol.send(
      payload: $payload,
      method: 'POST',
      requestUri: '/2017-08-29/queues',
      exceptionFnMap: _exceptionFns,
    );
    return CreateQueueResponse.fromJson(response);
  }

  /// Permanently delete a job template you have created.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [name] :
  /// The name of the job template to be deleted.
  Future<void> deleteJobTemplate({
    required String name,
  }) async {
    final response = await _protocol.send(
      payload: null,
      method: 'DELETE',
      requestUri: '/2017-08-29/jobTemplates/${Uri.encodeComponent(name)}',
      exceptionFnMap: _exceptionFns,
    );
  }

  /// Permanently delete a policy that you created.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  Future<void> deletePolicy() async {
    final response = await _protocol.send(
      payload: null,
      method: 'DELETE',
      requestUri: '/2017-08-29/policy',
      exceptionFnMap: _exceptionFns,
    );
  }

  /// Permanently delete a preset you have created.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [name] :
  /// The name of the preset to be deleted.
  Future<void> deletePreset({
    required String name,
  }) async {
    final response = await _protocol.send(
      payload: null,
      method: 'DELETE',
      requestUri: '/2017-08-29/presets/${Uri.encodeComponent(name)}',
      exceptionFnMap: _exceptionFns,
    );
  }

  /// Permanently delete a queue you have created.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [name] :
  /// The name of the queue that you want to delete.
  Future<void> deleteQueue({
    required String name,
  }) async {
    final response = await _protocol.send(
      payload: null,
      method: 'DELETE',
      requestUri: '/2017-08-29/queues/${Uri.encodeComponent(name)}',
      exceptionFnMap: _exceptionFns,
    );
  }

  /// Send a request with an empty body to the regional API endpoint to get your
  /// account API endpoint. Note that DescribeEndpoints is no longer required.
  /// We recommend that you send your requests directly to the regional endpoint
  /// instead.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [maxResults] :
  /// Optional. Max number of endpoints, up to twenty, that will be returned at
  /// one time.
  ///
  /// Parameter [mode] :
  /// Optional field, defaults to DEFAULT. Specify DEFAULT for this operation to
  /// return your endpoints if any exist, or to create an endpoint for you and
  /// return it if one doesn't already exist. Specify GET_ONLY to return your
  /// endpoints if any exist, or an empty list if none exist.
  ///
  /// Parameter [nextToken] :
  /// Use this string, provided with the response to a previous request, to
  /// request the next batch of endpoints.
  @Deprecated(
      'DescribeEndpoints and account specific endpoints are no longer required. We recommend that you send your requests directly to the regional endpoint instead.')
  Future<DescribeEndpointsResponse> describeEndpoints({
    int? maxResults,
    DescribeEndpointsMode? mode,
    String? nextToken,
  }) async {
    final $payload = <String, dynamic>{
      if (maxResults != null) 'maxResults': maxResults,
      if (mode != null) 'mode': mode.value,
      if (nextToken != null) 'nextToken': nextToken,
    };
    final response = await _protocol.send(
      payload: $payload,
      method: 'POST',
      requestUri: '/2017-08-29/endpoints',
      exceptionFnMap: _exceptionFns,
    );
    return DescribeEndpointsResponse.fromJson(response);
  }

  /// Removes an association between the Amazon Resource Name (ARN) of an AWS
  /// Certificate Manager (ACM) certificate and an AWS Elemental MediaConvert
  /// resource.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [arn] :
  /// The ARN of the ACM certificate that you want to disassociate from your
  /// MediaConvert resource.
  Future<void> disassociateCertificate({
    required String arn,
  }) async {
    final response = await _protocol.send(
      payload: null,
      method: 'DELETE',
      requestUri: '/2017-08-29/certificates/${Uri.encodeComponent(arn)}',
      exceptionFnMap: _exceptionFns,
    );
  }

  /// Retrieve the JSON for a specific transcoding job.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [id] :
  /// the job ID of the job.
  Future<GetJobResponse> getJob({
    required String id,
  }) async {
    final response = await _protocol.send(
      payload: null,
      method: 'GET',
      requestUri: '/2017-08-29/jobs/${Uri.encodeComponent(id)}',
      exceptionFnMap: _exceptionFns,
    );
    return GetJobResponse.fromJson(response);
  }

  /// Retrieve the JSON for a specific job template.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [name] :
  /// The name of the job template.
  Future<GetJobTemplateResponse> getJobTemplate({
    required String name,
  }) async {
    final response = await _protocol.send(
      payload: null,
      method: 'GET',
      requestUri: '/2017-08-29/jobTemplates/${Uri.encodeComponent(name)}',
      exceptionFnMap: _exceptionFns,
    );
    return GetJobTemplateResponse.fromJson(response);
  }

  /// Retrieve the JSON for your policy.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  Future<GetPolicyResponse> getPolicy() async {
    final response = await _protocol.send(
      payload: null,
      method: 'GET',
      requestUri: '/2017-08-29/policy',
      exceptionFnMap: _exceptionFns,
    );
    return GetPolicyResponse.fromJson(response);
  }

  /// Retrieve the JSON for a specific preset.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [name] :
  /// The name of the preset.
  Future<GetPresetResponse> getPreset({
    required String name,
  }) async {
    final response = await _protocol.send(
      payload: null,
      method: 'GET',
      requestUri: '/2017-08-29/presets/${Uri.encodeComponent(name)}',
      exceptionFnMap: _exceptionFns,
    );
    return GetPresetResponse.fromJson(response);
  }

  /// Retrieve the JSON for a specific queue.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [name] :
  /// The name of the queue that you want information about.
  Future<GetQueueResponse> getQueue({
    required String name,
  }) async {
    final response = await _protocol.send(
      payload: null,
      method: 'GET',
      requestUri: '/2017-08-29/queues/${Uri.encodeComponent(name)}',
      exceptionFnMap: _exceptionFns,
    );
    return GetQueueResponse.fromJson(response);
  }

  /// Retrieve a JSON array of up to twenty of your job templates. This will
  /// return the templates themselves, not just a list of them. To retrieve the
  /// next twenty templates, use the nextToken string returned with the array
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [category] :
  /// Optionally, specify a job template category to limit responses to only job
  /// templates from that category.
  ///
  /// Parameter [listBy] :
  /// Optional. When you request a list of job templates, you can choose to list
  /// them alphabetically by NAME or chronologically by CREATION_DATE. If you
  /// don't specify, the service will list them by name.
  ///
  /// Parameter [maxResults] :
  /// Optional. Number of job templates, up to twenty, that will be returned at
  /// one time.
  ///
  /// Parameter [nextToken] :
  /// Use this string, provided with the response to a previous request, to
  /// request the next batch of job templates.
  ///
  /// Parameter [order] :
  /// Optional. When you request lists of resources, you can specify whether
  /// they are sorted in ASCENDING or DESCENDING order. Default varies by
  /// resource.
  Future<ListJobTemplatesResponse> listJobTemplates({
    String? category,
    JobTemplateListBy? listBy,
    int? maxResults,
    String? nextToken,
    Order? order,
  }) async {
    _s.validateNumRange(
      'maxResults',
      maxResults,
      1,
      20,
    );
    final $query = <String, List<String>>{
      if (category != null) 'category': [category],
      if (listBy != null) 'listBy': [listBy.value],
      if (maxResults != null) 'maxResults': [maxResults.toString()],
      if (nextToken != null) 'nextToken': [nextToken],
      if (order != null) 'order': [order.value],
    };
    final response = await _protocol.send(
      payload: null,
      method: 'GET',
      requestUri: '/2017-08-29/jobTemplates',
      queryParams: $query,
      exceptionFnMap: _exceptionFns,
    );
    return ListJobTemplatesResponse.fromJson(response);
  }

  /// Retrieve a JSON array of up to twenty of your most recently created jobs.
  /// This array includes in-process, completed, and errored jobs. This will
  /// return the jobs themselves, not just a list of the jobs. To retrieve the
  /// twenty next most recent jobs, use the nextToken string returned with the
  /// array.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [maxResults] :
  /// Optional. Number of jobs, up to twenty, that will be returned at one time.
  ///
  /// Parameter [nextToken] :
  /// Optional. Use this string, provided with the response to a previous
  /// request, to request the next batch of jobs.
  ///
  /// Parameter [order] :
  /// Optional. When you request lists of resources, you can specify whether
  /// they are sorted in ASCENDING or DESCENDING order. Default varies by
  /// resource.
  ///
  /// Parameter [queue] :
  /// Optional. Provide a queue name to get back only jobs from that queue.
  ///
  /// Parameter [status] :
  /// Optional. A job's status can be SUBMITTED, PROGRESSING, COMPLETE,
  /// CANCELED, or ERROR.
  Future<ListJobsResponse> listJobs({
    int? maxResults,
    String? nextToken,
    Order? order,
    String? queue,
    JobStatus? status,
  }) async {
    _s.validateNumRange(
      'maxResults',
      maxResults,
      1,
      20,
    );
    final $query = <String, List<String>>{
      if (maxResults != null) 'maxResults': [maxResults.toString()],
      if (nextToken != null) 'nextToken': [nextToken],
      if (order != null) 'order': [order.value],
      if (queue != null) 'queue': [queue],
      if (status != null) 'status': [status.value],
    };
    final response = await _protocol.send(
      payload: null,
      method: 'GET',
      requestUri: '/2017-08-29/jobs',
      queryParams: $query,
      exceptionFnMap: _exceptionFns,
    );
    return ListJobsResponse.fromJson(response);
  }

  /// Retrieve a JSON array of up to twenty of your presets. This will return
  /// the presets themselves, not just a list of them. To retrieve the next
  /// twenty presets, use the nextToken string returned with the array.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [category] :
  /// Optionally, specify a preset category to limit responses to only presets
  /// from that category.
  ///
  /// Parameter [listBy] :
  /// Optional. When you request a list of presets, you can choose to list them
  /// alphabetically by NAME or chronologically by CREATION_DATE. If you don't
  /// specify, the service will list them by name.
  ///
  /// Parameter [maxResults] :
  /// Optional. Number of presets, up to twenty, that will be returned at one
  /// time
  ///
  /// Parameter [nextToken] :
  /// Use this string, provided with the response to a previous request, to
  /// request the next batch of presets.
  ///
  /// Parameter [order] :
  /// Optional. When you request lists of resources, you can specify whether
  /// they are sorted in ASCENDING or DESCENDING order. Default varies by
  /// resource.
  Future<ListPresetsResponse> listPresets({
    String? category,
    PresetListBy? listBy,
    int? maxResults,
    String? nextToken,
    Order? order,
  }) async {
    _s.validateNumRange(
      'maxResults',
      maxResults,
      1,
      20,
    );
    final $query = <String, List<String>>{
      if (category != null) 'category': [category],
      if (listBy != null) 'listBy': [listBy.value],
      if (maxResults != null) 'maxResults': [maxResults.toString()],
      if (nextToken != null) 'nextToken': [nextToken],
      if (order != null) 'order': [order.value],
    };
    final response = await _protocol.send(
      payload: null,
      method: 'GET',
      requestUri: '/2017-08-29/presets',
      queryParams: $query,
      exceptionFnMap: _exceptionFns,
    );
    return ListPresetsResponse.fromJson(response);
  }

  /// Retrieve a JSON array of up to twenty of your queues. This will return the
  /// queues themselves, not just a list of them. To retrieve the next twenty
  /// queues, use the nextToken string returned with the array.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [listBy] :
  /// Optional. When you request a list of queues, you can choose to list them
  /// alphabetically by NAME or chronologically by CREATION_DATE. If you don't
  /// specify, the service will list them by creation date.
  ///
  /// Parameter [maxResults] :
  /// Optional. Number of queues, up to twenty, that will be returned at one
  /// time.
  ///
  /// Parameter [nextToken] :
  /// Use this string, provided with the response to a previous request, to
  /// request the next batch of queues.
  ///
  /// Parameter [order] :
  /// Optional. When you request lists of resources, you can specify whether
  /// they are sorted in ASCENDING or DESCENDING order. Default varies by
  /// resource.
  Future<ListQueuesResponse> listQueues({
    QueueListBy? listBy,
    int? maxResults,
    String? nextToken,
    Order? order,
  }) async {
    _s.validateNumRange(
      'maxResults',
      maxResults,
      1,
      20,
    );
    final $query = <String, List<String>>{
      if (listBy != null) 'listBy': [listBy.value],
      if (maxResults != null) 'maxResults': [maxResults.toString()],
      if (nextToken != null) 'nextToken': [nextToken],
      if (order != null) 'order': [order.value],
    };
    final response = await _protocol.send(
      payload: null,
      method: 'GET',
      requestUri: '/2017-08-29/queues',
      queryParams: $query,
      exceptionFnMap: _exceptionFns,
    );
    return ListQueuesResponse.fromJson(response);
  }

  /// Retrieve the tags for a MediaConvert resource.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [arn] :
  /// The Amazon Resource Name (ARN) of the resource that you want to list tags
  /// for. To get the ARN, send a GET request with the resource name.
  Future<ListTagsForResourceResponse> listTagsForResource({
    required String arn,
  }) async {
    final response = await _protocol.send(
      payload: null,
      method: 'GET',
      requestUri: '/2017-08-29/tags/${Uri.encodeComponent(arn)}',
      exceptionFnMap: _exceptionFns,
    );
    return ListTagsForResourceResponse.fromJson(response);
  }

  /// Create or change your policy. For more information about policies, see the
  /// user guide at
  /// http://docs.aws.amazon.com/mediaconvert/latest/ug/what-is.html
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [policy] :
  /// A policy configures behavior that you allow or disallow for your account.
  /// For information about MediaConvert policies, see the user guide at
  /// http://docs.aws.amazon.com/mediaconvert/latest/ug/what-is.html
  Future<PutPolicyResponse> putPolicy({
    required Policy policy,
  }) async {
    final $payload = <String, dynamic>{
      'policy': policy,
    };
    final response = await _protocol.send(
      payload: $payload,
      method: 'PUT',
      requestUri: '/2017-08-29/policy',
      exceptionFnMap: _exceptionFns,
    );
    return PutPolicyResponse.fromJson(response);
  }

  /// Retrieve a JSON array that includes job details for up to twenty of your
  /// most recent jobs. Optionally filter results further according to input
  /// file, queue, or status. To retrieve the twenty next most recent jobs, use
  /// the nextToken string returned with the array.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [inputFile] :
  /// Optional. Provide your input file URL or your partial input file name. The
  /// maximum length for an input file is 300 characters.
  ///
  /// Parameter [maxResults] :
  /// Optional. Number of jobs, up to twenty, that will be returned at one time.
  ///
  /// Parameter [nextToken] :
  /// Optional. Use this string, provided with the response to a previous
  /// request, to request the next batch of jobs.
  ///
  /// Parameter [order] :
  /// Optional. When you request lists of resources, you can specify whether
  /// they are sorted in ASCENDING or DESCENDING order. Default varies by
  /// resource.
  ///
  /// Parameter [queue] :
  /// Optional. Provide a queue name, or a queue ARN, to return only jobs from
  /// that queue.
  ///
  /// Parameter [status] :
  /// Optional. A job's status can be SUBMITTED, PROGRESSING, COMPLETE,
  /// CANCELED, or ERROR.
  Future<SearchJobsResponse> searchJobs({
    String? inputFile,
    int? maxResults,
    String? nextToken,
    Order? order,
    String? queue,
    JobStatus? status,
  }) async {
    _s.validateNumRange(
      'maxResults',
      maxResults,
      1,
      20,
    );
    final $query = <String, List<String>>{
      if (inputFile != null) 'inputFile': [inputFile],
      if (maxResults != null) 'maxResults': [maxResults.toString()],
      if (nextToken != null) 'nextToken': [nextToken],
      if (order != null) 'order': [order.value],
      if (queue != null) 'queue': [queue],
      if (status != null) 'status': [status.value],
    };
    final response = await _protocol.send(
      payload: null,
      method: 'GET',
      requestUri: '/2017-08-29/search',
      queryParams: $query,
      exceptionFnMap: _exceptionFns,
    );
    return SearchJobsResponse.fromJson(response);
  }

  /// Add tags to a MediaConvert queue, preset, or job template. For information
  /// about tagging, see the User Guide at
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/tagging-resources.html
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [arn] :
  /// The Amazon Resource Name (ARN) of the resource that you want to tag. To
  /// get the ARN, send a GET request with the resource name.
  ///
  /// Parameter [tags] :
  /// The tags that you want to add to the resource. You can tag resources with
  /// a key-value pair or with only a key.
  Future<void> tagResource({
    required String arn,
    required Map<String, String> tags,
  }) async {
    final $payload = <String, dynamic>{
      'arn': arn,
      'tags': tags,
    };
    final response = await _protocol.send(
      payload: $payload,
      method: 'POST',
      requestUri: '/2017-08-29/tags',
      exceptionFnMap: _exceptionFns,
    );
  }

  /// Remove tags from a MediaConvert queue, preset, or job template. For
  /// information about tagging, see the User Guide at
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/tagging-resources.html
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [arn] :
  /// The Amazon Resource Name (ARN) of the resource that you want to remove
  /// tags from. To get the ARN, send a GET request with the resource name.
  ///
  /// Parameter [tagKeys] :
  /// The keys of the tags that you want to remove from the resource.
  Future<void> untagResource({
    required String arn,
    List<String>? tagKeys,
  }) async {
    final $payload = <String, dynamic>{
      if (tagKeys != null) 'tagKeys': tagKeys,
    };
    final response = await _protocol.send(
      payload: $payload,
      method: 'PUT',
      requestUri: '/2017-08-29/tags/${Uri.encodeComponent(arn)}',
      exceptionFnMap: _exceptionFns,
    );
  }

  /// Modify one of your existing job templates.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [name] :
  /// The name of the job template you are modifying
  ///
  /// Parameter [accelerationSettings] :
  /// Accelerated transcoding can significantly speed up jobs with long,
  /// visually complex content. Outputs that use this feature incur pro-tier
  /// pricing. For information about feature limitations, see the AWS Elemental
  /// MediaConvert User Guide.
  ///
  /// Parameter [category] :
  /// The new category for the job template, if you are changing it.
  ///
  /// Parameter [description] :
  /// The new description for the job template, if you are changing it.
  ///
  /// Parameter [hopDestinations] :
  /// Optional list of hop destinations.
  ///
  /// Parameter [priority] :
  /// Specify the relative priority for this job. In any given queue, the
  /// service begins processing the job with the highest value first. When more
  /// than one job has the same priority, the service begins processing the job
  /// that you submitted first. If you don't specify a priority, the service
  /// uses the default value 0.
  ///
  /// Parameter [queue] :
  /// The new queue for the job template, if you are changing it.
  ///
  /// Parameter [settings] :
  /// JobTemplateSettings contains all the transcode settings saved in the
  /// template that will be applied to jobs created from it.
  ///
  /// Parameter [statusUpdateInterval] :
  /// Specify how often MediaConvert sends STATUS_UPDATE events to Amazon
  /// CloudWatch Events. Set the interval, in seconds, between status updates.
  /// MediaConvert sends an update at this interval from the time the service
  /// begins processing your job to the time it completes the transcode or
  /// encounters an error.
  Future<UpdateJobTemplateResponse> updateJobTemplate({
    required String name,
    AccelerationSettings? accelerationSettings,
    String? category,
    String? description,
    List<HopDestination>? hopDestinations,
    int? priority,
    String? queue,
    JobTemplateSettings? settings,
    StatusUpdateInterval? statusUpdateInterval,
  }) async {
    _s.validateNumRange(
      'priority',
      priority,
      -50,
      50,
    );
    final $payload = <String, dynamic>{
      if (accelerationSettings != null)
        'accelerationSettings': accelerationSettings,
      if (category != null) 'category': category,
      if (description != null) 'description': description,
      if (hopDestinations != null) 'hopDestinations': hopDestinations,
      if (priority != null) 'priority': priority,
      if (queue != null) 'queue': queue,
      if (settings != null) 'settings': settings,
      if (statusUpdateInterval != null)
        'statusUpdateInterval': statusUpdateInterval.value,
    };
    final response = await _protocol.send(
      payload: $payload,
      method: 'PUT',
      requestUri: '/2017-08-29/jobTemplates/${Uri.encodeComponent(name)}',
      exceptionFnMap: _exceptionFns,
    );
    return UpdateJobTemplateResponse.fromJson(response);
  }

  /// Modify one of your existing presets.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [name] :
  /// The name of the preset you are modifying.
  ///
  /// Parameter [category] :
  /// The new category for the preset, if you are changing it.
  ///
  /// Parameter [description] :
  /// The new description for the preset, if you are changing it.
  ///
  /// Parameter [settings] :
  /// Settings for preset
  Future<UpdatePresetResponse> updatePreset({
    required String name,
    String? category,
    String? description,
    PresetSettings? settings,
  }) async {
    final $payload = <String, dynamic>{
      if (category != null) 'category': category,
      if (description != null) 'description': description,
      if (settings != null) 'settings': settings,
    };
    final response = await _protocol.send(
      payload: $payload,
      method: 'PUT',
      requestUri: '/2017-08-29/presets/${Uri.encodeComponent(name)}',
      exceptionFnMap: _exceptionFns,
    );
    return UpdatePresetResponse.fromJson(response);
  }

  /// Modify one of your existing queues.
  ///
  /// May throw [BadRequestException].
  /// May throw [InternalServerErrorException].
  /// May throw [ForbiddenException].
  /// May throw [NotFoundException].
  /// May throw [TooManyRequestsException].
  /// May throw [ConflictException].
  ///
  /// Parameter [name] :
  /// The name of the queue that you are modifying.
  ///
  /// Parameter [description] :
  /// The new description for the queue, if you are changing it.
  ///
  /// Parameter [reservationPlanSettings] :
  /// The new details of your pricing plan for your reserved queue. When you set
  /// up a new pricing plan to replace an expired one, you enter into another
  /// 12-month commitment. When you add capacity to your queue by increasing the
  /// number of RTS, you extend the term of your commitment to 12 months from
  /// when you add capacity. After you make these commitments, you can't cancel
  /// them.
  ///
  /// Parameter [status] :
  /// Pause or activate a queue by changing its status between ACTIVE and
  /// PAUSED. If you pause a queue, jobs in that queue won't begin. Jobs that
  /// are running when you pause the queue continue to run until they finish or
  /// result in an error.
  Future<UpdateQueueResponse> updateQueue({
    required String name,
    String? description,
    ReservationPlanSettings? reservationPlanSettings,
    QueueStatus? status,
  }) async {
    final $payload = <String, dynamic>{
      if (description != null) 'description': description,
      if (reservationPlanSettings != null)
        'reservationPlanSettings': reservationPlanSettings,
      if (status != null) 'status': status.value,
    };
    final response = await _protocol.send(
      payload: $payload,
      method: 'PUT',
      requestUri: '/2017-08-29/queues/${Uri.encodeComponent(name)}',
      exceptionFnMap: _exceptionFns,
    );
    return UpdateQueueResponse.fromJson(response);
  }
}

/// Choose BROADCASTER_MIXED_AD when the input contains pre-mixed main audio +
/// audio description (AD) as a stereo pair. The value for AudioType will be set
/// to 3, which signals to downstream systems that this stream contains
/// "broadcaster mixed AD". Note that the input received by the encoder must
/// contain pre-mixed audio; the encoder does not perform the mixing. When you
/// choose BROADCASTER_MIXED_AD, the encoder ignores any values you provide in
/// AudioType and FollowInputAudioType. Choose NORMAL when the input does not
/// contain pre-mixed audio + audio description (AD). In this case, the encoder
/// will use any values you provide for AudioType and FollowInputAudioType.
enum AacAudioDescriptionBroadcasterMix {
  broadcasterMixedAd('BROADCASTER_MIXED_AD'),
  normal('NORMAL'),
  ;

  final String value;

  const AacAudioDescriptionBroadcasterMix(this.value);

  static AacAudioDescriptionBroadcasterMix fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum AacAudioDescriptionBroadcasterMix'));
}

/// AAC Profile.
enum AacCodecProfile {
  lc('LC'),
  hev1('HEV1'),
  hev2('HEV2'),
  ;

  final String value;

  const AacCodecProfile(this.value);

  static AacCodecProfile fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum AacCodecProfile'));
}

/// The Coding mode that you specify determines the number of audio channels and
/// the audio channel layout metadata in your AAC output. Valid coding modes
/// depend on the Rate control mode and Profile that you select. The following
/// list shows the number of audio channels and channel layout for each coding
/// mode. * 1.0 Audio Description (Receiver Mix): One channel, C. Includes audio
/// description data from your stereo input. For more information see ETSI TS
/// 101 154 Annex E. * 1.0 Mono: One channel, C. * 2.0 Stereo: Two channels, L,
/// R. * 5.1 Surround: Six channels, C, L, R, Ls, Rs, LFE.
enum AacCodingMode {
  adReceiverMix('AD_RECEIVER_MIX'),
  codingMode_1_0('CODING_MODE_1_0'),
  codingMode_1_1('CODING_MODE_1_1'),
  codingMode_2_0('CODING_MODE_2_0'),
  codingMode_5_1('CODING_MODE_5_1'),
  ;

  final String value;

  const AacCodingMode(this.value);

  static AacCodingMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum AacCodingMode'));
}

/// Rate Control Mode.
enum AacRateControlMode {
  cbr('CBR'),
  vbr('VBR'),
  ;

  final String value;

  const AacRateControlMode(this.value);

  static AacRateControlMode fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum AacRateControlMode'));
}

/// Enables LATM/LOAS AAC output. Note that if you use LATM/LOAS AAC in an
/// output, you must choose "No container" for the output container.
enum AacRawFormat {
  latmLoas('LATM_LOAS'),
  none('NONE'),
  ;

  final String value;

  const AacRawFormat(this.value);

  static AacRawFormat fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum AacRawFormat'));
}

/// Required when you set Codec to the value AAC. The service accepts one of two
/// mutually exclusive groups of AAC settings--VBR and CBR. To select one of
/// these modes, set the value of Bitrate control mode to "VBR" or "CBR". In VBR
/// mode, you control the audio quality with the setting VBR quality. In CBR
/// mode, you use the setting Bitrate. Defaults and valid values depend on the
/// rate control mode.
class AacSettings {
  /// Choose BROADCASTER_MIXED_AD when the input contains pre-mixed main audio +
  /// audio description (AD) as a stereo pair. The value for AudioType will be set
  /// to 3, which signals to downstream systems that this stream contains
  /// "broadcaster mixed AD". Note that the input received by the encoder must
  /// contain pre-mixed audio; the encoder does not perform the mixing. When you
  /// choose BROADCASTER_MIXED_AD, the encoder ignores any values you provide in
  /// AudioType and FollowInputAudioType. Choose NORMAL when the input does not
  /// contain pre-mixed audio + audio description (AD). In this case, the encoder
  /// will use any values you provide for AudioType and FollowInputAudioType.
  final AacAudioDescriptionBroadcasterMix? audioDescriptionBroadcasterMix;

  /// Specify the average bitrate in bits per second. The set of valid values for
  /// this setting is: 6000, 8000, 10000, 12000, 14000, 16000, 20000, 24000,
  /// 28000, 32000, 40000, 48000, 56000, 64000, 80000, 96000, 112000, 128000,
  /// 160000, 192000, 224000, 256000, 288000, 320000, 384000, 448000, 512000,
  /// 576000, 640000, 768000, 896000, 1024000. The value you set is also
  /// constrained by the values that you choose for Profile, Bitrate control mode,
  /// and Sample rate. Default values depend on Bitrate control mode and Profile.
  final int? bitrate;

  /// AAC Profile.
  final AacCodecProfile? codecProfile;

  /// The Coding mode that you specify determines the number of audio channels and
  /// the audio channel layout metadata in your AAC output. Valid coding modes
  /// depend on the Rate control mode and Profile that you select. The following
  /// list shows the number of audio channels and channel layout for each coding
  /// mode. * 1.0 Audio Description (Receiver Mix): One channel, C. Includes audio
  /// description data from your stereo input. For more information see ETSI TS
  /// 101 154 Annex E. * 1.0 Mono: One channel, C. * 2.0 Stereo: Two channels, L,
  /// R. * 5.1 Surround: Six channels, C, L, R, Ls, Rs, LFE.
  final AacCodingMode? codingMode;

  /// Rate Control Mode.
  final AacRateControlMode? rateControlMode;

  /// Enables LATM/LOAS AAC output. Note that if you use LATM/LOAS AAC in an
  /// output, you must choose "No container" for the output container.
  final AacRawFormat? rawFormat;

  /// Specify the Sample rate in Hz. Valid sample rates depend on the Profile and
  /// Coding mode that you select. The following list shows valid sample rates for
  /// each Profile and Coding mode. * LC Profile, Coding mode 1.0, 2.0, and
  /// Receiver Mix: 8000, 12000, 16000, 22050, 24000, 32000, 44100, 48000, 88200,
  /// 96000. * LC Profile, Coding mode 5.1: 32000, 44100, 48000, 96000. * HEV1
  /// Profile, Coding mode 1.0 and Receiver Mix: 22050, 24000, 32000, 44100,
  /// 48000. * HEV1 Profile, Coding mode 2.0 and 5.1: 32000, 44100, 48000, 96000.
  /// * HEV2 Profile, Coding mode 2.0: 22050, 24000, 32000, 44100, 48000.
  final int? sampleRate;

  /// Use MPEG-2 AAC instead of MPEG-4 AAC audio for raw or MPEG-2 Transport
  /// Stream containers.
  final AacSpecification? specification;

  /// VBR Quality Level - Only used if rate_control_mode is VBR.
  final AacVbrQuality? vbrQuality;

  AacSettings({
    this.audioDescriptionBroadcasterMix,
    this.bitrate,
    this.codecProfile,
    this.codingMode,
    this.rateControlMode,
    this.rawFormat,
    this.sampleRate,
    this.specification,
    this.vbrQuality,
  });

  factory AacSettings.fromJson(Map<String, dynamic> json) {
    return AacSettings(
      audioDescriptionBroadcasterMix:
          (json['audioDescriptionBroadcasterMix'] as String?)
              ?.let(AacAudioDescriptionBroadcasterMix.fromString),
      bitrate: json['bitrate'] as int?,
      codecProfile:
          (json['codecProfile'] as String?)?.let(AacCodecProfile.fromString),
      codingMode:
          (json['codingMode'] as String?)?.let(AacCodingMode.fromString),
      rateControlMode: (json['rateControlMode'] as String?)
          ?.let(AacRateControlMode.fromString),
      rawFormat: (json['rawFormat'] as String?)?.let(AacRawFormat.fromString),
      sampleRate: json['sampleRate'] as int?,
      specification:
          (json['specification'] as String?)?.let(AacSpecification.fromString),
      vbrQuality:
          (json['vbrQuality'] as String?)?.let(AacVbrQuality.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final audioDescriptionBroadcasterMix = this.audioDescriptionBroadcasterMix;
    final bitrate = this.bitrate;
    final codecProfile = this.codecProfile;
    final codingMode = this.codingMode;
    final rateControlMode = this.rateControlMode;
    final rawFormat = this.rawFormat;
    final sampleRate = this.sampleRate;
    final specification = this.specification;
    final vbrQuality = this.vbrQuality;
    return {
      if (audioDescriptionBroadcasterMix != null)
        'audioDescriptionBroadcasterMix': audioDescriptionBroadcasterMix.value,
      if (bitrate != null) 'bitrate': bitrate,
      if (codecProfile != null) 'codecProfile': codecProfile.value,
      if (codingMode != null) 'codingMode': codingMode.value,
      if (rateControlMode != null) 'rateControlMode': rateControlMode.value,
      if (rawFormat != null) 'rawFormat': rawFormat.value,
      if (sampleRate != null) 'sampleRate': sampleRate,
      if (specification != null) 'specification': specification.value,
      if (vbrQuality != null) 'vbrQuality': vbrQuality.value,
    };
  }
}

/// Use MPEG-2 AAC instead of MPEG-4 AAC audio for raw or MPEG-2 Transport
/// Stream containers.
enum AacSpecification {
  mpeg2('MPEG2'),
  mpeg4('MPEG4'),
  ;

  final String value;

  const AacSpecification(this.value);

  static AacSpecification fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum AacSpecification'));
}

/// VBR Quality Level - Only used if rate_control_mode is VBR.
enum AacVbrQuality {
  low('LOW'),
  mediumLow('MEDIUM_LOW'),
  mediumHigh('MEDIUM_HIGH'),
  high('HIGH'),
  ;

  final String value;

  const AacVbrQuality(this.value);

  static AacVbrQuality fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum AacVbrQuality'));
}

/// Specify the bitstream mode for the AC-3 stream that the encoder emits. For
/// more information about the AC3 bitstream mode, see ATSC A/52-2012 (Annex E).
enum Ac3BitstreamMode {
  completeMain('COMPLETE_MAIN'),
  commentary('COMMENTARY'),
  dialogue('DIALOGUE'),
  emergency('EMERGENCY'),
  hearingImpaired('HEARING_IMPAIRED'),
  musicAndEffects('MUSIC_AND_EFFECTS'),
  visuallyImpaired('VISUALLY_IMPAIRED'),
  voiceOver('VOICE_OVER'),
  ;

  final String value;

  const Ac3BitstreamMode(this.value);

  static Ac3BitstreamMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Ac3BitstreamMode'));
}

/// Dolby Digital coding mode. Determines number of channels.
enum Ac3CodingMode {
  codingMode_1_0('CODING_MODE_1_0'),
  codingMode_1_1('CODING_MODE_1_1'),
  codingMode_2_0('CODING_MODE_2_0'),
  codingMode_3_2Lfe('CODING_MODE_3_2_LFE'),
  ;

  final String value;

  const Ac3CodingMode(this.value);

  static Ac3CodingMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Ac3CodingMode'));
}

/// Choose the Dolby Digital dynamic range control (DRC) profile that
/// MediaConvert uses when encoding the metadata in the Dolby Digital stream for
/// the line operating mode. Related setting: When you use this setting,
/// MediaConvert ignores any value you provide for Dynamic range compression
/// profile. For information about the Dolby Digital DRC operating modes and
/// profiles, see the Dynamic Range Control chapter of the Dolby Metadata Guide
/// at
/// https://developer.dolby.com/globalassets/professional/documents/dolby-metadata-guide.pdf.
enum Ac3DynamicRangeCompressionLine {
  filmStandard('FILM_STANDARD'),
  filmLight('FILM_LIGHT'),
  musicStandard('MUSIC_STANDARD'),
  musicLight('MUSIC_LIGHT'),
  speech('SPEECH'),
  none('NONE'),
  ;

  final String value;

  const Ac3DynamicRangeCompressionLine(this.value);

  static Ac3DynamicRangeCompressionLine fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Ac3DynamicRangeCompressionLine'));
}

/// When you want to add Dolby dynamic range compression (DRC) signaling to your
/// output stream, we recommend that you use the mode-specific settings instead
/// of Dynamic range compression profile. The mode-specific settings are Dynamic
/// range compression profile, line mode and Dynamic range compression profile,
/// RF mode. Note that when you specify values for all three settings,
/// MediaConvert ignores the value of this setting in favor of the mode-specific
/// settings. If you do use this setting instead of the mode-specific settings,
/// choose None to leave out DRC signaling. Keep the default Film standard to
/// set the profile to Dolby's film standard profile for all operating modes.
enum Ac3DynamicRangeCompressionProfile {
  filmStandard('FILM_STANDARD'),
  none('NONE'),
  ;

  final String value;

  const Ac3DynamicRangeCompressionProfile(this.value);

  static Ac3DynamicRangeCompressionProfile fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Ac3DynamicRangeCompressionProfile'));
}

/// Choose the Dolby Digital dynamic range control (DRC) profile that
/// MediaConvert uses when encoding the metadata in the Dolby Digital stream for
/// the RF operating mode. Related setting: When you use this setting,
/// MediaConvert ignores any value you provide for Dynamic range compression
/// profile. For information about the Dolby Digital DRC operating modes and
/// profiles, see the Dynamic Range Control chapter of the Dolby Metadata Guide
/// at
/// https://developer.dolby.com/globalassets/professional/documents/dolby-metadata-guide.pdf.
enum Ac3DynamicRangeCompressionRf {
  filmStandard('FILM_STANDARD'),
  filmLight('FILM_LIGHT'),
  musicStandard('MUSIC_STANDARD'),
  musicLight('MUSIC_LIGHT'),
  speech('SPEECH'),
  none('NONE'),
  ;

  final String value;

  const Ac3DynamicRangeCompressionRf(this.value);

  static Ac3DynamicRangeCompressionRf fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Ac3DynamicRangeCompressionRf'));
}

/// Applies a 120Hz lowpass filter to the LFE channel prior to encoding. Only
/// valid with 3_2_LFE coding mode.
enum Ac3LfeFilter {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const Ac3LfeFilter(this.value);

  static Ac3LfeFilter fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Ac3LfeFilter'));
}

/// When set to FOLLOW_INPUT, encoder metadata will be sourced from the DD, DD+,
/// or DolbyE decoder that supplied this audio data. If audio was not supplied
/// from one of these streams, then the static metadata settings will be used.
enum Ac3MetadataControl {
  followInput('FOLLOW_INPUT'),
  useConfigured('USE_CONFIGURED'),
  ;

  final String value;

  const Ac3MetadataControl(this.value);

  static Ac3MetadataControl fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Ac3MetadataControl'));
}

/// Required when you set Codec to the value AC3.
class Ac3Settings {
  /// Specify the average bitrate in bits per second. The bitrate that you specify
  /// must be a multiple of 8000 within the allowed minimum and maximum values.
  /// Leave blank to use the default bitrate for the coding mode you select
  /// according ETSI TS 102 366. Valid bitrates for coding mode 1/0: Default:
  /// 96000. Minimum: 64000. Maximum: 128000. Valid bitrates for coding mode 1/1:
  /// Default: 192000. Minimum: 128000. Maximum: 384000. Valid bitrates for coding
  /// mode 2/0: Default: 192000. Minimum: 128000. Maximum: 384000. Valid bitrates
  /// for coding mode 3/2 with FLE: Default: 384000. Minimum: 384000. Maximum:
  /// 640000.
  final int? bitrate;

  /// Specify the bitstream mode for the AC-3 stream that the encoder emits. For
  /// more information about the AC3 bitstream mode, see ATSC A/52-2012 (Annex E).
  final Ac3BitstreamMode? bitstreamMode;

  /// Dolby Digital coding mode. Determines number of channels.
  final Ac3CodingMode? codingMode;

  /// Sets the dialnorm for the output. If blank and input audio is Dolby Digital,
  /// dialnorm will be passed through.
  final int? dialnorm;

  /// Choose the Dolby Digital dynamic range control (DRC) profile that
  /// MediaConvert uses when encoding the metadata in the Dolby Digital stream for
  /// the line operating mode. Related setting: When you use this setting,
  /// MediaConvert ignores any value you provide for Dynamic range compression
  /// profile. For information about the Dolby Digital DRC operating modes and
  /// profiles, see the Dynamic Range Control chapter of the Dolby Metadata Guide
  /// at
  /// https://developer.dolby.com/globalassets/professional/documents/dolby-metadata-guide.pdf.
  final Ac3DynamicRangeCompressionLine? dynamicRangeCompressionLine;

  /// When you want to add Dolby dynamic range compression (DRC) signaling to your
  /// output stream, we recommend that you use the mode-specific settings instead
  /// of Dynamic range compression profile. The mode-specific settings are Dynamic
  /// range compression profile, line mode and Dynamic range compression profile,
  /// RF mode. Note that when you specify values for all three settings,
  /// MediaConvert ignores the value of this setting in favor of the mode-specific
  /// settings. If you do use this setting instead of the mode-specific settings,
  /// choose None to leave out DRC signaling. Keep the default Film standard to
  /// set the profile to Dolby's film standard profile for all operating modes.
  final Ac3DynamicRangeCompressionProfile? dynamicRangeCompressionProfile;

  /// Choose the Dolby Digital dynamic range control (DRC) profile that
  /// MediaConvert uses when encoding the metadata in the Dolby Digital stream for
  /// the RF operating mode. Related setting: When you use this setting,
  /// MediaConvert ignores any value you provide for Dynamic range compression
  /// profile. For information about the Dolby Digital DRC operating modes and
  /// profiles, see the Dynamic Range Control chapter of the Dolby Metadata Guide
  /// at
  /// https://developer.dolby.com/globalassets/professional/documents/dolby-metadata-guide.pdf.
  final Ac3DynamicRangeCompressionRf? dynamicRangeCompressionRf;

  /// Applies a 120Hz lowpass filter to the LFE channel prior to encoding. Only
  /// valid with 3_2_LFE coding mode.
  final Ac3LfeFilter? lfeFilter;

  /// When set to FOLLOW_INPUT, encoder metadata will be sourced from the DD, DD+,
  /// or DolbyE decoder that supplied this audio data. If audio was not supplied
  /// from one of these streams, then the static metadata settings will be used.
  final Ac3MetadataControl? metadataControl;

  /// This value is always 48000. It represents the sample rate in Hz.
  final int? sampleRate;

  Ac3Settings({
    this.bitrate,
    this.bitstreamMode,
    this.codingMode,
    this.dialnorm,
    this.dynamicRangeCompressionLine,
    this.dynamicRangeCompressionProfile,
    this.dynamicRangeCompressionRf,
    this.lfeFilter,
    this.metadataControl,
    this.sampleRate,
  });

  factory Ac3Settings.fromJson(Map<String, dynamic> json) {
    return Ac3Settings(
      bitrate: json['bitrate'] as int?,
      bitstreamMode:
          (json['bitstreamMode'] as String?)?.let(Ac3BitstreamMode.fromString),
      codingMode:
          (json['codingMode'] as String?)?.let(Ac3CodingMode.fromString),
      dialnorm: json['dialnorm'] as int?,
      dynamicRangeCompressionLine:
          (json['dynamicRangeCompressionLine'] as String?)
              ?.let(Ac3DynamicRangeCompressionLine.fromString),
      dynamicRangeCompressionProfile:
          (json['dynamicRangeCompressionProfile'] as String?)
              ?.let(Ac3DynamicRangeCompressionProfile.fromString),
      dynamicRangeCompressionRf: (json['dynamicRangeCompressionRf'] as String?)
          ?.let(Ac3DynamicRangeCompressionRf.fromString),
      lfeFilter: (json['lfeFilter'] as String?)?.let(Ac3LfeFilter.fromString),
      metadataControl: (json['metadataControl'] as String?)
          ?.let(Ac3MetadataControl.fromString),
      sampleRate: json['sampleRate'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final bitrate = this.bitrate;
    final bitstreamMode = this.bitstreamMode;
    final codingMode = this.codingMode;
    final dialnorm = this.dialnorm;
    final dynamicRangeCompressionLine = this.dynamicRangeCompressionLine;
    final dynamicRangeCompressionProfile = this.dynamicRangeCompressionProfile;
    final dynamicRangeCompressionRf = this.dynamicRangeCompressionRf;
    final lfeFilter = this.lfeFilter;
    final metadataControl = this.metadataControl;
    final sampleRate = this.sampleRate;
    return {
      if (bitrate != null) 'bitrate': bitrate,
      if (bitstreamMode != null) 'bitstreamMode': bitstreamMode.value,
      if (codingMode != null) 'codingMode': codingMode.value,
      if (dialnorm != null) 'dialnorm': dialnorm,
      if (dynamicRangeCompressionLine != null)
        'dynamicRangeCompressionLine': dynamicRangeCompressionLine.value,
      if (dynamicRangeCompressionProfile != null)
        'dynamicRangeCompressionProfile': dynamicRangeCompressionProfile.value,
      if (dynamicRangeCompressionRf != null)
        'dynamicRangeCompressionRf': dynamicRangeCompressionRf.value,
      if (lfeFilter != null) 'lfeFilter': lfeFilter.value,
      if (metadataControl != null) 'metadataControl': metadataControl.value,
      if (sampleRate != null) 'sampleRate': sampleRate,
    };
  }
}

/// Specify whether the service runs your job with accelerated transcoding.
/// Choose DISABLED if you don't want accelerated transcoding. Choose ENABLED if
/// you want your job to run with accelerated transcoding and to fail if your
/// input files or your job settings aren't compatible with accelerated
/// transcoding. Choose PREFERRED if you want your job to run with accelerated
/// transcoding if the job is compatible with the feature and to run at standard
/// speed if it's not.
enum AccelerationMode {
  disabled('DISABLED'),
  enabled('ENABLED'),
  preferred('PREFERRED'),
  ;

  final String value;

  const AccelerationMode(this.value);

  static AccelerationMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum AccelerationMode'));
}

/// Accelerated transcoding can significantly speed up jobs with long, visually
/// complex content.
class AccelerationSettings {
  /// Specify the conditions when the service will run your job with accelerated
  /// transcoding.
  final AccelerationMode mode;

  AccelerationSettings({
    required this.mode,
  });

  factory AccelerationSettings.fromJson(Map<String, dynamic> json) {
    return AccelerationSettings(
      mode: AccelerationMode.fromString((json['mode'] as String)),
    );
  }

  Map<String, dynamic> toJson() {
    final mode = this.mode;
    return {
      'mode': mode.value,
    };
  }
}

/// Describes whether the current job is running with accelerated transcoding.
/// For jobs that have Acceleration (AccelerationMode) set to DISABLED,
/// AccelerationStatus is always NOT_APPLICABLE. For jobs that have Acceleration
/// (AccelerationMode) set to ENABLED or PREFERRED, AccelerationStatus is one of
/// the other states. AccelerationStatus is IN_PROGRESS initially, while the
/// service determines whether the input files and job settings are compatible
/// with accelerated transcoding. If they are, AcclerationStatus is ACCELERATED.
/// If your input files and job settings aren't compatible with accelerated
/// transcoding, the service either fails your job or runs it without
/// accelerated transcoding, depending on how you set Acceleration
/// (AccelerationMode). When the service runs your job without accelerated
/// transcoding, AccelerationStatus is NOT_ACCELERATED.
enum AccelerationStatus {
  notApplicable('NOT_APPLICABLE'),
  inProgress('IN_PROGRESS'),
  accelerated('ACCELERATED'),
  notAccelerated('NOT_ACCELERATED'),
  ;

  final String value;

  const AccelerationStatus(this.value);

  static AccelerationStatus fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum AccelerationStatus'));
}

/// Use to remove noise, blocking, blurriness, or ringing from your input as a
/// pre-filter step before encoding. The Advanced input filter removes more
/// types of compression artifacts and is an improvement when compared to basic
/// Deblock and Denoise filters. To remove video compression artifacts from your
/// input and improve the video quality: Choose Enabled. Additionally, this
/// filter can help increase the video quality of your output relative to its
/// bitrate, since noisy inputs are more complex and require more bits to
/// encode. To help restore loss of detail after applying the filter, you can
/// optionally add texture or sharpening as an additional step. Jobs that use
/// this feature incur pro-tier pricing. To not apply advanced input filtering:
/// Choose Disabled. Note that you can still apply basic filtering with Deblock
/// and Denoise.
enum AdvancedInputFilter {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const AdvancedInputFilter(this.value);

  static AdvancedInputFilter fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum AdvancedInputFilter'));
}

/// Add texture and detail to areas of your input video content that were lost
/// after applying the Advanced input filter. To adaptively add texture and
/// reduce softness: Choose Enabled. To not add any texture: Keep the default
/// value, Disabled. We recommend that you choose Disabled for input video
/// content that doesn't have texture, including screen recordings, computer
/// graphics, or cartoons.
enum AdvancedInputFilterAddTexture {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const AdvancedInputFilterAddTexture(this.value);

  static AdvancedInputFilterAddTexture fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum AdvancedInputFilterAddTexture'));
}

/// Optional settings for Advanced input filter when you set Advanced input
/// filter to Enabled.
class AdvancedInputFilterSettings {
  /// Add texture and detail to areas of your input video content that were lost
  /// after applying the Advanced input filter. To adaptively add texture and
  /// reduce softness: Choose Enabled. To not add any texture: Keep the default
  /// value, Disabled. We recommend that you choose Disabled for input video
  /// content that doesn't have texture, including screen recordings, computer
  /// graphics, or cartoons.
  final AdvancedInputFilterAddTexture? addTexture;

  /// Optionally specify the amount of sharpening to apply when you use the
  /// Advanced input filter. Sharpening adds contrast to the edges of your video
  /// content and can reduce softness. To apply no sharpening: Keep the default
  /// value, Off. To apply a minimal amount of sharpening choose Low, or for the
  /// maximum choose High.
  final AdvancedInputFilterSharpen? sharpening;

  AdvancedInputFilterSettings({
    this.addTexture,
    this.sharpening,
  });

  factory AdvancedInputFilterSettings.fromJson(Map<String, dynamic> json) {
    return AdvancedInputFilterSettings(
      addTexture: (json['addTexture'] as String?)
          ?.let(AdvancedInputFilterAddTexture.fromString),
      sharpening: (json['sharpening'] as String?)
          ?.let(AdvancedInputFilterSharpen.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final addTexture = this.addTexture;
    final sharpening = this.sharpening;
    return {
      if (addTexture != null) 'addTexture': addTexture.value,
      if (sharpening != null) 'sharpening': sharpening.value,
    };
  }
}

/// Optionally specify the amount of sharpening to apply when you use the
/// Advanced input filter. Sharpening adds contrast to the edges of your video
/// content and can reduce softness. To apply no sharpening: Keep the default
/// value, Off. To apply a minimal amount of sharpening choose Low, or for the
/// maximum choose High.
enum AdvancedInputFilterSharpen {
  off('OFF'),
  low('LOW'),
  high('HIGH'),
  ;

  final String value;

  const AdvancedInputFilterSharpen(this.value);

  static AdvancedInputFilterSharpen fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum AdvancedInputFilterSharpen'));
}

/// This setting only applies to H.264, H.265, and MPEG2 outputs. Use Insert AFD
/// signaling to specify whether the service includes AFD values in the output
/// video data and what those values are. * Choose None to remove all AFD values
/// from this output. * Choose Fixed to ignore input AFD values and instead
/// encode the value specified in the job. * Choose Auto to calculate output AFD
/// values based on the input AFD scaler data.
enum AfdSignaling {
  none('NONE'),
  auto('AUTO'),
  fixed('FIXED'),
  ;

  final String value;

  const AfdSignaling(this.value);

  static AfdSignaling fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum AfdSignaling'));
}

/// Required when you set Codec to the value AIFF.
class AiffSettings {
  /// Specify Bit depth, in bits per sample, to choose the encoding quality for
  /// this audio track.
  final int? bitDepth;

  /// Specify the number of channels in this output audio track. Valid values are
  /// 1 and even numbers up to 64. For example, 1, 2, 4, 6, and so on, up to 64.
  final int? channels;

  /// Sample rate in Hz.
  final int? sampleRate;

  AiffSettings({
    this.bitDepth,
    this.channels,
    this.sampleRate,
  });

  factory AiffSettings.fromJson(Map<String, dynamic> json) {
    return AiffSettings(
      bitDepth: json['bitDepth'] as int?,
      channels: json['channels'] as int?,
      sampleRate: json['sampleRate'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final bitDepth = this.bitDepth;
    final channels = this.channels;
    final sampleRate = this.sampleRate;
    return {
      if (bitDepth != null) 'bitDepth': bitDepth,
      if (channels != null) 'channels': channels,
      if (sampleRate != null) 'sampleRate': sampleRate,
    };
  }
}

/// Use Allowed renditions to specify a list of possible resolutions in your ABR
/// stack. * MediaConvert will create an ABR stack exclusively from the list of
/// resolutions that you specify. * Some resolutions in the Allowed renditions
/// list may not be included, however you can force a resolution to be included
/// by setting Required to ENABLED. * You must specify at least one resolution
/// that is greater than or equal to any resolutions that you specify in Min top
/// rendition size or Min bottom rendition size. * If you specify Allowed
/// renditions, you must not specify a separate rule for Force include
/// renditions.
class AllowedRenditionSize {
  /// Use Height to define the video resolution height, in pixels, for this rule.
  final int? height;

  /// Set to ENABLED to force a rendition to be included.
  final RequiredFlag? required;

  /// Use Width to define the video resolution width, in pixels, for this rule.
  final int? width;

  AllowedRenditionSize({
    this.height,
    this.required,
    this.width,
  });

  factory AllowedRenditionSize.fromJson(Map<String, dynamic> json) {
    return AllowedRenditionSize(
      height: json['height'] as int?,
      required: (json['required'] as String?)?.let(RequiredFlag.fromString),
      width: json['width'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final height = this.height;
    final required = this.required;
    final width = this.width;
    return {
      if (height != null) 'height': height,
      if (required != null) 'required': required.value,
      if (width != null) 'width': width,
    };
  }
}

/// Ignore this setting unless this input is a QuickTime animation with an alpha
/// channel. Use this setting to create separate Key and Fill outputs. In each
/// output, specify which part of the input MediaConvert uses. Leave this
/// setting at the default value DISCARD to delete the alpha channel and
/// preserve the video. Set it to REMAP_TO_LUMA to delete the video and map the
/// alpha channel to the luma channel of your outputs.
enum AlphaBehavior {
  discard('DISCARD'),
  remapToLuma('REMAP_TO_LUMA'),
  ;

  final String value;

  const AlphaBehavior(this.value);

  static AlphaBehavior fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum AlphaBehavior'));
}

/// Specify whether this set of input captions appears in your outputs in both
/// 608 and 708 format. If you choose Upconvert, MediaConvert includes the
/// captions data in two ways: it passes the 608 data through using the 608
/// compatibility bytes fields of the 708 wrapper, and it also translates the
/// 608 data into 708.
enum AncillaryConvert608To708 {
  upconvert('UPCONVERT'),
  disabled('DISABLED'),
  ;

  final String value;

  const AncillaryConvert608To708(this.value);

  static AncillaryConvert608To708 fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum AncillaryConvert608To708'));
}

/// Settings for ancillary captions source.
class AncillarySourceSettings {
  /// Specify whether this set of input captions appears in your outputs in both
  /// 608 and 708 format. If you choose Upconvert, MediaConvert includes the
  /// captions data in two ways: it passes the 608 data through using the 608
  /// compatibility bytes fields of the 708 wrapper, and it also translates the
  /// 608 data into 708.
  final AncillaryConvert608To708? convert608To708;

  /// Specifies the 608 channel number in the ancillary data track from which to
  /// extract captions. Unused for passthrough.
  final int? sourceAncillaryChannelNumber;

  /// By default, the service terminates any unterminated captions at the end of
  /// each input. If you want the caption to continue onto your next input,
  /// disable this setting.
  final AncillaryTerminateCaptions? terminateCaptions;

  AncillarySourceSettings({
    this.convert608To708,
    this.sourceAncillaryChannelNumber,
    this.terminateCaptions,
  });

  factory AncillarySourceSettings.fromJson(Map<String, dynamic> json) {
    return AncillarySourceSettings(
      convert608To708: (json['convert608To708'] as String?)
          ?.let(AncillaryConvert608To708.fromString),
      sourceAncillaryChannelNumber:
          json['sourceAncillaryChannelNumber'] as int?,
      terminateCaptions: (json['terminateCaptions'] as String?)
          ?.let(AncillaryTerminateCaptions.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final convert608To708 = this.convert608To708;
    final sourceAncillaryChannelNumber = this.sourceAncillaryChannelNumber;
    final terminateCaptions = this.terminateCaptions;
    return {
      if (convert608To708 != null) 'convert608To708': convert608To708.value,
      if (sourceAncillaryChannelNumber != null)
        'sourceAncillaryChannelNumber': sourceAncillaryChannelNumber,
      if (terminateCaptions != null)
        'terminateCaptions': terminateCaptions.value,
    };
  }
}

/// By default, the service terminates any unterminated captions at the end of
/// each input. If you want the caption to continue onto your next input,
/// disable this setting.
enum AncillaryTerminateCaptions {
  endOfInput('END_OF_INPUT'),
  disabled('DISABLED'),
  ;

  final String value;

  const AncillaryTerminateCaptions(this.value);

  static AncillaryTerminateCaptions fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum AncillaryTerminateCaptions'));
}

/// The anti-alias filter is automatically applied to all outputs. The service
/// no longer accepts the value DISABLED for AntiAlias. If you specify that in
/// your job, the service will ignore the setting.
enum AntiAlias {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const AntiAlias(this.value);

  static AntiAlias fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum AntiAlias'));
}

class AssociateCertificateResponse {
  AssociateCertificateResponse();

  factory AssociateCertificateResponse.fromJson(Map<String, dynamic> _) {
    return AssociateCertificateResponse();
  }

  Map<String, dynamic> toJson() {
    return {};
  }
}

/// Specify the QuickTime audio channel layout tags for the audio channels in
/// this audio track. Enter channel layout tags in the same order as your
/// output's audio channel order. For example, if your output audio track has a
/// left and a right channel, enter Left (L) for the first channel and Right (R)
/// for the second. If your output has multiple single-channel audio tracks,
/// enter a single channel layout tag for each track.
enum AudioChannelTag {
  l('L'),
  r('R'),
  c('C'),
  lfe('LFE'),
  ls('LS'),
  rs('RS'),
  lc('LC'),
  rc('RC'),
  cs('CS'),
  lsd('LSD'),
  rsd('RSD'),
  tcs('TCS'),
  vhl('VHL'),
  vhc('VHC'),
  vhr('VHR'),
  tbl('TBL'),
  tbc('TBC'),
  tbr('TBR'),
  rsl('RSL'),
  rsr('RSR'),
  lw('LW'),
  rw('RW'),
  lfe2('LFE2'),
  lt('LT'),
  rt('RT'),
  hi('HI'),
  nar('NAR'),
  m('M'),
  ;

  final String value;

  const AudioChannelTag(this.value);

  static AudioChannelTag fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum AudioChannelTag'));
}

/// Specify the QuickTime audio channel layout tags for the audio channels in
/// this audio track. When you don't specify a value, MediaConvert labels your
/// track as Center (C) by default. To use Audio layout tagging, your output
/// must be in a QuickTime (MOV) container and your audio codec must be AAC,
/// WAV, or AIFF.
class AudioChannelTaggingSettings {
  /// Specify the QuickTime audio channel layout tags for the audio channels in
  /// this audio track. Enter channel layout tags in the same order as your
  /// output's audio channel order. For example, if your output audio track has a
  /// left and a right channel, enter Left (L) for the first channel and Right (R)
  /// for the second. If your output has multiple single-channel audio tracks,
  /// enter a single channel layout tag for each track.
  final AudioChannelTag? channelTag;

  /// Specify the QuickTime audio channel layout tags for the audio channels in
  /// this audio track. Enter channel layout tags in the same order as your
  /// output's audio channel order. For example, if your output audio track has a
  /// left and a right channel, enter Left (L) for the first channel and Right (R)
  /// for the second. If your output has multiple single-channel audio tracks,
  /// enter a single channel layout tag for each track.
  final List<AudioChannelTag>? channelTags;

  AudioChannelTaggingSettings({
    this.channelTag,
    this.channelTags,
  });

  factory AudioChannelTaggingSettings.fromJson(Map<String, dynamic> json) {
    return AudioChannelTaggingSettings(
      channelTag:
          (json['channelTag'] as String?)?.let(AudioChannelTag.fromString),
      channelTags: (json['channelTags'] as List?)
          ?.nonNulls
          .map((e) => AudioChannelTag.fromString((e as String)))
          .toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final channelTag = this.channelTag;
    final channelTags = this.channelTags;
    return {
      if (channelTag != null) 'channelTag': channelTag.value,
      if (channelTags != null)
        'channelTags': channelTags.map((e) => e.value).toList(),
    };
  }
}

/// Choose the audio codec for this output. Note that the option Dolby Digital
/// passthrough applies only to Dolby Digital and Dolby Digital Plus audio
/// inputs. Make sure that you choose a codec that's supported with your output
/// container:
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/reference-codecs-containers.html#reference-codecs-containers-output-audio
/// For audio-only outputs, make sure that both your input audio codec and your
/// output audio codec are supported for audio-only workflows. For more
/// information, see:
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/reference-codecs-containers-input.html#reference-codecs-containers-input-audio-only
/// and
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/reference-codecs-containers.html#audio-only-output
enum AudioCodec {
  aac('AAC'),
  mp2('MP2'),
  mp3('MP3'),
  wav('WAV'),
  aiff('AIFF'),
  ac3('AC3'),
  eac3('EAC3'),
  eac3Atmos('EAC3_ATMOS'),
  vorbis('VORBIS'),
  opus('OPUS'),
  passthrough('PASSTHROUGH'),
  flac('FLAC'),
  ;

  final String value;

  const AudioCodec(this.value);

  static AudioCodec fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum AudioCodec'));
}

/// Settings related to audio encoding. The settings in this group vary
/// depending on the value that you choose for your audio codec.
class AudioCodecSettings {
  /// Required when you set Codec to the value AAC. The service accepts one of two
  /// mutually exclusive groups of AAC settings--VBR and CBR. To select one of
  /// these modes, set the value of Bitrate control mode to "VBR" or "CBR". In VBR
  /// mode, you control the audio quality with the setting VBR quality. In CBR
  /// mode, you use the setting Bitrate. Defaults and valid values depend on the
  /// rate control mode.
  final AacSettings? aacSettings;

  /// Required when you set Codec to the value AC3.
  final Ac3Settings? ac3Settings;

  /// Required when you set Codec to the value AIFF.
  final AiffSettings? aiffSettings;

  /// Choose the audio codec for this output. Note that the option Dolby Digital
  /// passthrough applies only to Dolby Digital and Dolby Digital Plus audio
  /// inputs. Make sure that you choose a codec that's supported with your output
  /// container:
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/reference-codecs-containers.html#reference-codecs-containers-output-audio
  /// For audio-only outputs, make sure that both your input audio codec and your
  /// output audio codec are supported for audio-only workflows. For more
  /// information, see:
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/reference-codecs-containers-input.html#reference-codecs-containers-input-audio-only
  /// and
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/reference-codecs-containers.html#audio-only-output
  final AudioCodec? codec;

  /// Required when you set Codec to the value EAC3_ATMOS.
  final Eac3AtmosSettings? eac3AtmosSettings;

  /// Required when you set Codec to the value EAC3.
  final Eac3Settings? eac3Settings;

  /// Required when you set Codec, under AudioDescriptions>CodecSettings, to the
  /// value FLAC.
  final FlacSettings? flacSettings;

  /// Required when you set Codec to the value MP2.
  final Mp2Settings? mp2Settings;

  /// Required when you set Codec, under AudioDescriptions>CodecSettings, to the
  /// value MP3.
  final Mp3Settings? mp3Settings;

  /// Required when you set Codec, under AudioDescriptions>CodecSettings, to the
  /// value OPUS.
  final OpusSettings? opusSettings;

  /// Required when you set Codec, under AudioDescriptions>CodecSettings, to the
  /// value Vorbis.
  final VorbisSettings? vorbisSettings;

  /// Required when you set Codec to the value WAV.
  final WavSettings? wavSettings;

  AudioCodecSettings({
    this.aacSettings,
    this.ac3Settings,
    this.aiffSettings,
    this.codec,
    this.eac3AtmosSettings,
    this.eac3Settings,
    this.flacSettings,
    this.mp2Settings,
    this.mp3Settings,
    this.opusSettings,
    this.vorbisSettings,
    this.wavSettings,
  });

  factory AudioCodecSettings.fromJson(Map<String, dynamic> json) {
    return AudioCodecSettings(
      aacSettings: json['aacSettings'] != null
          ? AacSettings.fromJson(json['aacSettings'] as Map<String, dynamic>)
          : null,
      ac3Settings: json['ac3Settings'] != null
          ? Ac3Settings.fromJson(json['ac3Settings'] as Map<String, dynamic>)
          : null,
      aiffSettings: json['aiffSettings'] != null
          ? AiffSettings.fromJson(json['aiffSettings'] as Map<String, dynamic>)
          : null,
      codec: (json['codec'] as String?)?.let(AudioCodec.fromString),
      eac3AtmosSettings: json['eac3AtmosSettings'] != null
          ? Eac3AtmosSettings.fromJson(
              json['eac3AtmosSettings'] as Map<String, dynamic>)
          : null,
      eac3Settings: json['eac3Settings'] != null
          ? Eac3Settings.fromJson(json['eac3Settings'] as Map<String, dynamic>)
          : null,
      flacSettings: json['flacSettings'] != null
          ? FlacSettings.fromJson(json['flacSettings'] as Map<String, dynamic>)
          : null,
      mp2Settings: json['mp2Settings'] != null
          ? Mp2Settings.fromJson(json['mp2Settings'] as Map<String, dynamic>)
          : null,
      mp3Settings: json['mp3Settings'] != null
          ? Mp3Settings.fromJson(json['mp3Settings'] as Map<String, dynamic>)
          : null,
      opusSettings: json['opusSettings'] != null
          ? OpusSettings.fromJson(json['opusSettings'] as Map<String, dynamic>)
          : null,
      vorbisSettings: json['vorbisSettings'] != null
          ? VorbisSettings.fromJson(
              json['vorbisSettings'] as Map<String, dynamic>)
          : null,
      wavSettings: json['wavSettings'] != null
          ? WavSettings.fromJson(json['wavSettings'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final aacSettings = this.aacSettings;
    final ac3Settings = this.ac3Settings;
    final aiffSettings = this.aiffSettings;
    final codec = this.codec;
    final eac3AtmosSettings = this.eac3AtmosSettings;
    final eac3Settings = this.eac3Settings;
    final flacSettings = this.flacSettings;
    final mp2Settings = this.mp2Settings;
    final mp3Settings = this.mp3Settings;
    final opusSettings = this.opusSettings;
    final vorbisSettings = this.vorbisSettings;
    final wavSettings = this.wavSettings;
    return {
      if (aacSettings != null) 'aacSettings': aacSettings,
      if (ac3Settings != null) 'ac3Settings': ac3Settings,
      if (aiffSettings != null) 'aiffSettings': aiffSettings,
      if (codec != null) 'codec': codec.value,
      if (eac3AtmosSettings != null) 'eac3AtmosSettings': eac3AtmosSettings,
      if (eac3Settings != null) 'eac3Settings': eac3Settings,
      if (flacSettings != null) 'flacSettings': flacSettings,
      if (mp2Settings != null) 'mp2Settings': mp2Settings,
      if (mp3Settings != null) 'mp3Settings': mp3Settings,
      if (opusSettings != null) 'opusSettings': opusSettings,
      if (vorbisSettings != null) 'vorbisSettings': vorbisSettings,
      if (wavSettings != null) 'wavSettings': wavSettings,
    };
  }
}

/// Enable this setting on one audio selector to set it as the default for the
/// job. The service uses this default for outputs where it can't find the
/// specified input audio. If you don't set a default, those outputs have no
/// audio.
enum AudioDefaultSelection {
  $default('DEFAULT'),
  notDefault('NOT_DEFAULT'),
  ;

  final String value;

  const AudioDefaultSelection(this.value);

  static AudioDefaultSelection fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum AudioDefaultSelection'));
}

/// Settings related to one audio tab on the MediaConvert console. In your job
/// JSON, an instance of AudioDescription is equivalent to one audio tab in the
/// console. Usually, one audio tab corresponds to one output audio track.
/// Depending on how you set up your input audio selectors and whether you use
/// audio selector groups, one audio tab can correspond to a group of output
/// audio tracks.
class AudioDescription {
  /// Specify the QuickTime audio channel layout tags for the audio channels in
  /// this audio track. When you don't specify a value, MediaConvert labels your
  /// track as Center (C) by default. To use Audio layout tagging, your output
  /// must be in a QuickTime (MOV) container and your audio codec must be AAC,
  /// WAV, or AIFF.
  final AudioChannelTaggingSettings? audioChannelTaggingSettings;

  /// Advanced audio normalization settings. Ignore these settings unless you need
  /// to comply with a loudness standard.
  final AudioNormalizationSettings? audioNormalizationSettings;

  /// Specifies which audio data to use from each input. In the simplest case,
  /// specify an "Audio Selector":#inputs-audio_selector by name based on its
  /// order within each input. For example if you specify "Audio Selector 3", then
  /// the third audio selector will be used from each input. If an input does not
  /// have an "Audio Selector 3", then the audio selector marked as "default" in
  /// that input will be used. If there is no audio selector marked as "default",
  /// silence will be inserted for the duration of that input. Alternatively, an
  /// "Audio Selector Group":#inputs-audio_selector_group name may be specified,
  /// with similar default/silence behavior. If no audio_source_name is specified,
  /// then "Audio Selector 1" will be chosen automatically.
  final String? audioSourceName;

  /// Applies only if Follow Input Audio Type is unchecked (false). A number
  /// between 0 and 255. The following are defined in ISO-IEC 13818-1: 0 =
  /// Undefined, 1 = Clean Effects, 2 = Hearing Impaired, 3 = Visually Impaired
  /// Commentary, 4-255 = Reserved.
  final int? audioType;

  /// When set to FOLLOW_INPUT, if the input contains an ISO 639 audio_type, then
  /// that value is passed through to the output. If the input contains no ISO 639
  /// audio_type, the value in Audio Type is included in the output. Otherwise the
  /// value in Audio Type is included in the output. Note that this field and
  /// audioType are both ignored if audioDescriptionBroadcasterMix is set to
  /// BROADCASTER_MIXED_AD.
  final AudioTypeControl? audioTypeControl;

  /// Settings related to audio encoding. The settings in this group vary
  /// depending on the value that you choose for your audio codec.
  final AudioCodecSettings? codecSettings;

  /// Specify the language for this audio output track. The service puts this
  /// language code into your output audio track when you set Language code
  /// control to Use configured. The service also uses your specified custom
  /// language code when you set Language code control to Follow input, but your
  /// input file doesn't specify a language code. For all outputs, you can use an
  /// ISO 639-2 or ISO 639-3 code. For streaming outputs, you can also use any
  /// other code in the full RFC-5646 specification. Streaming outputs are those
  /// that are in one of the following output groups: CMAF, DASH ISO, Apple HLS,
  /// or Microsoft Smooth Streaming.
  final String? customLanguageCode;

  /// Indicates the language of the audio output track. The ISO 639 language
  /// specified in the 'Language Code' drop down will be used when 'Follow Input
  /// Language Code' is not selected or when 'Follow Input Language Code' is
  /// selected but there is no ISO 639 language code specified by the input.
  final LanguageCode? languageCode;

  /// Specify which source for language code takes precedence for this audio
  /// track. When you choose Follow input, the service uses the language code from
  /// the input track if it's present. If there's no languge code on the input
  /// track, the service uses the code that you specify in the setting Language
  /// code. When you choose Use configured, the service uses the language code
  /// that you specify.
  final AudioLanguageCodeControl? languageCodeControl;

  /// Advanced audio remixing settings.
  final RemixSettings? remixSettings;

  /// Specify a label for this output audio stream. For example, "English",
  /// "Director commentary", or "track_2". For streaming outputs, MediaConvert
  /// passes this information into destination manifests for display on the
  /// end-viewer's player device. For outputs in other output groups, the service
  /// ignores this setting.
  final String? streamName;

  AudioDescription({
    this.audioChannelTaggingSettings,
    this.audioNormalizationSettings,
    this.audioSourceName,
    this.audioType,
    this.audioTypeControl,
    this.codecSettings,
    this.customLanguageCode,
    this.languageCode,
    this.languageCodeControl,
    this.remixSettings,
    this.streamName,
  });

  factory AudioDescription.fromJson(Map<String, dynamic> json) {
    return AudioDescription(
      audioChannelTaggingSettings: json['audioChannelTaggingSettings'] != null
          ? AudioChannelTaggingSettings.fromJson(
              json['audioChannelTaggingSettings'] as Map<String, dynamic>)
          : null,
      audioNormalizationSettings: json['audioNormalizationSettings'] != null
          ? AudioNormalizationSettings.fromJson(
              json['audioNormalizationSettings'] as Map<String, dynamic>)
          : null,
      audioSourceName: json['audioSourceName'] as String?,
      audioType: json['audioType'] as int?,
      audioTypeControl: (json['audioTypeControl'] as String?)
          ?.let(AudioTypeControl.fromString),
      codecSettings: json['codecSettings'] != null
          ? AudioCodecSettings.fromJson(
              json['codecSettings'] as Map<String, dynamic>)
          : null,
      customLanguageCode: json['customLanguageCode'] as String?,
      languageCode:
          (json['languageCode'] as String?)?.let(LanguageCode.fromString),
      languageCodeControl: (json['languageCodeControl'] as String?)
          ?.let(AudioLanguageCodeControl.fromString),
      remixSettings: json['remixSettings'] != null
          ? RemixSettings.fromJson(
              json['remixSettings'] as Map<String, dynamic>)
          : null,
      streamName: json['streamName'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final audioChannelTaggingSettings = this.audioChannelTaggingSettings;
    final audioNormalizationSettings = this.audioNormalizationSettings;
    final audioSourceName = this.audioSourceName;
    final audioType = this.audioType;
    final audioTypeControl = this.audioTypeControl;
    final codecSettings = this.codecSettings;
    final customLanguageCode = this.customLanguageCode;
    final languageCode = this.languageCode;
    final languageCodeControl = this.languageCodeControl;
    final remixSettings = this.remixSettings;
    final streamName = this.streamName;
    return {
      if (audioChannelTaggingSettings != null)
        'audioChannelTaggingSettings': audioChannelTaggingSettings,
      if (audioNormalizationSettings != null)
        'audioNormalizationSettings': audioNormalizationSettings,
      if (audioSourceName != null) 'audioSourceName': audioSourceName,
      if (audioType != null) 'audioType': audioType,
      if (audioTypeControl != null) 'audioTypeControl': audioTypeControl.value,
      if (codecSettings != null) 'codecSettings': codecSettings,
      if (customLanguageCode != null) 'customLanguageCode': customLanguageCode,
      if (languageCode != null) 'languageCode': languageCode.value,
      if (languageCodeControl != null)
        'languageCodeControl': languageCodeControl.value,
      if (remixSettings != null) 'remixSettings': remixSettings,
      if (streamName != null) 'streamName': streamName,
    };
  }
}

/// Apply audio timing corrections to help synchronize audio and video in your
/// output. To apply timing corrections, your input must meet the following
/// requirements: * Container: MP4, or MOV, with an accurate time-to-sample
/// (STTS) table. * Audio track: AAC. Choose from the following audio timing
/// correction settings: * Disabled (Default): Apply no correction. * Auto:
/// Recommended for most inputs. MediaConvert analyzes the audio timing in your
/// input and determines which correction setting to use, if needed. * Track:
/// Adjust the duration of each audio frame by a constant amount to align the
/// audio track length with STTS duration. Track-level correction does not
/// affect pitch, and is recommended for tonal audio content such as music. *
/// Frame: Adjust the duration of each audio frame by a variable amount to align
/// audio frames with STTS timestamps. No corrections are made to
/// already-aligned frames. Frame-level correction may affect the pitch of
/// corrected frames, and is recommended for atonal audio content such as speech
/// or percussion.
enum AudioDurationCorrection {
  disabled('DISABLED'),
  auto('AUTO'),
  track('TRACK'),
  frame('FRAME'),
  ;

  final String value;

  const AudioDurationCorrection(this.value);

  static AudioDurationCorrection fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum AudioDurationCorrection'));
}

/// Specify which source for language code takes precedence for this audio
/// track. When you choose Follow input, the service uses the language code from
/// the input track if it's present. If there's no languge code on the input
/// track, the service uses the code that you specify in the setting Language
/// code. When you choose Use configured, the service uses the language code
/// that you specify.
enum AudioLanguageCodeControl {
  followInput('FOLLOW_INPUT'),
  useConfigured('USE_CONFIGURED'),
  ;

  final String value;

  const AudioLanguageCodeControl(this.value);

  static AudioLanguageCodeControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum AudioLanguageCodeControl'));
}

/// Choose one of the following audio normalization algorithms: ITU-R BS.1770-1:
/// Ungated loudness. A measurement of ungated average loudness for an entire
/// piece of content, suitable for measurement of short-form content under ATSC
/// recommendation A/85. Supports up to 5.1 audio channels. ITU-R BS.1770-2:
/// Gated loudness. A measurement of gated average loudness compliant with the
/// requirements of EBU-R128. Supports up to 5.1 audio channels. ITU-R
/// BS.1770-3: Modified peak. The same loudness measurement algorithm as 1770-2,
/// with an updated true peak measurement. ITU-R BS.1770-4: Higher channel
/// count. Allows for more audio channels than the other algorithms, including
/// configurations such as 7.1.
enum AudioNormalizationAlgorithm {
  ituBs_1770_1('ITU_BS_1770_1'),
  ituBs_1770_2('ITU_BS_1770_2'),
  ituBs_1770_3('ITU_BS_1770_3'),
  ituBs_1770_4('ITU_BS_1770_4'),
  ;

  final String value;

  const AudioNormalizationAlgorithm(this.value);

  static AudioNormalizationAlgorithm fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum AudioNormalizationAlgorithm'));
}

/// When enabled the output audio is corrected using the chosen algorithm. If
/// disabled, the audio will be measured but not adjusted.
enum AudioNormalizationAlgorithmControl {
  correctAudio('CORRECT_AUDIO'),
  measureOnly('MEASURE_ONLY'),
  ;

  final String value;

  const AudioNormalizationAlgorithmControl(this.value);

  static AudioNormalizationAlgorithmControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum AudioNormalizationAlgorithmControl'));
}

/// If set to LOG, log each output's audio track loudness to a CSV file.
enum AudioNormalizationLoudnessLogging {
  log('LOG'),
  dontLog('DONT_LOG'),
  ;

  final String value;

  const AudioNormalizationLoudnessLogging(this.value);

  static AudioNormalizationLoudnessLogging fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum AudioNormalizationLoudnessLogging'));
}

/// If set to TRUE_PEAK, calculate and log the TruePeak for each output's audio
/// track loudness.
enum AudioNormalizationPeakCalculation {
  truePeak('TRUE_PEAK'),
  none('NONE'),
  ;

  final String value;

  const AudioNormalizationPeakCalculation(this.value);

  static AudioNormalizationPeakCalculation fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum AudioNormalizationPeakCalculation'));
}

/// Advanced audio normalization settings. Ignore these settings unless you need
/// to comply with a loudness standard.
class AudioNormalizationSettings {
  /// Choose one of the following audio normalization algorithms: ITU-R BS.1770-1:
  /// Ungated loudness. A measurement of ungated average loudness for an entire
  /// piece of content, suitable for measurement of short-form content under ATSC
  /// recommendation A/85. Supports up to 5.1 audio channels. ITU-R BS.1770-2:
  /// Gated loudness. A measurement of gated average loudness compliant with the
  /// requirements of EBU-R128. Supports up to 5.1 audio channels. ITU-R
  /// BS.1770-3: Modified peak. The same loudness measurement algorithm as 1770-2,
  /// with an updated true peak measurement. ITU-R BS.1770-4: Higher channel
  /// count. Allows for more audio channels than the other algorithms, including
  /// configurations such as 7.1.
  final AudioNormalizationAlgorithm? algorithm;

  /// When enabled the output audio is corrected using the chosen algorithm. If
  /// disabled, the audio will be measured but not adjusted.
  final AudioNormalizationAlgorithmControl? algorithmControl;

  /// Content measuring above this level will be corrected to the target level.
  /// Content measuring below this level will not be corrected.
  final int? correctionGateLevel;

  /// If set to LOG, log each output's audio track loudness to a CSV file.
  final AudioNormalizationLoudnessLogging? loudnessLogging;

  /// If set to TRUE_PEAK, calculate and log the TruePeak for each output's audio
  /// track loudness.
  final AudioNormalizationPeakCalculation? peakCalculation;

  /// When you use Audio normalization, optionally use this setting to specify a
  /// target loudness. If you don't specify a value here, the encoder chooses a
  /// value for you, based on the algorithm that you choose for Algorithm. If you
  /// choose algorithm 1770-1, the encoder will choose -24 LKFS; otherwise, the
  /// encoder will choose -23 LKFS.
  final double? targetLkfs;

  /// Specify the True-peak limiter threshold in decibels relative to full scale
  /// (dBFS). The peak inter-audio sample loudness in your output will be limited
  /// to the value that you specify, without affecting the overall target LKFS.
  /// Enter a value from 0 to -8. Leave blank to use the default value 0.
  final double? truePeakLimiterThreshold;

  AudioNormalizationSettings({
    this.algorithm,
    this.algorithmControl,
    this.correctionGateLevel,
    this.loudnessLogging,
    this.peakCalculation,
    this.targetLkfs,
    this.truePeakLimiterThreshold,
  });

  factory AudioNormalizationSettings.fromJson(Map<String, dynamic> json) {
    return AudioNormalizationSettings(
      algorithm: (json['algorithm'] as String?)
          ?.let(AudioNormalizationAlgorithm.fromString),
      algorithmControl: (json['algorithmControl'] as String?)
          ?.let(AudioNormalizationAlgorithmControl.fromString),
      correctionGateLevel: json['correctionGateLevel'] as int?,
      loudnessLogging: (json['loudnessLogging'] as String?)
          ?.let(AudioNormalizationLoudnessLogging.fromString),
      peakCalculation: (json['peakCalculation'] as String?)
          ?.let(AudioNormalizationPeakCalculation.fromString),
      targetLkfs: json['targetLkfs'] as double?,
      truePeakLimiterThreshold: json['truePeakLimiterThreshold'] as double?,
    );
  }

  Map<String, dynamic> toJson() {
    final algorithm = this.algorithm;
    final algorithmControl = this.algorithmControl;
    final correctionGateLevel = this.correctionGateLevel;
    final loudnessLogging = this.loudnessLogging;
    final peakCalculation = this.peakCalculation;
    final targetLkfs = this.targetLkfs;
    final truePeakLimiterThreshold = this.truePeakLimiterThreshold;
    return {
      if (algorithm != null) 'algorithm': algorithm.value,
      if (algorithmControl != null) 'algorithmControl': algorithmControl.value,
      if (correctionGateLevel != null)
        'correctionGateLevel': correctionGateLevel,
      if (loudnessLogging != null) 'loudnessLogging': loudnessLogging.value,
      if (peakCalculation != null) 'peakCalculation': peakCalculation.value,
      if (targetLkfs != null) 'targetLkfs': targetLkfs,
      if (truePeakLimiterThreshold != null)
        'truePeakLimiterThreshold': truePeakLimiterThreshold,
    };
  }
}

/// Use Audio selectors to specify a track or set of tracks from the input that
/// you will use in your outputs. You can use multiple Audio selectors per
/// input.
class AudioSelector {
  /// Apply audio timing corrections to help synchronize audio and video in your
  /// output. To apply timing corrections, your input must meet the following
  /// requirements: * Container: MP4, or MOV, with an accurate time-to-sample
  /// (STTS) table. * Audio track: AAC. Choose from the following audio timing
  /// correction settings: * Disabled (Default): Apply no correction. * Auto:
  /// Recommended for most inputs. MediaConvert analyzes the audio timing in your
  /// input and determines which correction setting to use, if needed. * Track:
  /// Adjust the duration of each audio frame by a constant amount to align the
  /// audio track length with STTS duration. Track-level correction does not
  /// affect pitch, and is recommended for tonal audio content such as music. *
  /// Frame: Adjust the duration of each audio frame by a variable amount to align
  /// audio frames with STTS timestamps. No corrections are made to
  /// already-aligned frames. Frame-level correction may affect the pitch of
  /// corrected frames, and is recommended for atonal audio content such as speech
  /// or percussion.
  final AudioDurationCorrection? audioDurationCorrection;

  /// Selects a specific language code from within an audio source, using the ISO
  /// 639-2 or ISO 639-3 three-letter language code
  final String? customLanguageCode;

  /// Enable this setting on one audio selector to set it as the default for the
  /// job. The service uses this default for outputs where it can't find the
  /// specified input audio. If you don't set a default, those outputs have no
  /// audio.
  final AudioDefaultSelection? defaultSelection;

  /// Specifies audio data from an external file source.
  final String? externalAudioFileInput;

  /// Settings specific to audio sources in an HLS alternate rendition group.
  /// Specify the properties (renditionGroupId, renditionName or
  /// renditionLanguageCode) to identify the unique audio track among the
  /// alternative rendition groups present in the HLS manifest. If no unique track
  /// is found, or multiple tracks match the properties provided, the job fails.
  /// If no properties in hlsRenditionGroupSettings are specified, the default
  /// audio track within the video segment is chosen. If there is no audio within
  /// video segment, the alternative audio with DEFAULT=YES is chosen instead.
  final HlsRenditionGroupSettings? hlsRenditionGroupSettings;

  /// Selects a specific language code from within an audio source.
  final LanguageCode? languageCode;

  /// Specifies a time delta in milliseconds to offset the audio from the input
  /// video.
  final int? offset;

  /// Selects a specific PID from within an audio source (e.g. 257 selects PID
  /// 0x101).
  final List<int>? pids;

  /// Use this setting for input streams that contain Dolby E, to have the service
  /// extract specific program data from the track. To select multiple programs,
  /// create multiple selectors with the same Track and different Program numbers.
  /// In the console, this setting is visible when you set Selector type to Track.
  /// Choose the program number from the dropdown list. If your input file has
  /// incorrect metadata, you can choose All channels instead of a program number
  /// to have the service ignore the program IDs and include all the programs in
  /// the track.
  final int? programSelection;

  /// Use these settings to reorder the audio channels of one input to match those
  /// of another input. This allows you to combine the two files into a single
  /// output, one after the other.
  final RemixSettings? remixSettings;

  /// Specifies the type of the audio selector.
  final AudioSelectorType? selectorType;

  /// Identify a track from the input audio to include in this selector by
  /// entering the track index number. To include several tracks in a single audio
  /// selector, specify multiple tracks as follows. Using the console, enter a
  /// comma-separated list. For example, type "1,2,3" to include tracks 1 through
  /// 3.
  final List<int>? tracks;

  AudioSelector({
    this.audioDurationCorrection,
    this.customLanguageCode,
    this.defaultSelection,
    this.externalAudioFileInput,
    this.hlsRenditionGroupSettings,
    this.languageCode,
    this.offset,
    this.pids,
    this.programSelection,
    this.remixSettings,
    this.selectorType,
    this.tracks,
  });

  factory AudioSelector.fromJson(Map<String, dynamic> json) {
    return AudioSelector(
      audioDurationCorrection: (json['audioDurationCorrection'] as String?)
          ?.let(AudioDurationCorrection.fromString),
      customLanguageCode: json['customLanguageCode'] as String?,
      defaultSelection: (json['defaultSelection'] as String?)
          ?.let(AudioDefaultSelection.fromString),
      externalAudioFileInput: json['externalAudioFileInput'] as String?,
      hlsRenditionGroupSettings: json['hlsRenditionGroupSettings'] != null
          ? HlsRenditionGroupSettings.fromJson(
              json['hlsRenditionGroupSettings'] as Map<String, dynamic>)
          : null,
      languageCode:
          (json['languageCode'] as String?)?.let(LanguageCode.fromString),
      offset: json['offset'] as int?,
      pids: (json['pids'] as List?)?.nonNulls.map((e) => e as int).toList(),
      programSelection: json['programSelection'] as int?,
      remixSettings: json['remixSettings'] != null
          ? RemixSettings.fromJson(
              json['remixSettings'] as Map<String, dynamic>)
          : null,
      selectorType:
          (json['selectorType'] as String?)?.let(AudioSelectorType.fromString),
      tracks: (json['tracks'] as List?)?.nonNulls.map((e) => e as int).toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final audioDurationCorrection = this.audioDurationCorrection;
    final customLanguageCode = this.customLanguageCode;
    final defaultSelection = this.defaultSelection;
    final externalAudioFileInput = this.externalAudioFileInput;
    final hlsRenditionGroupSettings = this.hlsRenditionGroupSettings;
    final languageCode = this.languageCode;
    final offset = this.offset;
    final pids = this.pids;
    final programSelection = this.programSelection;
    final remixSettings = this.remixSettings;
    final selectorType = this.selectorType;
    final tracks = this.tracks;
    return {
      if (audioDurationCorrection != null)
        'audioDurationCorrection': audioDurationCorrection.value,
      if (customLanguageCode != null) 'customLanguageCode': customLanguageCode,
      if (defaultSelection != null) 'defaultSelection': defaultSelection.value,
      if (externalAudioFileInput != null)
        'externalAudioFileInput': externalAudioFileInput,
      if (hlsRenditionGroupSettings != null)
        'hlsRenditionGroupSettings': hlsRenditionGroupSettings,
      if (languageCode != null) 'languageCode': languageCode.value,
      if (offset != null) 'offset': offset,
      if (pids != null) 'pids': pids,
      if (programSelection != null) 'programSelection': programSelection,
      if (remixSettings != null) 'remixSettings': remixSettings,
      if (selectorType != null) 'selectorType': selectorType.value,
      if (tracks != null) 'tracks': tracks,
    };
  }
}

/// Use audio selector groups to combine multiple sidecar audio inputs so that
/// you can assign them to a single output audio tab. Note that, if you're
/// working with embedded audio, it's simpler to assign multiple input tracks
/// into a single audio selector rather than use an audio selector group.
class AudioSelectorGroup {
  /// Name of an Audio Selector within the same input to include in the group.
  /// Audio selector names are standardized, based on their order within the input
  /// (e.g., "Audio Selector 1"). The audio selector name parameter can be
  /// repeated to add any number of audio selectors to the group.
  final List<String>? audioSelectorNames;

  AudioSelectorGroup({
    this.audioSelectorNames,
  });

  factory AudioSelectorGroup.fromJson(Map<String, dynamic> json) {
    return AudioSelectorGroup(
      audioSelectorNames: (json['audioSelectorNames'] as List?)
          ?.nonNulls
          .map((e) => e as String)
          .toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final audioSelectorNames = this.audioSelectorNames;
    return {
      if (audioSelectorNames != null) 'audioSelectorNames': audioSelectorNames,
    };
  }
}

/// Specifies the type of the audio selector.
enum AudioSelectorType {
  pid('PID'),
  track('TRACK'),
  languageCode('LANGUAGE_CODE'),
  hlsRenditionGroup('HLS_RENDITION_GROUP'),
  ;

  final String value;

  const AudioSelectorType(this.value);

  static AudioSelectorType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum AudioSelectorType'));
}

/// When set to FOLLOW_INPUT, if the input contains an ISO 639 audio_type, then
/// that value is passed through to the output. If the input contains no ISO 639
/// audio_type, the value in Audio Type is included in the output. Otherwise the
/// value in Audio Type is included in the output. Note that this field and
/// audioType are both ignored if audioDescriptionBroadcasterMix is set to
/// BROADCASTER_MIXED_AD.
enum AudioTypeControl {
  followInput('FOLLOW_INPUT'),
  useConfigured('USE_CONFIGURED'),
  ;

  final String value;

  const AudioTypeControl(this.value);

  static AudioTypeControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum AudioTypeControl'));
}

/// Specify one or more Automated ABR rule types. Note: Force include and
/// Allowed renditions are mutually exclusive.
class AutomatedAbrRule {
  /// When customer adds the allowed renditions rule for auto ABR ladder, they are
  /// required to add at leat one rendition to allowedRenditions list
  final List<AllowedRenditionSize>? allowedRenditions;

  /// When customer adds the force include renditions rule for auto ABR ladder,
  /// they are required to add at leat one rendition to forceIncludeRenditions
  /// list
  final List<ForceIncludeRenditionSize>? forceIncludeRenditions;

  /// Use Min bottom rendition size to specify a minimum size for the lowest
  /// resolution in your ABR stack. * The lowest resolution in your ABR stack will
  /// be equal to or greater than the value that you enter. For example: If you
  /// specify 640x360 the lowest resolution in your ABR stack will be equal to or
  /// greater than to 640x360. * If you specify a Min top rendition size rule, the
  /// value that you specify for Min bottom rendition size must be less than, or
  /// equal to, Min top rendition size.
  final MinBottomRenditionSize? minBottomRenditionSize;

  /// Use Min top rendition size to specify a minimum size for the highest
  /// resolution in your ABR stack. * The highest resolution in your ABR stack
  /// will be equal to or greater than the value that you enter. For example: If
  /// you specify 1280x720 the highest resolution in your ABR stack will be equal
  /// to or greater than 1280x720. * If you specify a value for Max resolution,
  /// the value that you specify for Min top rendition size must be less than, or
  /// equal to, Max resolution.
  final MinTopRenditionSize? minTopRenditionSize;

  /// Use Min top rendition size to specify a minimum size for the highest
  /// resolution in your ABR stack. * The highest resolution in your ABR stack
  /// will be equal to or greater than the value that you enter. For example: If
  /// you specify 1280x720 the highest resolution in your ABR stack will be equal
  /// to or greater than 1280x720. * If you specify a value for Max resolution,
  /// the value that you specify for Min top rendition size must be less than, or
  /// equal to, Max resolution. Use Min bottom rendition size to specify a minimum
  /// size for the lowest resolution in your ABR stack. * The lowest resolution in
  /// your ABR stack will be equal to or greater than the value that you enter.
  /// For example: If you specify 640x360 the lowest resolution in your ABR stack
  /// will be equal to or greater than to 640x360. * If you specify a Min top
  /// rendition size rule, the value that you specify for Min bottom rendition
  /// size must be less than, or equal to, Min top rendition size. Use Force
  /// include renditions to specify one or more resolutions to include your ABR
  /// stack. * (Recommended) To optimize automated ABR, specify as few resolutions
  /// as possible. * (Required) The number of resolutions that you specify must be
  /// equal to, or less than, the Max renditions setting. * If you specify a Min
  /// top rendition size rule, specify at least one resolution that is equal to,
  /// or greater than, Min top rendition size. * If you specify a Min bottom
  /// rendition size rule, only specify resolutions that are equal to, or greater
  /// than, Min bottom rendition size. * If you specify a Force include renditions
  /// rule, do not specify a separate rule for Allowed renditions. * Note: The ABR
  /// stack may include other resolutions that you do not specify here, depending
  /// on the Max renditions setting. Use Allowed renditions to specify a list of
  /// possible resolutions in your ABR stack. * (Required) The number of
  /// resolutions that you specify must be equal to, or greater than, the Max
  /// renditions setting. * MediaConvert will create an ABR stack exclusively from
  /// the list of resolutions that you specify. * Some resolutions in the Allowed
  /// renditions list may not be included, however you can force a resolution to
  /// be included by setting Required to ENABLED. * You must specify at least one
  /// resolution that is greater than or equal to any resolutions that you specify
  /// in Min top rendition size or Min bottom rendition size. * If you specify
  /// Allowed renditions, you must not specify a separate rule for Force include
  /// renditions.
  final RuleType? type;

  AutomatedAbrRule({
    this.allowedRenditions,
    this.forceIncludeRenditions,
    this.minBottomRenditionSize,
    this.minTopRenditionSize,
    this.type,
  });

  factory AutomatedAbrRule.fromJson(Map<String, dynamic> json) {
    return AutomatedAbrRule(
      allowedRenditions: (json['allowedRenditions'] as List?)
          ?.nonNulls
          .map((e) => AllowedRenditionSize.fromJson(e as Map<String, dynamic>))
          .toList(),
      forceIncludeRenditions: (json['forceIncludeRenditions'] as List?)
          ?.nonNulls
          .map((e) =>
              ForceIncludeRenditionSize.fromJson(e as Map<String, dynamic>))
          .toList(),
      minBottomRenditionSize: json['minBottomRenditionSize'] != null
          ? MinBottomRenditionSize.fromJson(
              json['minBottomRenditionSize'] as Map<String, dynamic>)
          : null,
      minTopRenditionSize: json['minTopRenditionSize'] != null
          ? MinTopRenditionSize.fromJson(
              json['minTopRenditionSize'] as Map<String, dynamic>)
          : null,
      type: (json['type'] as String?)?.let(RuleType.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final allowedRenditions = this.allowedRenditions;
    final forceIncludeRenditions = this.forceIncludeRenditions;
    final minBottomRenditionSize = this.minBottomRenditionSize;
    final minTopRenditionSize = this.minTopRenditionSize;
    final type = this.type;
    return {
      if (allowedRenditions != null) 'allowedRenditions': allowedRenditions,
      if (forceIncludeRenditions != null)
        'forceIncludeRenditions': forceIncludeRenditions,
      if (minBottomRenditionSize != null)
        'minBottomRenditionSize': minBottomRenditionSize,
      if (minTopRenditionSize != null)
        'minTopRenditionSize': minTopRenditionSize,
      if (type != null) 'type': type.value,
    };
  }
}

/// Use automated ABR to have MediaConvert set up the renditions in your ABR
/// package for you automatically, based on characteristics of your input video.
/// This feature optimizes video quality while minimizing the overall size of
/// your ABR package.
class AutomatedAbrSettings {
  /// Specify the maximum average bitrate for MediaConvert to use in your
  /// automated ABR stack. If you don't specify a value, MediaConvert uses
  /// 8,000,000 (8 mb/s) by default. The average bitrate of your highest-quality
  /// rendition will be equal to or below this value, depending on the quality,
  /// complexity, and resolution of your content. Note that the instantaneous
  /// maximum bitrate may vary above the value that you specify.
  final int? maxAbrBitrate;

  /// Optional. The maximum number of renditions that MediaConvert will create in
  /// your automated ABR stack. The number of renditions is determined
  /// automatically, based on analysis of each job, but will never exceed this
  /// limit. When you set this to Auto in the console, which is equivalent to
  /// excluding it from your JSON job specification, MediaConvert defaults to a
  /// limit of 15.
  final int? maxRenditions;

  /// Specify the minimum average bitrate for MediaConvert to use in your
  /// automated ABR stack. If you don't specify a value, MediaConvert uses 600,000
  /// (600 kb/s) by default. The average bitrate of your lowest-quality rendition
  /// will be near this value. Note that the instantaneous minimum bitrate may
  /// vary below the value that you specify.
  final int? minAbrBitrate;

  /// Optional. Use Automated ABR rules to specify restrictions for the rendition
  /// sizes MediaConvert will create in your ABR stack. You can use these rules if
  /// your ABR workflow has specific rendition size requirements, but you still
  /// want MediaConvert to optimize for video quality and overall file size.
  final List<AutomatedAbrRule>? rules;

  AutomatedAbrSettings({
    this.maxAbrBitrate,
    this.maxRenditions,
    this.minAbrBitrate,
    this.rules,
  });

  factory AutomatedAbrSettings.fromJson(Map<String, dynamic> json) {
    return AutomatedAbrSettings(
      maxAbrBitrate: json['maxAbrBitrate'] as int?,
      maxRenditions: json['maxRenditions'] as int?,
      minAbrBitrate: json['minAbrBitrate'] as int?,
      rules: (json['rules'] as List?)
          ?.nonNulls
          .map((e) => AutomatedAbrRule.fromJson(e as Map<String, dynamic>))
          .toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final maxAbrBitrate = this.maxAbrBitrate;
    final maxRenditions = this.maxRenditions;
    final minAbrBitrate = this.minAbrBitrate;
    final rules = this.rules;
    return {
      if (maxAbrBitrate != null) 'maxAbrBitrate': maxAbrBitrate,
      if (maxRenditions != null) 'maxRenditions': maxRenditions,
      if (minAbrBitrate != null) 'minAbrBitrate': minAbrBitrate,
      if (rules != null) 'rules': rules,
    };
  }
}

/// Use automated encoding to have MediaConvert choose your encoding settings
/// for you, based on characteristics of your input video.
class AutomatedEncodingSettings {
  /// Use automated ABR to have MediaConvert set up the renditions in your ABR
  /// package for you automatically, based on characteristics of your input video.
  /// This feature optimizes video quality while minimizing the overall size of
  /// your ABR package.
  final AutomatedAbrSettings? abrSettings;

  AutomatedEncodingSettings({
    this.abrSettings,
  });

  factory AutomatedEncodingSettings.fromJson(Map<String, dynamic> json) {
    return AutomatedEncodingSettings(
      abrSettings: json['abrSettings'] != null
          ? AutomatedAbrSettings.fromJson(
              json['abrSettings'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final abrSettings = this.abrSettings;
    return {
      if (abrSettings != null) 'abrSettings': abrSettings,
    };
  }
}

/// Specify the strength of any adaptive quantization filters that you enable.
/// The value that you choose here applies to Spatial adaptive quantization.
enum Av1AdaptiveQuantization {
  off('OFF'),
  low('LOW'),
  medium('MEDIUM'),
  high('HIGH'),
  higher('HIGHER'),
  max('MAX'),
  ;

  final String value;

  const Av1AdaptiveQuantization(this.value);

  static Av1AdaptiveQuantization fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Av1AdaptiveQuantization'));
}

/// Specify the Bit depth. You can choose 8-bit or 10-bit.
enum Av1BitDepth {
  bit_8('BIT_8'),
  bit_10('BIT_10'),
  ;

  final String value;

  const Av1BitDepth(this.value);

  static Av1BitDepth fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum Av1BitDepth'));
}

/// Film grain synthesis replaces film grain present in your content with
/// similar quality synthesized AV1 film grain. We recommend that you choose
/// Enabled to reduce the bandwidth of your QVBR quality level 5, 6, 7, or 8
/// outputs. For QVBR quality level 9 or 10 outputs we recommend that you keep
/// the default value, Disabled. When you include Film grain synthesis, you
/// cannot include the Noise reducer preprocessor.
enum Av1FilmGrainSynthesis {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const Av1FilmGrainSynthesis(this.value);

  static Av1FilmGrainSynthesis fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Av1FilmGrainSynthesis'));
}

/// Use the Framerate setting to specify the frame rate for this output. If you
/// want to keep the same frame rate as the input video, choose Follow source.
/// If you want to do frame rate conversion, choose a frame rate from the
/// dropdown list or choose Custom. The framerates shown in the dropdown list
/// are decimal approximations of fractions. If you choose Custom, specify your
/// frame rate as a fraction.
enum Av1FramerateControl {
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  specified('SPECIFIED'),
  ;

  final String value;

  const Av1FramerateControl(this.value);

  static Av1FramerateControl fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Av1FramerateControl'));
}

/// Choose the method that you want MediaConvert to use when increasing or
/// decreasing the frame rate. For numerically simple conversions, such as 60
/// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
/// For numerically complex conversions, to avoid stutter: Choose Interpolate.
/// This results in a smooth picture, but might introduce undesirable video
/// artifacts. For complex frame rate conversions, especially if your source
/// video has already been converted from its original cadence: Choose
/// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
/// best conversion method frame by frame. Note that using FrameFormer increases
/// the transcoding time and incurs a significant add-on cost. When you choose
/// FrameFormer, your input video resolution must be at least 128x96.
enum Av1FramerateConversionAlgorithm {
  duplicateDrop('DUPLICATE_DROP'),
  interpolate('INTERPOLATE'),
  frameformer('FRAMEFORMER'),
  ;

  final String value;

  const Av1FramerateConversionAlgorithm(this.value);

  static Av1FramerateConversionAlgorithm fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Av1FramerateConversionAlgorithm'));
}

/// Settings for quality-defined variable bitrate encoding with the AV1 codec.
/// Use these settings only when you set QVBR for Rate control mode.
class Av1QvbrSettings {
  /// Use this setting only when you set Rate control mode to QVBR. Specify the
  /// target quality level for this output. MediaConvert determines the right
  /// number of bits to use for each part of the video to maintain the video
  /// quality that you specify. When you keep the default value, AUTO,
  /// MediaConvert picks a quality level for you, based on characteristics of your
  /// input video. If you prefer to specify a quality level, specify a number from
  /// 1 through 10. Use higher numbers for greater quality. Level 10 results in
  /// nearly lossless compression. The quality level for most broadcast-quality
  /// transcodes is between 6 and 9. Optionally, to specify a value between whole
  /// numbers, also provide a value for the setting qvbrQualityLevelFineTune. For
  /// example, if you want your QVBR quality level to be 7.33, set
  /// qvbrQualityLevel to 7 and set qvbrQualityLevelFineTune to .33.
  final int? qvbrQualityLevel;

  /// Optional. Specify a value here to set the QVBR quality to a level that is
  /// between whole numbers. For example, if you want your QVBR quality level to
  /// be 7.33, set qvbrQualityLevel to 7 and set qvbrQualityLevelFineTune to .33.
  /// MediaConvert rounds your QVBR quality level to the nearest third of a whole
  /// number. For example, if you set qvbrQualityLevel to 7 and you set
  /// qvbrQualityLevelFineTune to .25, your actual QVBR quality level is 7.33.
  final double? qvbrQualityLevelFineTune;

  Av1QvbrSettings({
    this.qvbrQualityLevel,
    this.qvbrQualityLevelFineTune,
  });

  factory Av1QvbrSettings.fromJson(Map<String, dynamic> json) {
    return Av1QvbrSettings(
      qvbrQualityLevel: json['qvbrQualityLevel'] as int?,
      qvbrQualityLevelFineTune: json['qvbrQualityLevelFineTune'] as double?,
    );
  }

  Map<String, dynamic> toJson() {
    final qvbrQualityLevel = this.qvbrQualityLevel;
    final qvbrQualityLevelFineTune = this.qvbrQualityLevelFineTune;
    return {
      if (qvbrQualityLevel != null) 'qvbrQualityLevel': qvbrQualityLevel,
      if (qvbrQualityLevelFineTune != null)
        'qvbrQualityLevelFineTune': qvbrQualityLevelFineTune,
    };
  }
}

/// 'With AV1 outputs, for rate control mode, MediaConvert supports only
/// quality-defined variable bitrate (QVBR). You can''t use CBR or VBR.'
enum Av1RateControlMode {
  qvbr('QVBR'),
  ;

  final String value;

  const Av1RateControlMode(this.value);

  static Av1RateControlMode fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Av1RateControlMode'));
}

/// Required when you set Codec, under VideoDescription>CodecSettings to the
/// value AV1.
class Av1Settings {
  /// Specify the strength of any adaptive quantization filters that you enable.
  /// The value that you choose here applies to Spatial adaptive quantization.
  final Av1AdaptiveQuantization? adaptiveQuantization;

  /// Specify the Bit depth. You can choose 8-bit or 10-bit.
  final Av1BitDepth? bitDepth;

  /// Film grain synthesis replaces film grain present in your content with
  /// similar quality synthesized AV1 film grain. We recommend that you choose
  /// Enabled to reduce the bandwidth of your QVBR quality level 5, 6, 7, or 8
  /// outputs. For QVBR quality level 9 or 10 outputs we recommend that you keep
  /// the default value, Disabled. When you include Film grain synthesis, you
  /// cannot include the Noise reducer preprocessor.
  final Av1FilmGrainSynthesis? filmGrainSynthesis;

  /// Use the Framerate setting to specify the frame rate for this output. If you
  /// want to keep the same frame rate as the input video, choose Follow source.
  /// If you want to do frame rate conversion, choose a frame rate from the
  /// dropdown list or choose Custom. The framerates shown in the dropdown list
  /// are decimal approximations of fractions. If you choose Custom, specify your
  /// frame rate as a fraction.
  final Av1FramerateControl? framerateControl;

  /// Choose the method that you want MediaConvert to use when increasing or
  /// decreasing the frame rate. For numerically simple conversions, such as 60
  /// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
  /// For numerically complex conversions, to avoid stutter: Choose Interpolate.
  /// This results in a smooth picture, but might introduce undesirable video
  /// artifacts. For complex frame rate conversions, especially if your source
  /// video has already been converted from its original cadence: Choose
  /// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
  /// best conversion method frame by frame. Note that using FrameFormer increases
  /// the transcoding time and incurs a significant add-on cost. When you choose
  /// FrameFormer, your input video resolution must be at least 128x96.
  final Av1FramerateConversionAlgorithm? framerateConversionAlgorithm;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateDenominator to specify the denominator of this fraction.
  /// In this example, use 1001 for the value of FramerateDenominator. When you
  /// use the console for transcode jobs that use frame rate conversion, provide
  /// the value as a decimal number for Framerate. In this example, specify
  /// 23.976.
  final int? framerateDenominator;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateNumerator to specify the numerator of this fraction. In
  /// this example, use 24000 for the value of FramerateNumerator. When you use
  /// the console for transcode jobs that use frame rate conversion, provide the
  /// value as a decimal number for Framerate. In this example, specify 23.976.
  final int? framerateNumerator;

  /// Specify the GOP length (keyframe interval) in frames. With AV1, MediaConvert
  /// doesn't support GOP length in seconds. This value must be greater than zero
  /// and preferably equal to 1 + ((numberBFrames + 1) * x), where x is an integer
  /// value.
  final double? gopSize;

  /// Maximum bitrate in bits/second. For example, enter five megabits per second
  /// as 5000000. Required when Rate control mode is QVBR.
  final int? maxBitrate;

  /// Specify from the number of B-frames, in the range of 0-15. For AV1 encoding,
  /// we recommend using 7 or 15. Choose a larger number for a lower bitrate and
  /// smaller file size; choose a smaller number for better video quality.
  final int? numberBFramesBetweenReferenceFrames;

  /// Settings for quality-defined variable bitrate encoding with the H.265 codec.
  /// Use these settings only when you set QVBR for Rate control mode.
  final Av1QvbrSettings? qvbrSettings;

  /// 'With AV1 outputs, for rate control mode, MediaConvert supports only
  /// quality-defined variable bitrate (QVBR). You can''t use CBR or VBR.'
  final Av1RateControlMode? rateControlMode;

  /// Specify the number of slices per picture. This value must be 1, 2, 4, 8, 16,
  /// or 32. For progressive pictures, this value must be less than or equal to
  /// the number of macroblock rows. For interlaced pictures, this value must be
  /// less than or equal to half the number of macroblock rows.
  final int? slices;

  /// Keep the default value, Enabled, to adjust quantization within each frame
  /// based on spatial variation of content complexity. When you enable this
  /// feature, the encoder uses fewer bits on areas that can sustain more
  /// distortion with no noticeable visual degradation and uses more bits on areas
  /// where any small distortion will be noticeable. For example, complex textured
  /// blocks are encoded with fewer bits and smooth textured blocks are encoded
  /// with more bits. Enabling this feature will almost always improve your video
  /// quality. Note, though, that this feature doesn't take into account where the
  /// viewer's attention is likely to be. If viewers are likely to be focusing
  /// their attention on a part of the screen with a lot of complex texture, you
  /// might choose to disable this feature. Related setting: When you enable
  /// spatial adaptive quantization, set the value for Adaptive quantization
  /// depending on your content. For homogeneous content, such as cartoons and
  /// video games, set it to Low. For content with a wider variety of textures,
  /// set it to High or Higher.
  final Av1SpatialAdaptiveQuantization? spatialAdaptiveQuantization;

  Av1Settings({
    this.adaptiveQuantization,
    this.bitDepth,
    this.filmGrainSynthesis,
    this.framerateControl,
    this.framerateConversionAlgorithm,
    this.framerateDenominator,
    this.framerateNumerator,
    this.gopSize,
    this.maxBitrate,
    this.numberBFramesBetweenReferenceFrames,
    this.qvbrSettings,
    this.rateControlMode,
    this.slices,
    this.spatialAdaptiveQuantization,
  });

  factory Av1Settings.fromJson(Map<String, dynamic> json) {
    return Av1Settings(
      adaptiveQuantization: (json['adaptiveQuantization'] as String?)
          ?.let(Av1AdaptiveQuantization.fromString),
      bitDepth: (json['bitDepth'] as String?)?.let(Av1BitDepth.fromString),
      filmGrainSynthesis: (json['filmGrainSynthesis'] as String?)
          ?.let(Av1FilmGrainSynthesis.fromString),
      framerateControl: (json['framerateControl'] as String?)
          ?.let(Av1FramerateControl.fromString),
      framerateConversionAlgorithm:
          (json['framerateConversionAlgorithm'] as String?)
              ?.let(Av1FramerateConversionAlgorithm.fromString),
      framerateDenominator: json['framerateDenominator'] as int?,
      framerateNumerator: json['framerateNumerator'] as int?,
      gopSize: json['gopSize'] as double?,
      maxBitrate: json['maxBitrate'] as int?,
      numberBFramesBetweenReferenceFrames:
          json['numberBFramesBetweenReferenceFrames'] as int?,
      qvbrSettings: json['qvbrSettings'] != null
          ? Av1QvbrSettings.fromJson(
              json['qvbrSettings'] as Map<String, dynamic>)
          : null,
      rateControlMode: (json['rateControlMode'] as String?)
          ?.let(Av1RateControlMode.fromString),
      slices: json['slices'] as int?,
      spatialAdaptiveQuantization:
          (json['spatialAdaptiveQuantization'] as String?)
              ?.let(Av1SpatialAdaptiveQuantization.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final adaptiveQuantization = this.adaptiveQuantization;
    final bitDepth = this.bitDepth;
    final filmGrainSynthesis = this.filmGrainSynthesis;
    final framerateControl = this.framerateControl;
    final framerateConversionAlgorithm = this.framerateConversionAlgorithm;
    final framerateDenominator = this.framerateDenominator;
    final framerateNumerator = this.framerateNumerator;
    final gopSize = this.gopSize;
    final maxBitrate = this.maxBitrate;
    final numberBFramesBetweenReferenceFrames =
        this.numberBFramesBetweenReferenceFrames;
    final qvbrSettings = this.qvbrSettings;
    final rateControlMode = this.rateControlMode;
    final slices = this.slices;
    final spatialAdaptiveQuantization = this.spatialAdaptiveQuantization;
    return {
      if (adaptiveQuantization != null)
        'adaptiveQuantization': adaptiveQuantization.value,
      if (bitDepth != null) 'bitDepth': bitDepth.value,
      if (filmGrainSynthesis != null)
        'filmGrainSynthesis': filmGrainSynthesis.value,
      if (framerateControl != null) 'framerateControl': framerateControl.value,
      if (framerateConversionAlgorithm != null)
        'framerateConversionAlgorithm': framerateConversionAlgorithm.value,
      if (framerateDenominator != null)
        'framerateDenominator': framerateDenominator,
      if (framerateNumerator != null) 'framerateNumerator': framerateNumerator,
      if (gopSize != null) 'gopSize': gopSize,
      if (maxBitrate != null) 'maxBitrate': maxBitrate,
      if (numberBFramesBetweenReferenceFrames != null)
        'numberBFramesBetweenReferenceFrames':
            numberBFramesBetweenReferenceFrames,
      if (qvbrSettings != null) 'qvbrSettings': qvbrSettings,
      if (rateControlMode != null) 'rateControlMode': rateControlMode.value,
      if (slices != null) 'slices': slices,
      if (spatialAdaptiveQuantization != null)
        'spatialAdaptiveQuantization': spatialAdaptiveQuantization.value,
    };
  }
}

/// Keep the default value, Enabled, to adjust quantization within each frame
/// based on spatial variation of content complexity. When you enable this
/// feature, the encoder uses fewer bits on areas that can sustain more
/// distortion with no noticeable visual degradation and uses more bits on areas
/// where any small distortion will be noticeable. For example, complex textured
/// blocks are encoded with fewer bits and smooth textured blocks are encoded
/// with more bits. Enabling this feature will almost always improve your video
/// quality. Note, though, that this feature doesn't take into account where the
/// viewer's attention is likely to be. If viewers are likely to be focusing
/// their attention on a part of the screen with a lot of complex texture, you
/// might choose to disable this feature. Related setting: When you enable
/// spatial adaptive quantization, set the value for Adaptive quantization
/// depending on your content. For homogeneous content, such as cartoons and
/// video games, set it to Low. For content with a wider variety of textures,
/// set it to High or Higher.
enum Av1SpatialAdaptiveQuantization {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const Av1SpatialAdaptiveQuantization(this.value);

  static Av1SpatialAdaptiveQuantization fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Av1SpatialAdaptiveQuantization'));
}

/// Use ad avail blanking settings to specify your output content during SCTE-35
/// triggered ad avails. You can blank your video or overlay it with an image.
/// MediaConvert also removes any audio and embedded captions during the ad
/// avail. For more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/ad-avail-blanking.html.
class AvailBlanking {
  /// Blanking image to be used. Leave empty for solid black. Only bmp and png
  /// images are supported.
  final String? availBlankingImage;

  AvailBlanking({
    this.availBlankingImage,
  });

  factory AvailBlanking.fromJson(Map<String, dynamic> json) {
    return AvailBlanking(
      availBlankingImage: json['availBlankingImage'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final availBlankingImage = this.availBlankingImage;
    return {
      if (availBlankingImage != null) 'availBlankingImage': availBlankingImage,
    };
  }
}

/// Specify the AVC-Intra class of your output. The AVC-Intra class selection
/// determines the output video bit rate depending on the frame rate of the
/// output. Outputs with higher class values have higher bitrates and improved
/// image quality. Note that for Class 4K/2K, MediaConvert supports only 4:2:2
/// chroma subsampling.
enum AvcIntraClass {
  class_50('CLASS_50'),
  class_100('CLASS_100'),
  class_200('CLASS_200'),
  class_4k_2k('CLASS_4K_2K'),
  ;

  final String value;

  const AvcIntraClass(this.value);

  static AvcIntraClass fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum AvcIntraClass'));
}

/// If you are using the console, use the Framerate setting to specify the frame
/// rate for this output. If you want to keep the same frame rate as the input
/// video, choose Follow source. If you want to do frame rate conversion, choose
/// a frame rate from the dropdown list or choose Custom. The framerates shown
/// in the dropdown list are decimal approximations of fractions. If you choose
/// Custom, specify your frame rate as a fraction.
enum AvcIntraFramerateControl {
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  specified('SPECIFIED'),
  ;

  final String value;

  const AvcIntraFramerateControl(this.value);

  static AvcIntraFramerateControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum AvcIntraFramerateControl'));
}

/// Choose the method that you want MediaConvert to use when increasing or
/// decreasing the frame rate. For numerically simple conversions, such as 60
/// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
/// For numerically complex conversions, to avoid stutter: Choose Interpolate.
/// This results in a smooth picture, but might introduce undesirable video
/// artifacts. For complex frame rate conversions, especially if your source
/// video has already been converted from its original cadence: Choose
/// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
/// best conversion method frame by frame. Note that using FrameFormer increases
/// the transcoding time and incurs a significant add-on cost. When you choose
/// FrameFormer, your input video resolution must be at least 128x96.
enum AvcIntraFramerateConversionAlgorithm {
  duplicateDrop('DUPLICATE_DROP'),
  interpolate('INTERPOLATE'),
  frameformer('FRAMEFORMER'),
  ;

  final String value;

  const AvcIntraFramerateConversionAlgorithm(this.value);

  static AvcIntraFramerateConversionAlgorithm fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum AvcIntraFramerateConversionAlgorithm'));
}

/// Choose the scan line type for the output. Keep the default value,
/// Progressive to create a progressive output, regardless of the scan type of
/// your input. Use Top field first or Bottom field first to create an output
/// that's interlaced with the same field polarity throughout. Use Follow,
/// default top or Follow, default bottom to produce outputs with the same field
/// polarity as the source. For jobs that have multiple inputs, the output field
/// polarity might change over the course of the output. Follow behavior depends
/// on the input scan type. If the source is interlaced, the output will be
/// interlaced with the same polarity as the source. If the source is
/// progressive, the output will be interlaced with top field bottom field
/// first, depending on which of the Follow options you choose.
enum AvcIntraInterlaceMode {
  progressive('PROGRESSIVE'),
  topField('TOP_FIELD'),
  bottomField('BOTTOM_FIELD'),
  followTopField('FOLLOW_TOP_FIELD'),
  followBottomField('FOLLOW_BOTTOM_FIELD'),
  ;

  final String value;

  const AvcIntraInterlaceMode(this.value);

  static AvcIntraInterlaceMode fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum AvcIntraInterlaceMode'));
}

/// Use this setting for interlaced outputs, when your output frame rate is half
/// of your input frame rate. In this situation, choose Optimized interlacing to
/// create a better quality interlaced output. In this case, each progressive
/// frame from the input corresponds to an interlaced field in the output. Keep
/// the default value, Basic interlacing, for all other output frame rates. With
/// basic interlacing, MediaConvert performs any frame rate conversion first and
/// then interlaces the frames. When you choose Optimized interlacing and you
/// set your output frame rate to a value that isn't suitable for optimized
/// interlacing, MediaConvert automatically falls back to basic interlacing.
/// Required settings: To use optimized interlacing, you must set Telecine to
/// None or Soft. You can't use optimized interlacing for hard telecine outputs.
/// You must also set Interlace mode to a value other than Progressive.
enum AvcIntraScanTypeConversionMode {
  interlaced('INTERLACED'),
  interlacedOptimize('INTERLACED_OPTIMIZE'),
  ;

  final String value;

  const AvcIntraScanTypeConversionMode(this.value);

  static AvcIntraScanTypeConversionMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum AvcIntraScanTypeConversionMode'));
}

/// Required when you choose AVC-Intra for your output video codec. For more
/// information about the AVC-Intra settings, see the relevant specification.
/// For detailed information about SD and HD in AVC-Intra, see
/// https://ieeexplore.ieee.org/document/7290936. For information about 4K/2K in
/// AVC-Intra, see
/// https://pro-av.panasonic.net/en/avc-ultra/AVC-ULTRAoverview.pdf.
class AvcIntraSettings {
  /// Specify the AVC-Intra class of your output. The AVC-Intra class selection
  /// determines the output video bit rate depending on the frame rate of the
  /// output. Outputs with higher class values have higher bitrates and improved
  /// image quality. Note that for Class 4K/2K, MediaConvert supports only 4:2:2
  /// chroma subsampling.
  final AvcIntraClass? avcIntraClass;

  /// Optional when you set AVC-Intra class to Class 4K/2K. When you set AVC-Intra
  /// class to a different value, this object isn't allowed.
  final AvcIntraUhdSettings? avcIntraUhdSettings;

  /// If you are using the console, use the Framerate setting to specify the frame
  /// rate for this output. If you want to keep the same frame rate as the input
  /// video, choose Follow source. If you want to do frame rate conversion, choose
  /// a frame rate from the dropdown list or choose Custom. The framerates shown
  /// in the dropdown list are decimal approximations of fractions. If you choose
  /// Custom, specify your frame rate as a fraction.
  final AvcIntraFramerateControl? framerateControl;

  /// Choose the method that you want MediaConvert to use when increasing or
  /// decreasing the frame rate. For numerically simple conversions, such as 60
  /// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
  /// For numerically complex conversions, to avoid stutter: Choose Interpolate.
  /// This results in a smooth picture, but might introduce undesirable video
  /// artifacts. For complex frame rate conversions, especially if your source
  /// video has already been converted from its original cadence: Choose
  /// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
  /// best conversion method frame by frame. Note that using FrameFormer increases
  /// the transcoding time and incurs a significant add-on cost. When you choose
  /// FrameFormer, your input video resolution must be at least 128x96.
  final AvcIntraFramerateConversionAlgorithm? framerateConversionAlgorithm;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateDenominator to specify the denominator of this fraction.
  /// In this example, use 1001 for the value of FramerateDenominator. When you
  /// use the console for transcode jobs that use frame rate conversion, provide
  /// the value as a decimal number for Framerate. In this example, specify
  /// 23.976.
  final int? framerateDenominator;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateNumerator to specify the numerator of this fraction. In
  /// this example, use 24000 for the value of FramerateNumerator. When you use
  /// the console for transcode jobs that use frame rate conversion, provide the
  /// value as a decimal number for Framerate. In this example, specify 23.976.
  final int? framerateNumerator;

  /// Choose the scan line type for the output. Keep the default value,
  /// Progressive to create a progressive output, regardless of the scan type of
  /// your input. Use Top field first or Bottom field first to create an output
  /// that's interlaced with the same field polarity throughout. Use Follow,
  /// default top or Follow, default bottom to produce outputs with the same field
  /// polarity as the source. For jobs that have multiple inputs, the output field
  /// polarity might change over the course of the output. Follow behavior depends
  /// on the input scan type. If the source is interlaced, the output will be
  /// interlaced with the same polarity as the source. If the source is
  /// progressive, the output will be interlaced with top field bottom field
  /// first, depending on which of the Follow options you choose.
  final AvcIntraInterlaceMode? interlaceMode;

  /// Use this setting for interlaced outputs, when your output frame rate is half
  /// of your input frame rate. In this situation, choose Optimized interlacing to
  /// create a better quality interlaced output. In this case, each progressive
  /// frame from the input corresponds to an interlaced field in the output. Keep
  /// the default value, Basic interlacing, for all other output frame rates. With
  /// basic interlacing, MediaConvert performs any frame rate conversion first and
  /// then interlaces the frames. When you choose Optimized interlacing and you
  /// set your output frame rate to a value that isn't suitable for optimized
  /// interlacing, MediaConvert automatically falls back to basic interlacing.
  /// Required settings: To use optimized interlacing, you must set Telecine to
  /// None or Soft. You can't use optimized interlacing for hard telecine outputs.
  /// You must also set Interlace mode to a value other than Progressive.
  final AvcIntraScanTypeConversionMode? scanTypeConversionMode;

  /// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
  /// second (fps). Enable slow PAL to create a 25 fps output. When you enable
  /// slow PAL, MediaConvert relabels the video frames to 25 fps and resamples
  /// your audio to keep it synchronized with the video. Note that enabling this
  /// setting will slightly reduce the duration of your video. Required settings:
  /// You must also set Framerate to 25.
  final AvcIntraSlowPal? slowPal;

  /// When you do frame rate conversion from 23.976 frames per second (fps) to
  /// 29.97 fps, and your output scan type is interlaced, you can optionally
  /// enable hard telecine to create a smoother picture. When you keep the default
  /// value, None, MediaConvert does a standard frame rate conversion to 29.97
  /// without doing anything with the field polarity to create a smoother picture.
  final AvcIntraTelecine? telecine;

  AvcIntraSettings({
    this.avcIntraClass,
    this.avcIntraUhdSettings,
    this.framerateControl,
    this.framerateConversionAlgorithm,
    this.framerateDenominator,
    this.framerateNumerator,
    this.interlaceMode,
    this.scanTypeConversionMode,
    this.slowPal,
    this.telecine,
  });

  factory AvcIntraSettings.fromJson(Map<String, dynamic> json) {
    return AvcIntraSettings(
      avcIntraClass:
          (json['avcIntraClass'] as String?)?.let(AvcIntraClass.fromString),
      avcIntraUhdSettings: json['avcIntraUhdSettings'] != null
          ? AvcIntraUhdSettings.fromJson(
              json['avcIntraUhdSettings'] as Map<String, dynamic>)
          : null,
      framerateControl: (json['framerateControl'] as String?)
          ?.let(AvcIntraFramerateControl.fromString),
      framerateConversionAlgorithm:
          (json['framerateConversionAlgorithm'] as String?)
              ?.let(AvcIntraFramerateConversionAlgorithm.fromString),
      framerateDenominator: json['framerateDenominator'] as int?,
      framerateNumerator: json['framerateNumerator'] as int?,
      interlaceMode: (json['interlaceMode'] as String?)
          ?.let(AvcIntraInterlaceMode.fromString),
      scanTypeConversionMode: (json['scanTypeConversionMode'] as String?)
          ?.let(AvcIntraScanTypeConversionMode.fromString),
      slowPal: (json['slowPal'] as String?)?.let(AvcIntraSlowPal.fromString),
      telecine: (json['telecine'] as String?)?.let(AvcIntraTelecine.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final avcIntraClass = this.avcIntraClass;
    final avcIntraUhdSettings = this.avcIntraUhdSettings;
    final framerateControl = this.framerateControl;
    final framerateConversionAlgorithm = this.framerateConversionAlgorithm;
    final framerateDenominator = this.framerateDenominator;
    final framerateNumerator = this.framerateNumerator;
    final interlaceMode = this.interlaceMode;
    final scanTypeConversionMode = this.scanTypeConversionMode;
    final slowPal = this.slowPal;
    final telecine = this.telecine;
    return {
      if (avcIntraClass != null) 'avcIntraClass': avcIntraClass.value,
      if (avcIntraUhdSettings != null)
        'avcIntraUhdSettings': avcIntraUhdSettings,
      if (framerateControl != null) 'framerateControl': framerateControl.value,
      if (framerateConversionAlgorithm != null)
        'framerateConversionAlgorithm': framerateConversionAlgorithm.value,
      if (framerateDenominator != null)
        'framerateDenominator': framerateDenominator,
      if (framerateNumerator != null) 'framerateNumerator': framerateNumerator,
      if (interlaceMode != null) 'interlaceMode': interlaceMode.value,
      if (scanTypeConversionMode != null)
        'scanTypeConversionMode': scanTypeConversionMode.value,
      if (slowPal != null) 'slowPal': slowPal.value,
      if (telecine != null) 'telecine': telecine.value,
    };
  }
}

/// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
/// second (fps). Enable slow PAL to create a 25 fps output. When you enable
/// slow PAL, MediaConvert relabels the video frames to 25 fps and resamples
/// your audio to keep it synchronized with the video. Note that enabling this
/// setting will slightly reduce the duration of your video. Required settings:
/// You must also set Framerate to 25.
enum AvcIntraSlowPal {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const AvcIntraSlowPal(this.value);

  static AvcIntraSlowPal fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum AvcIntraSlowPal'));
}

/// When you do frame rate conversion from 23.976 frames per second (fps) to
/// 29.97 fps, and your output scan type is interlaced, you can optionally
/// enable hard telecine to create a smoother picture. When you keep the default
/// value, None, MediaConvert does a standard frame rate conversion to 29.97
/// without doing anything with the field polarity to create a smoother picture.
enum AvcIntraTelecine {
  none('NONE'),
  hard('HARD'),
  ;

  final String value;

  const AvcIntraTelecine(this.value);

  static AvcIntraTelecine fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum AvcIntraTelecine'));
}

/// Optional. Use Quality tuning level to choose how many transcoding passes
/// MediaConvert does with your video. When you choose Multi-pass, your video
/// quality is better and your output bitrate is more accurate. That is, the
/// actual bitrate of your output is closer to the target bitrate defined in the
/// specification. When you choose Single-pass, your encoding time is faster.
/// The default behavior is Single-pass.
enum AvcIntraUhdQualityTuningLevel {
  singlePass('SINGLE_PASS'),
  multiPass('MULTI_PASS'),
  ;

  final String value;

  const AvcIntraUhdQualityTuningLevel(this.value);

  static AvcIntraUhdQualityTuningLevel fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum AvcIntraUhdQualityTuningLevel'));
}

/// Optional when you set AVC-Intra class to Class 4K/2K. When you set AVC-Intra
/// class to a different value, this object isn't allowed.
class AvcIntraUhdSettings {
  /// Optional. Use Quality tuning level to choose how many transcoding passes
  /// MediaConvert does with your video. When you choose Multi-pass, your video
  /// quality is better and your output bitrate is more accurate. That is, the
  /// actual bitrate of your output is closer to the target bitrate defined in the
  /// specification. When you choose Single-pass, your encoding time is faster.
  /// The default behavior is Single-pass.
  final AvcIntraUhdQualityTuningLevel? qualityTuningLevel;

  AvcIntraUhdSettings({
    this.qualityTuningLevel,
  });

  factory AvcIntraUhdSettings.fromJson(Map<String, dynamic> json) {
    return AvcIntraUhdSettings(
      qualityTuningLevel: (json['qualityTuningLevel'] as String?)
          ?.let(AvcIntraUhdQualityTuningLevel.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final qualityTuningLevel = this.qualityTuningLevel;
    return {
      if (qualityTuningLevel != null)
        'qualityTuningLevel': qualityTuningLevel.value,
    };
  }
}

/// The Bandwidth reduction filter increases the video quality of your output
/// relative to its bitrate. Use to lower the bitrate of your constant quality
/// QVBR output, with little or no perceptual decrease in quality. Or, use to
/// increase the video quality of outputs with other rate control modes relative
/// to the bitrate that you specify. Bandwidth reduction increases further when
/// your input is low quality or noisy. Outputs that use this feature incur
/// pro-tier pricing. When you include Bandwidth reduction filter, you cannot
/// include the Noise reducer preprocessor.
class BandwidthReductionFilter {
  /// Optionally specify the level of sharpening to apply when you use the
  /// Bandwidth reduction filter. Sharpening adds contrast to the edges of your
  /// video content and can reduce softness. Keep the default value Off to apply
  /// no sharpening. Set Sharpening strength to Low to apply a minimal amount of
  /// sharpening, or High to apply a maximum amount of sharpening.
  final BandwidthReductionFilterSharpening? sharpening;

  /// Specify the strength of the Bandwidth reduction filter. For most workflows,
  /// we recommend that you choose Auto to reduce the bandwidth of your output
  /// with little to no perceptual decrease in video quality. For high quality and
  /// high bitrate outputs, choose Low. For the most bandwidth reduction, choose
  /// High. We recommend that you choose High for low bitrate outputs. Note that
  /// High may incur a slight increase in the softness of your output.
  final BandwidthReductionFilterStrength? strength;

  BandwidthReductionFilter({
    this.sharpening,
    this.strength,
  });

  factory BandwidthReductionFilter.fromJson(Map<String, dynamic> json) {
    return BandwidthReductionFilter(
      sharpening: (json['sharpening'] as String?)
          ?.let(BandwidthReductionFilterSharpening.fromString),
      strength: (json['strength'] as String?)
          ?.let(BandwidthReductionFilterStrength.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final sharpening = this.sharpening;
    final strength = this.strength;
    return {
      if (sharpening != null) 'sharpening': sharpening.value,
      if (strength != null) 'strength': strength.value,
    };
  }
}

/// Optionally specify the level of sharpening to apply when you use the
/// Bandwidth reduction filter. Sharpening adds contrast to the edges of your
/// video content and can reduce softness. Keep the default value Off to apply
/// no sharpening. Set Sharpening strength to Low to apply a minimal amount of
/// sharpening, or High to apply a maximum amount of sharpening.
enum BandwidthReductionFilterSharpening {
  low('LOW'),
  medium('MEDIUM'),
  high('HIGH'),
  off('OFF'),
  ;

  final String value;

  const BandwidthReductionFilterSharpening(this.value);

  static BandwidthReductionFilterSharpening fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum BandwidthReductionFilterSharpening'));
}

/// Specify the strength of the Bandwidth reduction filter. For most workflows,
/// we recommend that you choose Auto to reduce the bandwidth of your output
/// with little to no perceptual decrease in video quality. For high quality and
/// high bitrate outputs, choose Low. For the most bandwidth reduction, choose
/// High. We recommend that you choose High for low bitrate outputs. Note that
/// High may incur a slight increase in the softness of your output.
enum BandwidthReductionFilterStrength {
  low('LOW'),
  medium('MEDIUM'),
  high('HIGH'),
  auto('AUTO'),
  off('OFF'),
  ;

  final String value;

  const BandwidthReductionFilterStrength(this.value);

  static BandwidthReductionFilterStrength fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum BandwidthReductionFilterStrength'));
}

/// The tag type that AWS Billing and Cost Management will use to sort your AWS
/// Elemental MediaConvert costs on any billing report that you set up.
enum BillingTagsSource {
  queue('QUEUE'),
  preset('PRESET'),
  jobTemplate('JOB_TEMPLATE'),
  job('JOB'),
  ;

  final String value;

  const BillingTagsSource(this.value);

  static BillingTagsSource fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum BillingTagsSource'));
}

/// To use the available style, color, and position information from your input
/// captions: Set Style passthrough to Enabled. Note that MediaConvert uses
/// default settings for any missing style or position information in your input
/// captions To ignore the style and position information from your input
/// captions and use default settings: Leave blank or keep the default value,
/// Disabled. Default settings include white text with black outlining,
/// bottom-center positioning, and automatic sizing. Whether you set Style
/// passthrough to enabled or not, you can also choose to manually override any
/// of the individual style and position settings. You can also override any
/// fonts by manually specifying custom font files.
enum BurnInSubtitleStylePassthrough {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const BurnInSubtitleStylePassthrough(this.value);

  static BurnInSubtitleStylePassthrough fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum BurnInSubtitleStylePassthrough'));
}

/// Burn-in is a captions delivery method, rather than a captions format.
/// Burn-in writes the captions directly on your video frames, replacing pixels
/// of video content with the captions. Set up burn-in captions in the same
/// output as your video. For more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/burn-in-output-captions.html.
class BurninDestinationSettings {
  /// Specify the alignment of your captions. If no explicit x_position is
  /// provided, setting alignment to centered will placethe captions at the bottom
  /// center of the output. Similarly, setting a left alignment willalign captions
  /// to the bottom left of the output. If x and y positions are given in
  /// conjunction with the alignment parameter, the font will be justified (either
  /// left or centered) relative to those coordinates.
  final BurninSubtitleAlignment? alignment;

  /// Ignore this setting unless Style passthrough is set to Enabled and Font
  /// color set to Black, Yellow, Red, Green, Blue, or Hex. Use Apply font color
  /// for additional font color controls. When you choose White text only, or
  /// leave blank, your font color setting only applies to white text in your
  /// input captions. For example, if your font color setting is Yellow, and your
  /// input captions have red and white text, your output captions will have red
  /// and yellow text. When you choose ALL_TEXT, your font color setting applies
  /// to all of your output captions text.
  final BurninSubtitleApplyFontColor? applyFontColor;

  /// Specify the color of the rectangle behind the captions. Leave background
  /// color blank and set Style passthrough to enabled to use the background color
  /// data from your input captions, if present.
  final BurninSubtitleBackgroundColor? backgroundColor;

  /// Specify the opacity of the background rectangle. Enter a value from 0 to
  /// 255, where 0 is transparent and 255 is opaque. If Style passthrough is set
  /// to enabled, leave blank to pass through the background style information in
  /// your input captions to your output captions. If Style passthrough is set to
  /// disabled, leave blank to use a value of 0 and remove all backgrounds from
  /// your output captions.
  final int? backgroundOpacity;

  /// Specify the font that you want the service to use for your burn in captions
  /// when your input captions specify a font that MediaConvert doesn't support.
  /// When you set Fallback font to best match, or leave blank, MediaConvert uses
  /// a supported font that most closely matches the font that your input captions
  /// specify. When there are multiple unsupported fonts in your input captions,
  /// MediaConvert matches each font with the supported font that matches best.
  /// When you explicitly choose a replacement font, MediaConvert uses that font
  /// to replace all unsupported fonts from your input.
  final BurninSubtitleFallbackFont? fallbackFont;

  /// Specify the color of the burned-in captions text. Leave Font color blank and
  /// set Style passthrough to enabled to use the font color data from your input
  /// captions, if present.
  final BurninSubtitleFontColor? fontColor;

  /// Specify a bold TrueType font file to use when rendering your output
  /// captions. Enter an S3, HTTP, or HTTPS URL. When you do, you must also
  /// separately specify a regular, an italic, and a bold italic font file.
  final String? fontFileBold;

  /// Specify a bold italic TrueType font file to use when rendering your output
  /// captions.
  /// Enter an S3, HTTP, or HTTPS URL.
  /// When you do, you must also separately specify a regular, a bold, and an
  /// italic font file.
  final String? fontFileBoldItalic;

  /// Specify an italic TrueType font file to use when rendering your output
  /// captions. Enter an S3, HTTP, or HTTPS URL. When you do, you must also
  /// separately specify a regular, a bold, and a bold italic font file.
  final String? fontFileItalic;

  /// Specify a regular TrueType font file to use when rendering your output
  /// captions. Enter an S3, HTTP, or HTTPS URL. When you do, you must also
  /// separately specify a bold, an italic, and a bold italic font file.
  final String? fontFileRegular;

  /// Specify the opacity of the burned-in captions. 255 is opaque; 0 is
  /// transparent.
  final int? fontOpacity;

  /// Specify the Font resolution in DPI (dots per inch).
  final int? fontResolution;

  /// Set Font script to Automatically determined, or leave blank, to
  /// automatically determine the font script in your input captions. Otherwise,
  /// set to Simplified Chinese (HANS) or Traditional Chinese (HANT) if your input
  /// font script uses Simplified or Traditional Chinese.
  final FontScript? fontScript;

  /// Specify the Font size in pixels. Must be a positive integer. Set to 0, or
  /// leave blank, for automatic font size.
  final int? fontSize;

  /// Ignore this setting unless your Font color is set to Hex. Enter either six
  /// or eight hexidecimal digits, representing red, green, and blue, with two
  /// optional extra digits for alpha. For example a value of 1122AABB is a red
  /// value of 0x11, a green value of 0x22, a blue value of 0xAA, and an alpha
  /// value of 0xBB.
  final String? hexFontColor;

  /// Specify font outline color. Leave Outline color blank and set Style
  /// passthrough to enabled to use the font outline color data from your input
  /// captions, if present.
  final BurninSubtitleOutlineColor? outlineColor;

  /// Specify the Outline size of the caption text, in pixels. Leave Outline size
  /// blank and set Style passthrough to enabled to use the outline size data from
  /// your input captions, if present.
  final int? outlineSize;

  /// Specify the color of the shadow cast by the captions. Leave Shadow color
  /// blank and set Style passthrough to enabled to use the shadow color data from
  /// your input captions, if present.
  final BurninSubtitleShadowColor? shadowColor;

  /// Specify the opacity of the shadow. Enter a value from 0 to 255, where 0 is
  /// transparent and 255 is opaque. If Style passthrough is set to Enabled, leave
  /// Shadow opacity blank to pass through the shadow style information in your
  /// input captions to your output captions. If Style passthrough is set to
  /// disabled, leave blank to use a value of 0 and remove all shadows from your
  /// output captions.
  final int? shadowOpacity;

  /// Specify the horizontal offset of the shadow, relative to the captions in
  /// pixels. A value of -2 would result in a shadow offset 2 pixels to the left.
  final int? shadowXOffset;

  /// Specify the vertical offset of the shadow relative to the captions in
  /// pixels. A value of -2 would result in a shadow offset 2 pixels above the
  /// text. Leave Shadow y-offset blank and set Style passthrough to enabled to
  /// use the shadow y-offset data from your input captions, if present.
  final int? shadowYOffset;

  /// To use the available style, color, and position information from your input
  /// captions: Set Style passthrough to Enabled. Note that MediaConvert uses
  /// default settings for any missing style or position information in your input
  /// captions To ignore the style and position information from your input
  /// captions and use default settings: Leave blank or keep the default value,
  /// Disabled. Default settings include white text with black outlining,
  /// bottom-center positioning, and automatic sizing. Whether you set Style
  /// passthrough to enabled or not, you can also choose to manually override any
  /// of the individual style and position settings. You can also override any
  /// fonts by manually specifying custom font files.
  final BurnInSubtitleStylePassthrough? stylePassthrough;

  /// Specify whether the text spacing in your captions is set by the captions
  /// grid, or varies depending on letter width. Choose fixed grid to conform to
  /// the spacing specified in the captions file more accurately. Choose
  /// proportional to make the text easier to read for closed captions.
  final BurninSubtitleTeletextSpacing? teletextSpacing;

  /// Specify the horizontal position of the captions, relative to the left side
  /// of the output in pixels. A value of 10 would result in the captions starting
  /// 10 pixels from the left of the output. If no explicit x_position is
  /// provided, the horizontal caption position will be determined by the
  /// alignment parameter.
  final int? xPosition;

  /// Specify the vertical position of the captions, relative to the top of the
  /// output in pixels. A value of 10 would result in the captions starting 10
  /// pixels from the top of the output. If no explicit y_position is provided,
  /// the caption will be positioned towards the bottom of the output.
  final int? yPosition;

  BurninDestinationSettings({
    this.alignment,
    this.applyFontColor,
    this.backgroundColor,
    this.backgroundOpacity,
    this.fallbackFont,
    this.fontColor,
    this.fontFileBold,
    this.fontFileBoldItalic,
    this.fontFileItalic,
    this.fontFileRegular,
    this.fontOpacity,
    this.fontResolution,
    this.fontScript,
    this.fontSize,
    this.hexFontColor,
    this.outlineColor,
    this.outlineSize,
    this.shadowColor,
    this.shadowOpacity,
    this.shadowXOffset,
    this.shadowYOffset,
    this.stylePassthrough,
    this.teletextSpacing,
    this.xPosition,
    this.yPosition,
  });

  factory BurninDestinationSettings.fromJson(Map<String, dynamic> json) {
    return BurninDestinationSettings(
      alignment: (json['alignment'] as String?)
          ?.let(BurninSubtitleAlignment.fromString),
      applyFontColor: (json['applyFontColor'] as String?)
          ?.let(BurninSubtitleApplyFontColor.fromString),
      backgroundColor: (json['backgroundColor'] as String?)
          ?.let(BurninSubtitleBackgroundColor.fromString),
      backgroundOpacity: json['backgroundOpacity'] as int?,
      fallbackFont: (json['fallbackFont'] as String?)
          ?.let(BurninSubtitleFallbackFont.fromString),
      fontColor: (json['fontColor'] as String?)
          ?.let(BurninSubtitleFontColor.fromString),
      fontFileBold: json['fontFileBold'] as String?,
      fontFileBoldItalic: json['fontFileBoldItalic'] as String?,
      fontFileItalic: json['fontFileItalic'] as String?,
      fontFileRegular: json['fontFileRegular'] as String?,
      fontOpacity: json['fontOpacity'] as int?,
      fontResolution: json['fontResolution'] as int?,
      fontScript: (json['fontScript'] as String?)?.let(FontScript.fromString),
      fontSize: json['fontSize'] as int?,
      hexFontColor: json['hexFontColor'] as String?,
      outlineColor: (json['outlineColor'] as String?)
          ?.let(BurninSubtitleOutlineColor.fromString),
      outlineSize: json['outlineSize'] as int?,
      shadowColor: (json['shadowColor'] as String?)
          ?.let(BurninSubtitleShadowColor.fromString),
      shadowOpacity: json['shadowOpacity'] as int?,
      shadowXOffset: json['shadowXOffset'] as int?,
      shadowYOffset: json['shadowYOffset'] as int?,
      stylePassthrough: (json['stylePassthrough'] as String?)
          ?.let(BurnInSubtitleStylePassthrough.fromString),
      teletextSpacing: (json['teletextSpacing'] as String?)
          ?.let(BurninSubtitleTeletextSpacing.fromString),
      xPosition: json['xPosition'] as int?,
      yPosition: json['yPosition'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final alignment = this.alignment;
    final applyFontColor = this.applyFontColor;
    final backgroundColor = this.backgroundColor;
    final backgroundOpacity = this.backgroundOpacity;
    final fallbackFont = this.fallbackFont;
    final fontColor = this.fontColor;
    final fontFileBold = this.fontFileBold;
    final fontFileBoldItalic = this.fontFileBoldItalic;
    final fontFileItalic = this.fontFileItalic;
    final fontFileRegular = this.fontFileRegular;
    final fontOpacity = this.fontOpacity;
    final fontResolution = this.fontResolution;
    final fontScript = this.fontScript;
    final fontSize = this.fontSize;
    final hexFontColor = this.hexFontColor;
    final outlineColor = this.outlineColor;
    final outlineSize = this.outlineSize;
    final shadowColor = this.shadowColor;
    final shadowOpacity = this.shadowOpacity;
    final shadowXOffset = this.shadowXOffset;
    final shadowYOffset = this.shadowYOffset;
    final stylePassthrough = this.stylePassthrough;
    final teletextSpacing = this.teletextSpacing;
    final xPosition = this.xPosition;
    final yPosition = this.yPosition;
    return {
      if (alignment != null) 'alignment': alignment.value,
      if (applyFontColor != null) 'applyFontColor': applyFontColor.value,
      if (backgroundColor != null) 'backgroundColor': backgroundColor.value,
      if (backgroundOpacity != null) 'backgroundOpacity': backgroundOpacity,
      if (fallbackFont != null) 'fallbackFont': fallbackFont.value,
      if (fontColor != null) 'fontColor': fontColor.value,
      if (fontFileBold != null) 'fontFileBold': fontFileBold,
      if (fontFileBoldItalic != null) 'fontFileBoldItalic': fontFileBoldItalic,
      if (fontFileItalic != null) 'fontFileItalic': fontFileItalic,
      if (fontFileRegular != null) 'fontFileRegular': fontFileRegular,
      if (fontOpacity != null) 'fontOpacity': fontOpacity,
      if (fontResolution != null) 'fontResolution': fontResolution,
      if (fontScript != null) 'fontScript': fontScript.value,
      if (fontSize != null) 'fontSize': fontSize,
      if (hexFontColor != null) 'hexFontColor': hexFontColor,
      if (outlineColor != null) 'outlineColor': outlineColor.value,
      if (outlineSize != null) 'outlineSize': outlineSize,
      if (shadowColor != null) 'shadowColor': shadowColor.value,
      if (shadowOpacity != null) 'shadowOpacity': shadowOpacity,
      if (shadowXOffset != null) 'shadowXOffset': shadowXOffset,
      if (shadowYOffset != null) 'shadowYOffset': shadowYOffset,
      if (stylePassthrough != null) 'stylePassthrough': stylePassthrough.value,
      if (teletextSpacing != null) 'teletextSpacing': teletextSpacing.value,
      if (xPosition != null) 'xPosition': xPosition,
      if (yPosition != null) 'yPosition': yPosition,
    };
  }
}

/// Specify the alignment of your captions. If no explicit x_position is
/// provided, setting alignment to centered will placethe captions at the bottom
/// center of the output. Similarly, setting a left alignment willalign captions
/// to the bottom left of the output. If x and y positions are given in
/// conjunction with the alignment parameter, the font will be justified (either
/// left or centered) relative to those coordinates.
enum BurninSubtitleAlignment {
  centered('CENTERED'),
  left('LEFT'),
  auto('AUTO'),
  ;

  final String value;

  const BurninSubtitleAlignment(this.value);

  static BurninSubtitleAlignment fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum BurninSubtitleAlignment'));
}

/// Ignore this setting unless Style passthrough is set to Enabled and Font
/// color set to Black, Yellow, Red, Green, Blue, or Hex. Use Apply font color
/// for additional font color controls. When you choose White text only, or
/// leave blank, your font color setting only applies to white text in your
/// input captions. For example, if your font color setting is Yellow, and your
/// input captions have red and white text, your output captions will have red
/// and yellow text. When you choose ALL_TEXT, your font color setting applies
/// to all of your output captions text.
enum BurninSubtitleApplyFontColor {
  whiteTextOnly('WHITE_TEXT_ONLY'),
  allText('ALL_TEXT'),
  ;

  final String value;

  const BurninSubtitleApplyFontColor(this.value);

  static BurninSubtitleApplyFontColor fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum BurninSubtitleApplyFontColor'));
}

/// Specify the color of the rectangle behind the captions. Leave background
/// color blank and set Style passthrough to enabled to use the background color
/// data from your input captions, if present.
enum BurninSubtitleBackgroundColor {
  none('NONE'),
  black('BLACK'),
  white('WHITE'),
  auto('AUTO'),
  ;

  final String value;

  const BurninSubtitleBackgroundColor(this.value);

  static BurninSubtitleBackgroundColor fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum BurninSubtitleBackgroundColor'));
}

/// Specify the font that you want the service to use for your burn in captions
/// when your input captions specify a font that MediaConvert doesn't support.
/// When you set Fallback font to best match, or leave blank, MediaConvert uses
/// a supported font that most closely matches the font that your input captions
/// specify. When there are multiple unsupported fonts in your input captions,
/// MediaConvert matches each font with the supported font that matches best.
/// When you explicitly choose a replacement font, MediaConvert uses that font
/// to replace all unsupported fonts from your input.
enum BurninSubtitleFallbackFont {
  bestMatch('BEST_MATCH'),
  monospacedSansserif('MONOSPACED_SANSSERIF'),
  monospacedSerif('MONOSPACED_SERIF'),
  proportionalSansserif('PROPORTIONAL_SANSSERIF'),
  proportionalSerif('PROPORTIONAL_SERIF'),
  ;

  final String value;

  const BurninSubtitleFallbackFont(this.value);

  static BurninSubtitleFallbackFont fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum BurninSubtitleFallbackFont'));
}

/// Specify the color of the burned-in captions text. Leave Font color blank and
/// set Style passthrough to enabled to use the font color data from your input
/// captions, if present.
enum BurninSubtitleFontColor {
  white('WHITE'),
  black('BLACK'),
  yellow('YELLOW'),
  red('RED'),
  green('GREEN'),
  blue('BLUE'),
  hex('HEX'),
  auto('AUTO'),
  ;

  final String value;

  const BurninSubtitleFontColor(this.value);

  static BurninSubtitleFontColor fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum BurninSubtitleFontColor'));
}

/// Specify font outline color. Leave Outline color blank and set Style
/// passthrough to enabled to use the font outline color data from your input
/// captions, if present.
enum BurninSubtitleOutlineColor {
  black('BLACK'),
  white('WHITE'),
  yellow('YELLOW'),
  red('RED'),
  green('GREEN'),
  blue('BLUE'),
  auto('AUTO'),
  ;

  final String value;

  const BurninSubtitleOutlineColor(this.value);

  static BurninSubtitleOutlineColor fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum BurninSubtitleOutlineColor'));
}

/// Specify the color of the shadow cast by the captions. Leave Shadow color
/// blank and set Style passthrough to enabled to use the shadow color data from
/// your input captions, if present.
enum BurninSubtitleShadowColor {
  none('NONE'),
  black('BLACK'),
  white('WHITE'),
  auto('AUTO'),
  ;

  final String value;

  const BurninSubtitleShadowColor(this.value);

  static BurninSubtitleShadowColor fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum BurninSubtitleShadowColor'));
}

/// Specify whether the text spacing in your captions is set by the captions
/// grid, or varies depending on letter width. Choose fixed grid to conform to
/// the spacing specified in the captions file more accurately. Choose
/// proportional to make the text easier to read for closed captions.
enum BurninSubtitleTeletextSpacing {
  fixedGrid('FIXED_GRID'),
  proportional('PROPORTIONAL'),
  auto('AUTO'),
  ;

  final String value;

  const BurninSubtitleTeletextSpacing(this.value);

  static BurninSubtitleTeletextSpacing fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum BurninSubtitleTeletextSpacing'));
}

class CancelJobResponse {
  CancelJobResponse();

  factory CancelJobResponse.fromJson(Map<String, dynamic> _) {
    return CancelJobResponse();
  }

  Map<String, dynamic> toJson() {
    return {};
  }
}

/// This object holds groups of settings related to captions for one output. For
/// each output that has captions, include one instance of CaptionDescriptions.
class CaptionDescription {
  /// Specifies which "Caption Selector":#inputs-caption_selector to use from each
  /// input when generating captions. The name should be of the format "Caption
  /// Selector <N>", which denotes that the Nth Caption Selector will be used from
  /// each input.
  final String? captionSelectorName;

  /// Specify the language for this captions output track. For most captions
  /// output formats, the encoder puts this language information in the output
  /// captions metadata. If your output captions format is DVB-Sub or Burn in, the
  /// encoder uses this language information when automatically selecting the font
  /// script for rendering the captions text. For all outputs, you can use an ISO
  /// 639-2 or ISO 639-3 code. For streaming outputs, you can also use any other
  /// code in the full RFC-5646 specification. Streaming outputs are those that
  /// are in one of the following output groups: CMAF, DASH ISO, Apple HLS, or
  /// Microsoft Smooth Streaming.
  final String? customLanguageCode;

  /// Settings related to one captions tab on the MediaConvert console. Usually,
  /// one captions tab corresponds to one output captions track. Depending on your
  /// output captions format, one tab might correspond to a set of output captions
  /// tracks. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/including-captions.html.
  final CaptionDestinationSettings? destinationSettings;

  /// Specify the language of this captions output track. For most captions output
  /// formats, the encoder puts this language information in the output captions
  /// metadata. If your output captions format is DVB-Sub or Burn in, the encoder
  /// uses this language information to choose the font language for rendering the
  /// captions text.
  final LanguageCode? languageCode;

  /// Specify a label for this set of output captions. For example, "English",
  /// "Director commentary", or "track_2". For streaming outputs, MediaConvert
  /// passes this information into destination manifests for display on the
  /// end-viewer's player device. For outputs in other output groups, the service
  /// ignores this setting.
  final String? languageDescription;

  CaptionDescription({
    this.captionSelectorName,
    this.customLanguageCode,
    this.destinationSettings,
    this.languageCode,
    this.languageDescription,
  });

  factory CaptionDescription.fromJson(Map<String, dynamic> json) {
    return CaptionDescription(
      captionSelectorName: json['captionSelectorName'] as String?,
      customLanguageCode: json['customLanguageCode'] as String?,
      destinationSettings: json['destinationSettings'] != null
          ? CaptionDestinationSettings.fromJson(
              json['destinationSettings'] as Map<String, dynamic>)
          : null,
      languageCode:
          (json['languageCode'] as String?)?.let(LanguageCode.fromString),
      languageDescription: json['languageDescription'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final captionSelectorName = this.captionSelectorName;
    final customLanguageCode = this.customLanguageCode;
    final destinationSettings = this.destinationSettings;
    final languageCode = this.languageCode;
    final languageDescription = this.languageDescription;
    return {
      if (captionSelectorName != null)
        'captionSelectorName': captionSelectorName,
      if (customLanguageCode != null) 'customLanguageCode': customLanguageCode,
      if (destinationSettings != null)
        'destinationSettings': destinationSettings,
      if (languageCode != null) 'languageCode': languageCode.value,
      if (languageDescription != null)
        'languageDescription': languageDescription,
    };
  }
}

/// Caption Description for preset
class CaptionDescriptionPreset {
  /// Specify the language for this captions output track. For most captions
  /// output formats, the encoder puts this language information in the output
  /// captions metadata. If your output captions format is DVB-Sub or Burn in, the
  /// encoder uses this language information when automatically selecting the font
  /// script for rendering the captions text. For all outputs, you can use an ISO
  /// 639-2 or ISO 639-3 code. For streaming outputs, you can also use any other
  /// code in the full RFC-5646 specification. Streaming outputs are those that
  /// are in one of the following output groups: CMAF, DASH ISO, Apple HLS, or
  /// Microsoft Smooth Streaming.
  final String? customLanguageCode;

  /// Settings related to one captions tab on the MediaConvert console. Usually,
  /// one captions tab corresponds to one output captions track. Depending on your
  /// output captions format, one tab might correspond to a set of output captions
  /// tracks. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/including-captions.html.
  final CaptionDestinationSettings? destinationSettings;

  /// Specify the language of this captions output track. For most captions output
  /// formats, the encoder puts this language information in the output captions
  /// metadata. If your output captions format is DVB-Sub or Burn in, the encoder
  /// uses this language information to choose the font language for rendering the
  /// captions text.
  final LanguageCode? languageCode;

  /// Specify a label for this set of output captions. For example, "English",
  /// "Director commentary", or "track_2". For streaming outputs, MediaConvert
  /// passes this information into destination manifests for display on the
  /// end-viewer's player device. For outputs in other output groups, the service
  /// ignores this setting.
  final String? languageDescription;

  CaptionDescriptionPreset({
    this.customLanguageCode,
    this.destinationSettings,
    this.languageCode,
    this.languageDescription,
  });

  factory CaptionDescriptionPreset.fromJson(Map<String, dynamic> json) {
    return CaptionDescriptionPreset(
      customLanguageCode: json['customLanguageCode'] as String?,
      destinationSettings: json['destinationSettings'] != null
          ? CaptionDestinationSettings.fromJson(
              json['destinationSettings'] as Map<String, dynamic>)
          : null,
      languageCode:
          (json['languageCode'] as String?)?.let(LanguageCode.fromString),
      languageDescription: json['languageDescription'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final customLanguageCode = this.customLanguageCode;
    final destinationSettings = this.destinationSettings;
    final languageCode = this.languageCode;
    final languageDescription = this.languageDescription;
    return {
      if (customLanguageCode != null) 'customLanguageCode': customLanguageCode,
      if (destinationSettings != null)
        'destinationSettings': destinationSettings,
      if (languageCode != null) 'languageCode': languageCode.value,
      if (languageDescription != null)
        'languageDescription': languageDescription,
    };
  }
}

/// Settings related to one captions tab on the MediaConvert console. Usually,
/// one captions tab corresponds to one output captions track. Depending on your
/// output captions format, one tab might correspond to a set of output captions
/// tracks. For more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/including-captions.html.
class CaptionDestinationSettings {
  /// Burn-in is a captions delivery method, rather than a captions format.
  /// Burn-in writes the captions directly on your video frames, replacing pixels
  /// of video content with the captions. Set up burn-in captions in the same
  /// output as your video. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/burn-in-output-captions.html.
  final BurninDestinationSettings? burninDestinationSettings;

  /// Specify the format for this set of captions on this output. The default
  /// format is embedded without SCTE-20. Note that your choice of video output
  /// container constrains your choice of output captions format. For more
  /// information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/captions-support-tables.html.
  /// If you are using SCTE-20 and you want to create an output that complies with
  /// the SCTE-43 spec, choose SCTE-20 plus embedded. To create a non-compliant
  /// output where the embedded captions come first, choose Embedded plus SCTE-20.
  final CaptionDestinationType? destinationType;

  /// Settings related to DVB-Sub captions. Set up DVB-Sub captions in the same
  /// output as your video. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/dvb-sub-output-captions.html.
  final DvbSubDestinationSettings? dvbSubDestinationSettings;

  /// Settings related to CEA/EIA-608 and CEA/EIA-708 (also called embedded or
  /// ancillary) captions. Set up embedded captions in the same output as your
  /// video. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/embedded-output-captions.html.
  final EmbeddedDestinationSettings? embeddedDestinationSettings;

  /// Settings related to IMSC captions. IMSC is a sidecar format that holds
  /// captions in a file that is separate from the video container. Set up sidecar
  /// captions in the same output group, but different output from your video. For
  /// more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/ttml-and-webvtt-output-captions.html.
  final ImscDestinationSettings? imscDestinationSettings;

  /// Settings related to SCC captions. SCC is a sidecar format that holds
  /// captions in a file that is separate from the video container. Set up sidecar
  /// captions in the same output group, but different output from your video. For
  /// more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/scc-srt-output-captions.html.
  final SccDestinationSettings? sccDestinationSettings;

  /// Settings related to SRT captions. SRT is a sidecar format that holds
  /// captions in a file that is separate from the video container. Set up sidecar
  /// captions in the same output group, but different output from your video.
  final SrtDestinationSettings? srtDestinationSettings;

  /// Settings related to teletext captions. Set up teletext captions in the same
  /// output as your video. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/teletext-output-captions.html.
  final TeletextDestinationSettings? teletextDestinationSettings;

  /// Settings related to TTML captions. TTML is a sidecar format that holds
  /// captions in a file that is separate from the video container. Set up sidecar
  /// captions in the same output group, but different output from your video. For
  /// more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/ttml-and-webvtt-output-captions.html.
  final TtmlDestinationSettings? ttmlDestinationSettings;

  /// Settings related to WebVTT captions. WebVTT is a sidecar format that holds
  /// captions in a file that is separate from the video container. Set up sidecar
  /// captions in the same output group, but different output from your video. For
  /// more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/ttml-and-webvtt-output-captions.html.
  final WebvttDestinationSettings? webvttDestinationSettings;

  CaptionDestinationSettings({
    this.burninDestinationSettings,
    this.destinationType,
    this.dvbSubDestinationSettings,
    this.embeddedDestinationSettings,
    this.imscDestinationSettings,
    this.sccDestinationSettings,
    this.srtDestinationSettings,
    this.teletextDestinationSettings,
    this.ttmlDestinationSettings,
    this.webvttDestinationSettings,
  });

  factory CaptionDestinationSettings.fromJson(Map<String, dynamic> json) {
    return CaptionDestinationSettings(
      burninDestinationSettings: json['burninDestinationSettings'] != null
          ? BurninDestinationSettings.fromJson(
              json['burninDestinationSettings'] as Map<String, dynamic>)
          : null,
      destinationType: (json['destinationType'] as String?)
          ?.let(CaptionDestinationType.fromString),
      dvbSubDestinationSettings: json['dvbSubDestinationSettings'] != null
          ? DvbSubDestinationSettings.fromJson(
              json['dvbSubDestinationSettings'] as Map<String, dynamic>)
          : null,
      embeddedDestinationSettings: json['embeddedDestinationSettings'] != null
          ? EmbeddedDestinationSettings.fromJson(
              json['embeddedDestinationSettings'] as Map<String, dynamic>)
          : null,
      imscDestinationSettings: json['imscDestinationSettings'] != null
          ? ImscDestinationSettings.fromJson(
              json['imscDestinationSettings'] as Map<String, dynamic>)
          : null,
      sccDestinationSettings: json['sccDestinationSettings'] != null
          ? SccDestinationSettings.fromJson(
              json['sccDestinationSettings'] as Map<String, dynamic>)
          : null,
      srtDestinationSettings: json['srtDestinationSettings'] != null
          ? SrtDestinationSettings.fromJson(
              json['srtDestinationSettings'] as Map<String, dynamic>)
          : null,
      teletextDestinationSettings: json['teletextDestinationSettings'] != null
          ? TeletextDestinationSettings.fromJson(
              json['teletextDestinationSettings'] as Map<String, dynamic>)
          : null,
      ttmlDestinationSettings: json['ttmlDestinationSettings'] != null
          ? TtmlDestinationSettings.fromJson(
              json['ttmlDestinationSettings'] as Map<String, dynamic>)
          : null,
      webvttDestinationSettings: json['webvttDestinationSettings'] != null
          ? WebvttDestinationSettings.fromJson(
              json['webvttDestinationSettings'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final burninDestinationSettings = this.burninDestinationSettings;
    final destinationType = this.destinationType;
    final dvbSubDestinationSettings = this.dvbSubDestinationSettings;
    final embeddedDestinationSettings = this.embeddedDestinationSettings;
    final imscDestinationSettings = this.imscDestinationSettings;
    final sccDestinationSettings = this.sccDestinationSettings;
    final srtDestinationSettings = this.srtDestinationSettings;
    final teletextDestinationSettings = this.teletextDestinationSettings;
    final ttmlDestinationSettings = this.ttmlDestinationSettings;
    final webvttDestinationSettings = this.webvttDestinationSettings;
    return {
      if (burninDestinationSettings != null)
        'burninDestinationSettings': burninDestinationSettings,
      if (destinationType != null) 'destinationType': destinationType.value,
      if (dvbSubDestinationSettings != null)
        'dvbSubDestinationSettings': dvbSubDestinationSettings,
      if (embeddedDestinationSettings != null)
        'embeddedDestinationSettings': embeddedDestinationSettings,
      if (imscDestinationSettings != null)
        'imscDestinationSettings': imscDestinationSettings,
      if (sccDestinationSettings != null)
        'sccDestinationSettings': sccDestinationSettings,
      if (srtDestinationSettings != null)
        'srtDestinationSettings': srtDestinationSettings,
      if (teletextDestinationSettings != null)
        'teletextDestinationSettings': teletextDestinationSettings,
      if (ttmlDestinationSettings != null)
        'ttmlDestinationSettings': ttmlDestinationSettings,
      if (webvttDestinationSettings != null)
        'webvttDestinationSettings': webvttDestinationSettings,
    };
  }
}

/// Specify the format for this set of captions on this output. The default
/// format is embedded without SCTE-20. Note that your choice of video output
/// container constrains your choice of output captions format. For more
/// information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/captions-support-tables.html.
/// If you are using SCTE-20 and you want to create an output that complies with
/// the SCTE-43 spec, choose SCTE-20 plus embedded. To create a non-compliant
/// output where the embedded captions come first, choose Embedded plus SCTE-20.
enum CaptionDestinationType {
  burnIn('BURN_IN'),
  dvbSub('DVB_SUB'),
  embedded('EMBEDDED'),
  embeddedPlusScte20('EMBEDDED_PLUS_SCTE20'),
  imsc('IMSC'),
  scte20PlusEmbedded('SCTE20_PLUS_EMBEDDED'),
  scc('SCC'),
  srt('SRT'),
  smi('SMI'),
  teletext('TELETEXT'),
  ttml('TTML'),
  webvtt('WEBVTT'),
  ;

  final String value;

  const CaptionDestinationType(this.value);

  static CaptionDestinationType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CaptionDestinationType'));
}

/// Use captions selectors to specify the captions data from your input that you
/// use in your outputs. You can use up to 100 captions selectors per input.
class CaptionSelector {
  /// The specific language to extract from source, using the ISO 639-2 or ISO
  /// 639-3 three-letter language code. If input is SCTE-27, complete this field
  /// and/or PID to select the caption language to extract. If input is DVB-Sub
  /// and output is Burn-in, complete this field and/or PID to select the caption
  /// language to extract. If input is DVB-Sub that is being passed through, omit
  /// this field (and PID field); there is no way to extract a specific language
  /// with pass-through captions.
  final String? customLanguageCode;

  /// The specific language to extract from source. If input is SCTE-27, complete
  /// this field and/or PID to select the caption language to extract. If input is
  /// DVB-Sub and output is Burn-in, complete this field and/or PID to select the
  /// caption language to extract. If input is DVB-Sub that is being passed
  /// through, omit this field (and PID field); there is no way to extract a
  /// specific language with pass-through captions.
  final LanguageCode? languageCode;

  /// If your input captions are SCC, TTML, STL, SMI, SRT, or IMSC in an xml file,
  /// specify the URI of the input captions source file. If your input captions
  /// are IMSC in an IMF package, use TrackSourceSettings instead of
  /// FileSoureSettings.
  final CaptionSourceSettings? sourceSettings;

  CaptionSelector({
    this.customLanguageCode,
    this.languageCode,
    this.sourceSettings,
  });

  factory CaptionSelector.fromJson(Map<String, dynamic> json) {
    return CaptionSelector(
      customLanguageCode: json['customLanguageCode'] as String?,
      languageCode:
          (json['languageCode'] as String?)?.let(LanguageCode.fromString),
      sourceSettings: json['sourceSettings'] != null
          ? CaptionSourceSettings.fromJson(
              json['sourceSettings'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final customLanguageCode = this.customLanguageCode;
    final languageCode = this.languageCode;
    final sourceSettings = this.sourceSettings;
    return {
      if (customLanguageCode != null) 'customLanguageCode': customLanguageCode,
      if (languageCode != null) 'languageCode': languageCode.value,
      if (sourceSettings != null) 'sourceSettings': sourceSettings,
    };
  }
}

/// Choose the presentation style of your input SCC captions. To use the same
/// presentation style as your input: Keep the default value, Disabled. To
/// convert paint-on captions to pop-on: Choose Enabled. We also recommend that
/// you choose Enabled if you notice additional repeated lines in your output
/// captions.
enum CaptionSourceConvertPaintOnToPopOn {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const CaptionSourceConvertPaintOnToPopOn(this.value);

  static CaptionSourceConvertPaintOnToPopOn fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CaptionSourceConvertPaintOnToPopOn'));
}

/// Ignore this setting unless your input captions format is SCC. To have the
/// service compensate for differing frame rates between your input captions and
/// input video, specify the frame rate of the captions file. Specify this value
/// as a fraction. For example, you might specify 24 / 1 for 24 fps, 25 / 1 for
/// 25 fps, 24000 / 1001 for 23.976 fps, or 30000 / 1001 for 29.97 fps.
class CaptionSourceFramerate {
  /// Specify the denominator of the fraction that represents the frame rate for
  /// the setting Caption source frame rate. Use this setting along with the
  /// setting Framerate numerator.
  final int? framerateDenominator;

  /// Specify the numerator of the fraction that represents the frame rate for the
  /// setting Caption source frame rate. Use this setting along with the setting
  /// Framerate denominator.
  final int? framerateNumerator;

  CaptionSourceFramerate({
    this.framerateDenominator,
    this.framerateNumerator,
  });

  factory CaptionSourceFramerate.fromJson(Map<String, dynamic> json) {
    return CaptionSourceFramerate(
      framerateDenominator: json['framerateDenominator'] as int?,
      framerateNumerator: json['framerateNumerator'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final framerateDenominator = this.framerateDenominator;
    final framerateNumerator = this.framerateNumerator;
    return {
      if (framerateDenominator != null)
        'framerateDenominator': framerateDenominator,
      if (framerateNumerator != null) 'framerateNumerator': framerateNumerator,
    };
  }
}

/// If your input captions are SCC, TTML, STL, SMI, SRT, or IMSC in an xml file,
/// specify the URI of the input captions source file. If your input captions
/// are IMSC in an IMF package, use TrackSourceSettings instead of
/// FileSoureSettings.
class CaptionSourceSettings {
  /// Settings for ancillary captions source.
  final AncillarySourceSettings? ancillarySourceSettings;

  /// DVB Sub Source Settings
  final DvbSubSourceSettings? dvbSubSourceSettings;

  /// Settings for embedded captions Source
  final EmbeddedSourceSettings? embeddedSourceSettings;

  /// If your input captions are SCC, SMI, SRT, STL, TTML, WebVTT, or IMSC 1.1 in
  /// an xml file, specify the URI of the input caption source file. If your
  /// caption source is IMSC in an IMF package, use TrackSourceSettings instead of
  /// FileSoureSettings.
  final FileSourceSettings? fileSourceSettings;

  /// Use Source to identify the format of your input captions. The service cannot
  /// auto-detect caption format.
  final CaptionSourceType? sourceType;

  /// Settings specific to Teletext caption sources, including Page number.
  final TeletextSourceSettings? teletextSourceSettings;

  /// Settings specific to caption sources that are specified by track number.
  /// Currently, this is only IMSC captions in an IMF package. If your caption
  /// source is IMSC 1.1 in a separate xml file, use FileSourceSettings instead of
  /// TrackSourceSettings.
  final TrackSourceSettings? trackSourceSettings;

  /// Settings specific to WebVTT sources in HLS alternative rendition group.
  /// Specify the properties (renditionGroupId, renditionName or
  /// renditionLanguageCode) to identify the unique subtitle track among the
  /// alternative rendition groups present in the HLS manifest. If no unique track
  /// is found, or multiple tracks match the specified properties, the job fails.
  /// If there is only one subtitle track in the rendition group, the settings can
  /// be left empty and the default subtitle track will be chosen. If your caption
  /// source is a sidecar file, use FileSourceSettings instead of
  /// WebvttHlsSourceSettings.
  final WebvttHlsSourceSettings? webvttHlsSourceSettings;

  CaptionSourceSettings({
    this.ancillarySourceSettings,
    this.dvbSubSourceSettings,
    this.embeddedSourceSettings,
    this.fileSourceSettings,
    this.sourceType,
    this.teletextSourceSettings,
    this.trackSourceSettings,
    this.webvttHlsSourceSettings,
  });

  factory CaptionSourceSettings.fromJson(Map<String, dynamic> json) {
    return CaptionSourceSettings(
      ancillarySourceSettings: json['ancillarySourceSettings'] != null
          ? AncillarySourceSettings.fromJson(
              json['ancillarySourceSettings'] as Map<String, dynamic>)
          : null,
      dvbSubSourceSettings: json['dvbSubSourceSettings'] != null
          ? DvbSubSourceSettings.fromJson(
              json['dvbSubSourceSettings'] as Map<String, dynamic>)
          : null,
      embeddedSourceSettings: json['embeddedSourceSettings'] != null
          ? EmbeddedSourceSettings.fromJson(
              json['embeddedSourceSettings'] as Map<String, dynamic>)
          : null,
      fileSourceSettings: json['fileSourceSettings'] != null
          ? FileSourceSettings.fromJson(
              json['fileSourceSettings'] as Map<String, dynamic>)
          : null,
      sourceType:
          (json['sourceType'] as String?)?.let(CaptionSourceType.fromString),
      teletextSourceSettings: json['teletextSourceSettings'] != null
          ? TeletextSourceSettings.fromJson(
              json['teletextSourceSettings'] as Map<String, dynamic>)
          : null,
      trackSourceSettings: json['trackSourceSettings'] != null
          ? TrackSourceSettings.fromJson(
              json['trackSourceSettings'] as Map<String, dynamic>)
          : null,
      webvttHlsSourceSettings: json['webvttHlsSourceSettings'] != null
          ? WebvttHlsSourceSettings.fromJson(
              json['webvttHlsSourceSettings'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final ancillarySourceSettings = this.ancillarySourceSettings;
    final dvbSubSourceSettings = this.dvbSubSourceSettings;
    final embeddedSourceSettings = this.embeddedSourceSettings;
    final fileSourceSettings = this.fileSourceSettings;
    final sourceType = this.sourceType;
    final teletextSourceSettings = this.teletextSourceSettings;
    final trackSourceSettings = this.trackSourceSettings;
    final webvttHlsSourceSettings = this.webvttHlsSourceSettings;
    return {
      if (ancillarySourceSettings != null)
        'ancillarySourceSettings': ancillarySourceSettings,
      if (dvbSubSourceSettings != null)
        'dvbSubSourceSettings': dvbSubSourceSettings,
      if (embeddedSourceSettings != null)
        'embeddedSourceSettings': embeddedSourceSettings,
      if (fileSourceSettings != null) 'fileSourceSettings': fileSourceSettings,
      if (sourceType != null) 'sourceType': sourceType.value,
      if (teletextSourceSettings != null)
        'teletextSourceSettings': teletextSourceSettings,
      if (trackSourceSettings != null)
        'trackSourceSettings': trackSourceSettings,
      if (webvttHlsSourceSettings != null)
        'webvttHlsSourceSettings': webvttHlsSourceSettings,
    };
  }
}

/// Use Source to identify the format of your input captions. The service cannot
/// auto-detect caption format.
enum CaptionSourceType {
  ancillary('ANCILLARY'),
  dvbSub('DVB_SUB'),
  embedded('EMBEDDED'),
  scte20('SCTE20'),
  scc('SCC'),
  ttml('TTML'),
  stl('STL'),
  srt('SRT'),
  smi('SMI'),
  smpteTt('SMPTE_TT'),
  teletext('TELETEXT'),
  nullSource('NULL_SOURCE'),
  imsc('IMSC'),
  webvtt('WEBVTT'),
  ;

  final String value;

  const CaptionSourceType(this.value);

  static CaptionSourceType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum CaptionSourceType'));
}

/// Channel mapping contains the group of fields that hold the remixing value
/// for each channel, in dB. Specify remix values to indicate how much of the
/// content from your input audio channel you want in your output audio
/// channels. Each instance of the InputChannels or InputChannelsFineTune array
/// specifies these values for one output channel. Use one instance of this
/// array for each output channel. In the console, each array corresponds to a
/// column in the graphical depiction of the mapping matrix. The rows of the
/// graphical matrix correspond to input channels. Valid values are within the
/// range from -60 (mute) through 6. A setting of 0 passes the input channel
/// unchanged to the output channel (no attenuation or amplification). Use
/// InputChannels or InputChannelsFineTune to specify your remix values. Don't
/// use both.
class ChannelMapping {
  /// In your JSON job specification, include one child of OutputChannels for each
  /// audio channel that you want in your output. Each child should contain one
  /// instance of InputChannels or InputChannelsFineTune.
  final List<OutputChannelMapping>? outputChannels;

  ChannelMapping({
    this.outputChannels,
  });

  factory ChannelMapping.fromJson(Map<String, dynamic> json) {
    return ChannelMapping(
      outputChannels: (json['outputChannels'] as List?)
          ?.nonNulls
          .map((e) => OutputChannelMapping.fromJson(e as Map<String, dynamic>))
          .toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final outputChannels = this.outputChannels;
    return {
      if (outputChannels != null) 'outputChannels': outputChannels,
    };
  }
}

/// Specify YUV limits and RGB tolerances when you set Sample range conversion
/// to Limited range clip.
class ClipLimits {
  /// Specify the Maximum RGB color sample range tolerance for your output.
  /// MediaConvert corrects any YUV values that, when converted to RGB, would be
  /// outside the upper tolerance that you specify. Enter an integer from 90 to
  /// 105 as an offset percentage to the maximum possible value. Leave blank to
  /// use the default value 100. When you specify a value for Maximum RGB
  /// tolerance, you must set Sample range conversion to Limited range clip.
  final int? maximumRGBTolerance;

  /// Specify the Maximum YUV color sample limit. MediaConvert conforms any pixels
  /// in your input above the value that you specify to typical limited range
  /// bounds. Enter an integer from 920 to 1023. Leave blank to use the default
  /// value 940. The value that you enter applies to 10-bit ranges. For 8-bit
  /// ranges, MediaConvert automatically scales this value down. When you specify
  /// a value for Maximum YUV, you must set Sample range conversion to Limited
  /// range clip.
  final int? maximumYUV;

  /// Specify the Minimum RGB color sample range tolerance for your output.
  /// MediaConvert corrects any YUV values that, when converted to RGB, would be
  /// outside the lower tolerance that you specify. Enter an integer from -5 to 10
  /// as an offset percentage to the minimum possible value. Leave blank to use
  /// the default value 0. When you specify a value for Minimum RGB tolerance, you
  /// must set Sample range conversion to Limited range clip.
  final int? minimumRGBTolerance;

  /// Specify the Minimum YUV color sample limit. MediaConvert conforms any pixels
  /// in your input below the value that you specify to typical limited range
  /// bounds. Enter an integer from 0 to 128. Leave blank to use the default value
  /// 64. The value that you enter applies to 10-bit ranges. For 8-bit ranges,
  /// MediaConvert automatically scales this value down. When you specify a value
  /// for Minumum YUV, you must set Sample range conversion to Limited range clip.
  final int? minimumYUV;

  ClipLimits({
    this.maximumRGBTolerance,
    this.maximumYUV,
    this.minimumRGBTolerance,
    this.minimumYUV,
  });

  factory ClipLimits.fromJson(Map<String, dynamic> json) {
    return ClipLimits(
      maximumRGBTolerance: json['maximumRGBTolerance'] as int?,
      maximumYUV: json['maximumYUV'] as int?,
      minimumRGBTolerance: json['minimumRGBTolerance'] as int?,
      minimumYUV: json['minimumYUV'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final maximumRGBTolerance = this.maximumRGBTolerance;
    final maximumYUV = this.maximumYUV;
    final minimumRGBTolerance = this.minimumRGBTolerance;
    final minimumYUV = this.minimumYUV;
    return {
      if (maximumRGBTolerance != null)
        'maximumRGBTolerance': maximumRGBTolerance,
      if (maximumYUV != null) 'maximumYUV': maximumYUV,
      if (minimumRGBTolerance != null)
        'minimumRGBTolerance': minimumRGBTolerance,
      if (minimumYUV != null) 'minimumYUV': minimumYUV,
    };
  }
}

/// Specify the details for each pair of HLS and DASH additional manifests that
/// you want the service to generate for this CMAF output group. Each pair of
/// manifests can reference a different subset of outputs in the group.
class CmafAdditionalManifest {
  /// Specify a name modifier that the service adds to the name of this manifest
  /// to make it different from the file names of the other main manifests in the
  /// output group. For example, say that the default main manifest for your HLS
  /// group is film-name.m3u8. If you enter "-no-premium" for this setting, then
  /// the file name the service generates for this top-level manifest is
  /// film-name-no-premium.m3u8. For HLS output groups, specify a
  /// manifestNameModifier that is different from the nameModifier of the output.
  /// The service uses the output name modifier to create unique names for the
  /// individual variant manifests.
  final String? manifestNameModifier;

  /// Specify the outputs that you want this additional top-level manifest to
  /// reference.
  final List<String>? selectedOutputs;

  CmafAdditionalManifest({
    this.manifestNameModifier,
    this.selectedOutputs,
  });

  factory CmafAdditionalManifest.fromJson(Map<String, dynamic> json) {
    return CmafAdditionalManifest(
      manifestNameModifier: json['manifestNameModifier'] as String?,
      selectedOutputs: (json['selectedOutputs'] as List?)
          ?.nonNulls
          .map((e) => e as String)
          .toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final manifestNameModifier = this.manifestNameModifier;
    final selectedOutputs = this.selectedOutputs;
    return {
      if (manifestNameModifier != null)
        'manifestNameModifier': manifestNameModifier,
      if (selectedOutputs != null) 'selectedOutputs': selectedOutputs,
    };
  }
}

/// Disable this setting only when your workflow requires the
/// #EXT-X-ALLOW-CACHE:no tag. Otherwise, keep the default value Enabled and
/// control caching in your video distribution set up. For example, use the
/// Cache-Control http header.
enum CmafClientCache {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const CmafClientCache(this.value);

  static CmafClientCache fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum CmafClientCache'));
}

/// Specification to use (RFC-6381 or the default RFC-4281) during m3u8 playlist
/// generation.
enum CmafCodecSpecification {
  rfc_6381('RFC_6381'),
  rfc_4281('RFC_4281'),
  ;

  final String value;

  const CmafCodecSpecification(this.value);

  static CmafCodecSpecification fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CmafCodecSpecification'));
}

/// Settings for CMAF encryption
class CmafEncryptionSettings {
  /// This is a 128-bit, 16-byte hex value represented by a 32-character text
  /// string. If this parameter is not set then the Initialization Vector will
  /// follow the segment number by default.
  final String? constantInitializationVector;

  /// Specify the encryption scheme that you want the service to use when
  /// encrypting your CMAF segments. Choose AES-CBC subsample or AES_CTR.
  final CmafEncryptionType? encryptionMethod;

  /// When you use DRM with CMAF outputs, choose whether the service writes the
  /// 128-bit encryption initialization vector in the HLS and DASH manifests.
  final CmafInitializationVectorInManifest? initializationVectorInManifest;

  /// If your output group type is CMAF, use these settings when doing DRM
  /// encryption with a SPEKE-compliant key provider. If your output group type is
  /// HLS, DASH, or Microsoft Smooth, use the SpekeKeyProvider settings instead.
  final SpekeKeyProviderCmaf? spekeKeyProvider;

  /// Use these settings to set up encryption with a static key provider.
  final StaticKeyProvider? staticKeyProvider;

  /// Specify whether your DRM encryption key is static or from a key provider
  /// that follows the SPEKE standard. For more information about SPEKE, see
  /// https://docs.aws.amazon.com/speke/latest/documentation/what-is-speke.html.
  final CmafKeyProviderType? type;

  CmafEncryptionSettings({
    this.constantInitializationVector,
    this.encryptionMethod,
    this.initializationVectorInManifest,
    this.spekeKeyProvider,
    this.staticKeyProvider,
    this.type,
  });

  factory CmafEncryptionSettings.fromJson(Map<String, dynamic> json) {
    return CmafEncryptionSettings(
      constantInitializationVector:
          json['constantInitializationVector'] as String?,
      encryptionMethod: (json['encryptionMethod'] as String?)
          ?.let(CmafEncryptionType.fromString),
      initializationVectorInManifest:
          (json['initializationVectorInManifest'] as String?)
              ?.let(CmafInitializationVectorInManifest.fromString),
      spekeKeyProvider: json['spekeKeyProvider'] != null
          ? SpekeKeyProviderCmaf.fromJson(
              json['spekeKeyProvider'] as Map<String, dynamic>)
          : null,
      staticKeyProvider: json['staticKeyProvider'] != null
          ? StaticKeyProvider.fromJson(
              json['staticKeyProvider'] as Map<String, dynamic>)
          : null,
      type: (json['type'] as String?)?.let(CmafKeyProviderType.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final constantInitializationVector = this.constantInitializationVector;
    final encryptionMethod = this.encryptionMethod;
    final initializationVectorInManifest = this.initializationVectorInManifest;
    final spekeKeyProvider = this.spekeKeyProvider;
    final staticKeyProvider = this.staticKeyProvider;
    final type = this.type;
    return {
      if (constantInitializationVector != null)
        'constantInitializationVector': constantInitializationVector,
      if (encryptionMethod != null) 'encryptionMethod': encryptionMethod.value,
      if (initializationVectorInManifest != null)
        'initializationVectorInManifest': initializationVectorInManifest.value,
      if (spekeKeyProvider != null) 'spekeKeyProvider': spekeKeyProvider,
      if (staticKeyProvider != null) 'staticKeyProvider': staticKeyProvider,
      if (type != null) 'type': type.value,
    };
  }
}

/// Specify the encryption scheme that you want the service to use when
/// encrypting your CMAF segments. Choose AES-CBC subsample or AES_CTR.
enum CmafEncryptionType {
  sampleAes('SAMPLE_AES'),
  aesCtr('AES_CTR'),
  ;

  final String value;

  const CmafEncryptionType(this.value);

  static CmafEncryptionType fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum CmafEncryptionType'));
}

/// Settings related to your CMAF output package. For more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/outputs-file-ABR.html.
class CmafGroupSettings {
  /// By default, the service creates one top-level .m3u8 HLS manifest and one top
  /// -level .mpd DASH manifest for each CMAF output group in your job. These
  /// default manifests reference every output in the output group. To create
  /// additional top-level manifests that reference a subset of the outputs in the
  /// output group, specify a list of them here. For each additional manifest that
  /// you specify, the service creates one HLS manifest and one DASH manifest.
  final List<CmafAdditionalManifest>? additionalManifests;

  /// A partial URI prefix that will be put in the manifest file at the top level
  /// BaseURL element. Can be used if streams are delivered from a different URL
  /// than the manifest file.
  final String? baseUrl;

  /// Disable this setting only when your workflow requires the
  /// #EXT-X-ALLOW-CACHE:no tag. Otherwise, keep the default value Enabled and
  /// control caching in your video distribution set up. For example, use the
  /// Cache-Control http header.
  final CmafClientCache? clientCache;

  /// Specification to use (RFC-6381 or the default RFC-4281) during m3u8 playlist
  /// generation.
  final CmafCodecSpecification? codecSpecification;

  /// Specify whether MediaConvert generates I-frame only video segments for DASH
  /// trick play, also known as trick mode. When specified, the I-frame only video
  /// segments are included within an additional AdaptationSet in your DASH output
  /// manifest. To generate I-frame only video segments: Enter a name as a text
  /// string, up to 256 character long. This name is appended to the end of this
  /// output group's base filename, that you specify as part of your destination
  /// URI, and used for the I-frame only video segment files. You may also include
  /// format identifiers. For more information, see:
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/using-variables-in-your-job-settings.html#using-settings-variables-with-streaming-outputs
  /// To not generate I-frame only video segments: Leave blank.
  final String? dashIFrameTrickPlayNameModifier;

  /// Specify how MediaConvert writes SegmentTimeline in your output DASH
  /// manifest. To write a SegmentTimeline in each video Representation: Keep the
  /// default value, Basic. To write a common SegmentTimeline in the video
  /// AdaptationSet: Choose Compact. Note that MediaConvert will still write a
  /// SegmentTimeline in any Representation that does not share a common timeline.
  /// To write a video AdaptationSet for each different output framerate, and a
  /// common SegmentTimeline in each AdaptationSet: Choose Distinct.
  final DashManifestStyle? dashManifestStyle;

  /// Use Destination to specify the S3 output location and the output filename
  /// base. Destination accepts format identifiers. If you do not specify the base
  /// filename in the URI, the service will use the filename of the input file. If
  /// your job has multiple inputs, the service uses the filename of the first
  /// input file.
  final String? destination;

  /// Settings associated with the destination. Will vary based on the type of
  /// destination
  final DestinationSettings? destinationSettings;

  /// DRM settings.
  final CmafEncryptionSettings? encryption;

  /// Specify the length, in whole seconds, of the mp4 fragments. When you don't
  /// specify a value, MediaConvert defaults to 2. Related setting: Use Fragment
  /// length control to specify whether the encoder enforces this value strictly.
  final int? fragmentLength;

  /// Specify whether MediaConvert generates images for trick play. Keep the
  /// default value, None, to not generate any images. Choose Thumbnail to
  /// generate tiled thumbnails. Choose Thumbnail and full frame to generate tiled
  /// thumbnails and full-resolution images of single frames. When you enable
  /// Write HLS manifest, MediaConvert creates a child manifest for each set of
  /// images that you generate and adds corresponding entries to the parent
  /// manifest. When you enable Write DASH manifest, MediaConvert adds an entry in
  /// the .mpd manifest for each set of images that you generate. A common
  /// application for these images is Roku trick mode. The thumbnails and
  /// full-frame images that MediaConvert creates with this feature are compatible
  /// with this Roku specification:
  /// https://developer.roku.com/docs/developer-program/media-playback/trick-mode/hls-and-dash.md
  final CmafImageBasedTrickPlay? imageBasedTrickPlay;

  /// Tile and thumbnail settings applicable when imageBasedTrickPlay is ADVANCED
  final CmafImageBasedTrickPlaySettings? imageBasedTrickPlaySettings;

  /// When set to GZIP, compresses HLS playlist.
  final CmafManifestCompression? manifestCompression;

  /// Indicates whether the output manifest should use floating point values for
  /// segment duration.
  final CmafManifestDurationFormat? manifestDurationFormat;

  /// Minimum time of initially buffered media that is needed to ensure smooth
  /// playout.
  final int? minBufferTime;

  /// Keep this setting at the default value of 0, unless you are troubleshooting
  /// a problem with how devices play back the end of your video asset. If you
  /// know that player devices are hanging on the final segment of your video
  /// because the length of your final segment is too short, use this setting to
  /// specify a minimum final segment length, in seconds. Choose a value that is
  /// greater than or equal to 1 and less than your segment length. When you
  /// specify a value for this setting, the encoder will combine any final segment
  /// that is shorter than the length that you specify with the previous segment.
  /// For example, your segment length is 3 seconds and your final segment is .5
  /// seconds without a minimum final segment length; when you set the minimum
  /// final segment length to 1, your final segment is 3.5 seconds.
  final double? minFinalSegmentLength;

  /// Specify how the value for bandwidth is determined for each video
  /// Representation in your output MPD manifest. We recommend that you choose a
  /// MPD manifest bandwidth type that is compatible with your downstream player
  /// configuration. Max: Use the same value that you specify for Max bitrate in
  /// the video output, in bits per second. Average: Use the calculated average
  /// bitrate of the encoded video output, in bits per second.
  final CmafMpdManifestBandwidthType? mpdManifestBandwidthType;

  /// Specify whether your DASH profile is on-demand or main. When you choose Main
  /// profile, the service signals urn:mpeg:dash:profile:isoff-main:2011 in your
  /// .mpd DASH manifest. When you choose On-demand, the service signals
  /// urn:mpeg:dash:profile:isoff-on-demand:2011 in your .mpd. When you choose
  /// On-demand, you must also set the output group setting Segment control to
  /// Single file.
  final CmafMpdProfile? mpdProfile;

  /// Use this setting only when your output video stream has B-frames, which
  /// causes the initial presentation time stamp (PTS) to be offset from the
  /// initial decode time stamp (DTS). Specify how MediaConvert handles PTS when
  /// writing time stamps in output DASH manifests. Choose Match initial PTS when
  /// you want MediaConvert to use the initial PTS as the first time stamp in the
  /// manifest. Choose Zero-based to have MediaConvert ignore the initial PTS in
  /// the video stream and instead write the initial time stamp as zero in the
  /// manifest. For outputs that don't have B-frames, the time stamps in your DASH
  /// manifests start at zero regardless of your choice here.
  final CmafPtsOffsetHandlingForBFrames? ptsOffsetHandlingForBFrames;

  /// When set to SINGLE_FILE, a single output file is generated, which is
  /// internally segmented using the Fragment Length and Segment Length. When set
  /// to SEGMENTED_FILES, separate segment files will be created.
  final CmafSegmentControl? segmentControl;

  /// Specify the length, in whole seconds, of each segment. When you don't
  /// specify a value, MediaConvert defaults to 10. Related settings: Use Segment
  /// length control to specify whether the encoder enforces this value strictly.
  /// Use Segment control to specify whether MediaConvert creates separate segment
  /// files or one content file that has metadata to mark the segment boundaries.
  final int? segmentLength;

  /// Specify how you want MediaConvert to determine the segment length. Choose
  /// Exact to have the encoder use the exact length that you specify with the
  /// setting Segment length. This might result in extra I-frames. Choose Multiple
  /// of GOP to have the encoder round up the segment lengths to match the next
  /// GOP boundary.
  final CmafSegmentLengthControl? segmentLengthControl;

  /// Include or exclude RESOLUTION attribute for video in EXT-X-STREAM-INF tag of
  /// variant manifest.
  final CmafStreamInfResolution? streamInfResolution;

  /// When set to LEGACY, the segment target duration is always rounded up to the
  /// nearest integer value above its current value in seconds. When set to
  /// SPEC\\_COMPLIANT, the segment target duration is rounded up to the nearest
  /// integer value if fraction seconds are greater than or equal to 0.5 (>= 0.5)
  /// and rounded down if less than 0.5 (< 0.5). You may need to use LEGACY if
  /// your client needs to ensure that the target duration is always longer than
  /// the actual duration of the segment. Some older players may experience
  /// interrupted playback when the actual duration of a track in a segment is
  /// longer than the target duration.
  final CmafTargetDurationCompatibilityMode? targetDurationCompatibilityMode;

  /// Specify the video sample composition time offset mode in the output fMP4
  /// TRUN box. For wider player compatibility, set Video composition offsets to
  /// Unsigned or leave blank. The earliest presentation time may be greater than
  /// zero, and sample composition time offsets will increment using unsigned
  /// integers. For strict fMP4 video and audio timing, set Video composition
  /// offsets to Signed. The earliest presentation time will be equal to zero, and
  /// sample composition time offsets will increment using signed integers.
  final CmafVideoCompositionOffsets? videoCompositionOffsets;

  /// When set to ENABLED, a DASH MPD manifest will be generated for this output.
  final CmafWriteDASHManifest? writeDashManifest;

  /// When set to ENABLED, an Apple HLS manifest will be generated for this
  /// output.
  final CmafWriteHLSManifest? writeHlsManifest;

  /// When you enable Precise segment duration in DASH manifests, your DASH
  /// manifest shows precise segment durations. The segment duration information
  /// appears inside the SegmentTimeline element, inside SegmentTemplate at the
  /// Representation level. When this feature isn't enabled, the segment durations
  /// in your DASH manifest are approximate. The segment duration information
  /// appears in the duration attribute of the SegmentTemplate element.
  final CmafWriteSegmentTimelineInRepresentation?
      writeSegmentTimelineInRepresentation;

  CmafGroupSettings({
    this.additionalManifests,
    this.baseUrl,
    this.clientCache,
    this.codecSpecification,
    this.dashIFrameTrickPlayNameModifier,
    this.dashManifestStyle,
    this.destination,
    this.destinationSettings,
    this.encryption,
    this.fragmentLength,
    this.imageBasedTrickPlay,
    this.imageBasedTrickPlaySettings,
    this.manifestCompression,
    this.manifestDurationFormat,
    this.minBufferTime,
    this.minFinalSegmentLength,
    this.mpdManifestBandwidthType,
    this.mpdProfile,
    this.ptsOffsetHandlingForBFrames,
    this.segmentControl,
    this.segmentLength,
    this.segmentLengthControl,
    this.streamInfResolution,
    this.targetDurationCompatibilityMode,
    this.videoCompositionOffsets,
    this.writeDashManifest,
    this.writeHlsManifest,
    this.writeSegmentTimelineInRepresentation,
  });

  factory CmafGroupSettings.fromJson(Map<String, dynamic> json) {
    return CmafGroupSettings(
      additionalManifests: (json['additionalManifests'] as List?)
          ?.nonNulls
          .map(
              (e) => CmafAdditionalManifest.fromJson(e as Map<String, dynamic>))
          .toList(),
      baseUrl: json['baseUrl'] as String?,
      clientCache:
          (json['clientCache'] as String?)?.let(CmafClientCache.fromString),
      codecSpecification: (json['codecSpecification'] as String?)
          ?.let(CmafCodecSpecification.fromString),
      dashIFrameTrickPlayNameModifier:
          json['dashIFrameTrickPlayNameModifier'] as String?,
      dashManifestStyle: (json['dashManifestStyle'] as String?)
          ?.let(DashManifestStyle.fromString),
      destination: json['destination'] as String?,
      destinationSettings: json['destinationSettings'] != null
          ? DestinationSettings.fromJson(
              json['destinationSettings'] as Map<String, dynamic>)
          : null,
      encryption: json['encryption'] != null
          ? CmafEncryptionSettings.fromJson(
              json['encryption'] as Map<String, dynamic>)
          : null,
      fragmentLength: json['fragmentLength'] as int?,
      imageBasedTrickPlay: (json['imageBasedTrickPlay'] as String?)
          ?.let(CmafImageBasedTrickPlay.fromString),
      imageBasedTrickPlaySettings: json['imageBasedTrickPlaySettings'] != null
          ? CmafImageBasedTrickPlaySettings.fromJson(
              json['imageBasedTrickPlaySettings'] as Map<String, dynamic>)
          : null,
      manifestCompression: (json['manifestCompression'] as String?)
          ?.let(CmafManifestCompression.fromString),
      manifestDurationFormat: (json['manifestDurationFormat'] as String?)
          ?.let(CmafManifestDurationFormat.fromString),
      minBufferTime: json['minBufferTime'] as int?,
      minFinalSegmentLength: json['minFinalSegmentLength'] as double?,
      mpdManifestBandwidthType: (json['mpdManifestBandwidthType'] as String?)
          ?.let(CmafMpdManifestBandwidthType.fromString),
      mpdProfile:
          (json['mpdProfile'] as String?)?.let(CmafMpdProfile.fromString),
      ptsOffsetHandlingForBFrames:
          (json['ptsOffsetHandlingForBFrames'] as String?)
              ?.let(CmafPtsOffsetHandlingForBFrames.fromString),
      segmentControl: (json['segmentControl'] as String?)
          ?.let(CmafSegmentControl.fromString),
      segmentLength: json['segmentLength'] as int?,
      segmentLengthControl: (json['segmentLengthControl'] as String?)
          ?.let(CmafSegmentLengthControl.fromString),
      streamInfResolution: (json['streamInfResolution'] as String?)
          ?.let(CmafStreamInfResolution.fromString),
      targetDurationCompatibilityMode:
          (json['targetDurationCompatibilityMode'] as String?)
              ?.let(CmafTargetDurationCompatibilityMode.fromString),
      videoCompositionOffsets: (json['videoCompositionOffsets'] as String?)
          ?.let(CmafVideoCompositionOffsets.fromString),
      writeDashManifest: (json['writeDashManifest'] as String?)
          ?.let(CmafWriteDASHManifest.fromString),
      writeHlsManifest: (json['writeHlsManifest'] as String?)
          ?.let(CmafWriteHLSManifest.fromString),
      writeSegmentTimelineInRepresentation:
          (json['writeSegmentTimelineInRepresentation'] as String?)
              ?.let(CmafWriteSegmentTimelineInRepresentation.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final additionalManifests = this.additionalManifests;
    final baseUrl = this.baseUrl;
    final clientCache = this.clientCache;
    final codecSpecification = this.codecSpecification;
    final dashIFrameTrickPlayNameModifier =
        this.dashIFrameTrickPlayNameModifier;
    final dashManifestStyle = this.dashManifestStyle;
    final destination = this.destination;
    final destinationSettings = this.destinationSettings;
    final encryption = this.encryption;
    final fragmentLength = this.fragmentLength;
    final imageBasedTrickPlay = this.imageBasedTrickPlay;
    final imageBasedTrickPlaySettings = this.imageBasedTrickPlaySettings;
    final manifestCompression = this.manifestCompression;
    final manifestDurationFormat = this.manifestDurationFormat;
    final minBufferTime = this.minBufferTime;
    final minFinalSegmentLength = this.minFinalSegmentLength;
    final mpdManifestBandwidthType = this.mpdManifestBandwidthType;
    final mpdProfile = this.mpdProfile;
    final ptsOffsetHandlingForBFrames = this.ptsOffsetHandlingForBFrames;
    final segmentControl = this.segmentControl;
    final segmentLength = this.segmentLength;
    final segmentLengthControl = this.segmentLengthControl;
    final streamInfResolution = this.streamInfResolution;
    final targetDurationCompatibilityMode =
        this.targetDurationCompatibilityMode;
    final videoCompositionOffsets = this.videoCompositionOffsets;
    final writeDashManifest = this.writeDashManifest;
    final writeHlsManifest = this.writeHlsManifest;
    final writeSegmentTimelineInRepresentation =
        this.writeSegmentTimelineInRepresentation;
    return {
      if (additionalManifests != null)
        'additionalManifests': additionalManifests,
      if (baseUrl != null) 'baseUrl': baseUrl,
      if (clientCache != null) 'clientCache': clientCache.value,
      if (codecSpecification != null)
        'codecSpecification': codecSpecification.value,
      if (dashIFrameTrickPlayNameModifier != null)
        'dashIFrameTrickPlayNameModifier': dashIFrameTrickPlayNameModifier,
      if (dashManifestStyle != null)
        'dashManifestStyle': dashManifestStyle.value,
      if (destination != null) 'destination': destination,
      if (destinationSettings != null)
        'destinationSettings': destinationSettings,
      if (encryption != null) 'encryption': encryption,
      if (fragmentLength != null) 'fragmentLength': fragmentLength,
      if (imageBasedTrickPlay != null)
        'imageBasedTrickPlay': imageBasedTrickPlay.value,
      if (imageBasedTrickPlaySettings != null)
        'imageBasedTrickPlaySettings': imageBasedTrickPlaySettings,
      if (manifestCompression != null)
        'manifestCompression': manifestCompression.value,
      if (manifestDurationFormat != null)
        'manifestDurationFormat': manifestDurationFormat.value,
      if (minBufferTime != null) 'minBufferTime': minBufferTime,
      if (minFinalSegmentLength != null)
        'minFinalSegmentLength': minFinalSegmentLength,
      if (mpdManifestBandwidthType != null)
        'mpdManifestBandwidthType': mpdManifestBandwidthType.value,
      if (mpdProfile != null) 'mpdProfile': mpdProfile.value,
      if (ptsOffsetHandlingForBFrames != null)
        'ptsOffsetHandlingForBFrames': ptsOffsetHandlingForBFrames.value,
      if (segmentControl != null) 'segmentControl': segmentControl.value,
      if (segmentLength != null) 'segmentLength': segmentLength,
      if (segmentLengthControl != null)
        'segmentLengthControl': segmentLengthControl.value,
      if (streamInfResolution != null)
        'streamInfResolution': streamInfResolution.value,
      if (targetDurationCompatibilityMode != null)
        'targetDurationCompatibilityMode':
            targetDurationCompatibilityMode.value,
      if (videoCompositionOffsets != null)
        'videoCompositionOffsets': videoCompositionOffsets.value,
      if (writeDashManifest != null)
        'writeDashManifest': writeDashManifest.value,
      if (writeHlsManifest != null) 'writeHlsManifest': writeHlsManifest.value,
      if (writeSegmentTimelineInRepresentation != null)
        'writeSegmentTimelineInRepresentation':
            writeSegmentTimelineInRepresentation.value,
    };
  }
}

/// Specify whether MediaConvert generates images for trick play. Keep the
/// default value, None, to not generate any images. Choose Thumbnail to
/// generate tiled thumbnails. Choose Thumbnail and full frame to generate tiled
/// thumbnails and full-resolution images of single frames. When you enable
/// Write HLS manifest, MediaConvert creates a child manifest for each set of
/// images that you generate and adds corresponding entries to the parent
/// manifest. When you enable Write DASH manifest, MediaConvert adds an entry in
/// the .mpd manifest for each set of images that you generate. A common
/// application for these images is Roku trick mode. The thumbnails and
/// full-frame images that MediaConvert creates with this feature are compatible
/// with this Roku specification:
/// https://developer.roku.com/docs/developer-program/media-playback/trick-mode/hls-and-dash.md
enum CmafImageBasedTrickPlay {
  none('NONE'),
  thumbnail('THUMBNAIL'),
  thumbnailAndFullframe('THUMBNAIL_AND_FULLFRAME'),
  advanced('ADVANCED'),
  ;

  final String value;

  const CmafImageBasedTrickPlay(this.value);

  static CmafImageBasedTrickPlay fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CmafImageBasedTrickPlay'));
}

/// Tile and thumbnail settings applicable when imageBasedTrickPlay is ADVANCED
class CmafImageBasedTrickPlaySettings {
  /// The cadence MediaConvert follows for generating thumbnails. If set to
  /// FOLLOW_IFRAME, MediaConvert generates thumbnails for each IDR frame in the
  /// output (matching the GOP cadence). If set to FOLLOW_CUSTOM, MediaConvert
  /// generates thumbnails according to the interval you specify in
  /// thumbnailInterval.
  final CmafIntervalCadence? intervalCadence;

  /// Height of each thumbnail within each tile image, in pixels. Leave blank to
  /// maintain aspect ratio with thumbnail width. If following the aspect ratio
  /// would lead to a total tile height greater than 4096, then the job will be
  /// rejected. Must be divisible by 2.
  final int? thumbnailHeight;

  /// Enter the interval, in seconds, that MediaConvert uses to generate
  /// thumbnails. If the interval you enter doesn't align with the output frame
  /// rate, MediaConvert automatically rounds the interval to align with the
  /// output frame rate. For example, if the output frame rate is 29.97 frames per
  /// second and you enter 5, MediaConvert uses a 150 frame interval to generate
  /// thumbnails.
  final double? thumbnailInterval;

  /// Width of each thumbnail within each tile image, in pixels. Default is 312.
  /// Must be divisible by 8.
  final int? thumbnailWidth;

  /// Number of thumbnails in each column of a tile image. Set a value between 2
  /// and 2048. Must be divisible by 2.
  final int? tileHeight;

  /// Number of thumbnails in each row of a tile image. Set a value between 1 and
  /// 512.
  final int? tileWidth;

  CmafImageBasedTrickPlaySettings({
    this.intervalCadence,
    this.thumbnailHeight,
    this.thumbnailInterval,
    this.thumbnailWidth,
    this.tileHeight,
    this.tileWidth,
  });

  factory CmafImageBasedTrickPlaySettings.fromJson(Map<String, dynamic> json) {
    return CmafImageBasedTrickPlaySettings(
      intervalCadence: (json['intervalCadence'] as String?)
          ?.let(CmafIntervalCadence.fromString),
      thumbnailHeight: json['thumbnailHeight'] as int?,
      thumbnailInterval: json['thumbnailInterval'] as double?,
      thumbnailWidth: json['thumbnailWidth'] as int?,
      tileHeight: json['tileHeight'] as int?,
      tileWidth: json['tileWidth'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final intervalCadence = this.intervalCadence;
    final thumbnailHeight = this.thumbnailHeight;
    final thumbnailInterval = this.thumbnailInterval;
    final thumbnailWidth = this.thumbnailWidth;
    final tileHeight = this.tileHeight;
    final tileWidth = this.tileWidth;
    return {
      if (intervalCadence != null) 'intervalCadence': intervalCadence.value,
      if (thumbnailHeight != null) 'thumbnailHeight': thumbnailHeight,
      if (thumbnailInterval != null) 'thumbnailInterval': thumbnailInterval,
      if (thumbnailWidth != null) 'thumbnailWidth': thumbnailWidth,
      if (tileHeight != null) 'tileHeight': tileHeight,
      if (tileWidth != null) 'tileWidth': tileWidth,
    };
  }
}

/// When you use DRM with CMAF outputs, choose whether the service writes the
/// 128-bit encryption initialization vector in the HLS and DASH manifests.
enum CmafInitializationVectorInManifest {
  include('INCLUDE'),
  exclude('EXCLUDE'),
  ;

  final String value;

  const CmafInitializationVectorInManifest(this.value);

  static CmafInitializationVectorInManifest fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CmafInitializationVectorInManifest'));
}

/// The cadence MediaConvert follows for generating thumbnails. If set to
/// FOLLOW_IFRAME, MediaConvert generates thumbnails for each IDR frame in the
/// output (matching the GOP cadence). If set to FOLLOW_CUSTOM, MediaConvert
/// generates thumbnails according to the interval you specify in
/// thumbnailInterval.
enum CmafIntervalCadence {
  followIframe('FOLLOW_IFRAME'),
  followCustom('FOLLOW_CUSTOM'),
  ;

  final String value;

  const CmafIntervalCadence(this.value);

  static CmafIntervalCadence fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum CmafIntervalCadence'));
}

/// Specify whether your DRM encryption key is static or from a key provider
/// that follows the SPEKE standard. For more information about SPEKE, see
/// https://docs.aws.amazon.com/speke/latest/documentation/what-is-speke.html.
enum CmafKeyProviderType {
  speke('SPEKE'),
  staticKey('STATIC_KEY'),
  ;

  final String value;

  const CmafKeyProviderType(this.value);

  static CmafKeyProviderType fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum CmafKeyProviderType'));
}

/// When set to GZIP, compresses HLS playlist.
enum CmafManifestCompression {
  gzip('GZIP'),
  none('NONE'),
  ;

  final String value;

  const CmafManifestCompression(this.value);

  static CmafManifestCompression fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CmafManifestCompression'));
}

/// Indicates whether the output manifest should use floating point values for
/// segment duration.
enum CmafManifestDurationFormat {
  floatingPoint('FLOATING_POINT'),
  integer('INTEGER'),
  ;

  final String value;

  const CmafManifestDurationFormat(this.value);

  static CmafManifestDurationFormat fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CmafManifestDurationFormat'));
}

/// Specify how the value for bandwidth is determined for each video
/// Representation in your output MPD manifest. We recommend that you choose a
/// MPD manifest bandwidth type that is compatible with your downstream player
/// configuration. Max: Use the same value that you specify for Max bitrate in
/// the video output, in bits per second. Average: Use the calculated average
/// bitrate of the encoded video output, in bits per second.
enum CmafMpdManifestBandwidthType {
  average('AVERAGE'),
  max('MAX'),
  ;

  final String value;

  const CmafMpdManifestBandwidthType(this.value);

  static CmafMpdManifestBandwidthType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CmafMpdManifestBandwidthType'));
}

/// Specify whether your DASH profile is on-demand or main. When you choose Main
/// profile, the service signals urn:mpeg:dash:profile:isoff-main:2011 in your
/// .mpd DASH manifest. When you choose On-demand, the service signals
/// urn:mpeg:dash:profile:isoff-on-demand:2011 in your .mpd. When you choose
/// On-demand, you must also set the output group setting Segment control to
/// Single file.
enum CmafMpdProfile {
  mainProfile('MAIN_PROFILE'),
  onDemandProfile('ON_DEMAND_PROFILE'),
  ;

  final String value;

  const CmafMpdProfile(this.value);

  static CmafMpdProfile fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum CmafMpdProfile'));
}

/// Use this setting only when your output video stream has B-frames, which
/// causes the initial presentation time stamp (PTS) to be offset from the
/// initial decode time stamp (DTS). Specify how MediaConvert handles PTS when
/// writing time stamps in output DASH manifests. Choose Match initial PTS when
/// you want MediaConvert to use the initial PTS as the first time stamp in the
/// manifest. Choose Zero-based to have MediaConvert ignore the initial PTS in
/// the video stream and instead write the initial time stamp as zero in the
/// manifest. For outputs that don't have B-frames, the time stamps in your DASH
/// manifests start at zero regardless of your choice here.
enum CmafPtsOffsetHandlingForBFrames {
  zeroBased('ZERO_BASED'),
  matchInitialPts('MATCH_INITIAL_PTS'),
  ;

  final String value;

  const CmafPtsOffsetHandlingForBFrames(this.value);

  static CmafPtsOffsetHandlingForBFrames fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CmafPtsOffsetHandlingForBFrames'));
}

/// When set to SINGLE_FILE, a single output file is generated, which is
/// internally segmented using the Fragment Length and Segment Length. When set
/// to SEGMENTED_FILES, separate segment files will be created.
enum CmafSegmentControl {
  singleFile('SINGLE_FILE'),
  segmentedFiles('SEGMENTED_FILES'),
  ;

  final String value;

  const CmafSegmentControl(this.value);

  static CmafSegmentControl fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum CmafSegmentControl'));
}

/// Specify how you want MediaConvert to determine the segment length. Choose
/// Exact to have the encoder use the exact length that you specify with the
/// setting Segment length. This might result in extra I-frames. Choose Multiple
/// of GOP to have the encoder round up the segment lengths to match the next
/// GOP boundary.
enum CmafSegmentLengthControl {
  exact('EXACT'),
  gopMultiple('GOP_MULTIPLE'),
  ;

  final String value;

  const CmafSegmentLengthControl(this.value);

  static CmafSegmentLengthControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CmafSegmentLengthControl'));
}

/// Include or exclude RESOLUTION attribute for video in EXT-X-STREAM-INF tag of
/// variant manifest.
enum CmafStreamInfResolution {
  include('INCLUDE'),
  exclude('EXCLUDE'),
  ;

  final String value;

  const CmafStreamInfResolution(this.value);

  static CmafStreamInfResolution fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CmafStreamInfResolution'));
}

/// When set to LEGACY, the segment target duration is always rounded up to the
/// nearest integer value above its current value in seconds. When set to
/// SPEC\\_COMPLIANT, the segment target duration is rounded up to the nearest
/// integer value if fraction seconds are greater than or equal to 0.5 (>= 0.5)
/// and rounded down if less than 0.5 (< 0.5). You may need to use LEGACY if
/// your client needs to ensure that the target duration is always longer than
/// the actual duration of the segment. Some older players may experience
/// interrupted playback when the actual duration of a track in a segment is
/// longer than the target duration.
enum CmafTargetDurationCompatibilityMode {
  legacy('LEGACY'),
  specCompliant('SPEC_COMPLIANT'),
  ;

  final String value;

  const CmafTargetDurationCompatibilityMode(this.value);

  static CmafTargetDurationCompatibilityMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CmafTargetDurationCompatibilityMode'));
}

/// Specify the video sample composition time offset mode in the output fMP4
/// TRUN box. For wider player compatibility, set Video composition offsets to
/// Unsigned or leave blank. The earliest presentation time may be greater than
/// zero, and sample composition time offsets will increment using unsigned
/// integers. For strict fMP4 video and audio timing, set Video composition
/// offsets to Signed. The earliest presentation time will be equal to zero, and
/// sample composition time offsets will increment using signed integers.
enum CmafVideoCompositionOffsets {
  signed('SIGNED'),
  unsigned('UNSIGNED'),
  ;

  final String value;

  const CmafVideoCompositionOffsets(this.value);

  static CmafVideoCompositionOffsets fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CmafVideoCompositionOffsets'));
}

/// When set to ENABLED, a DASH MPD manifest will be generated for this output.
enum CmafWriteDASHManifest {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const CmafWriteDASHManifest(this.value);

  static CmafWriteDASHManifest fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum CmafWriteDASHManifest'));
}

/// When set to ENABLED, an Apple HLS manifest will be generated for this
/// output.
enum CmafWriteHLSManifest {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const CmafWriteHLSManifest(this.value);

  static CmafWriteHLSManifest fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum CmafWriteHLSManifest'));
}

/// When you enable Precise segment duration in DASH manifests, your DASH
/// manifest shows precise segment durations. The segment duration information
/// appears inside the SegmentTimeline element, inside SegmentTemplate at the
/// Representation level. When this feature isn't enabled, the segment durations
/// in your DASH manifest are approximate. The segment duration information
/// appears in the duration attribute of the SegmentTemplate element.
enum CmafWriteSegmentTimelineInRepresentation {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const CmafWriteSegmentTimelineInRepresentation(this.value);

  static CmafWriteSegmentTimelineInRepresentation fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CmafWriteSegmentTimelineInRepresentation'));
}

/// Specify this setting only when your output will be consumed by a downstream
/// repackaging workflow that is sensitive to very small duration differences
/// between video and audio. For this situation, choose Match video duration. In
/// all other cases, keep the default value, Default codec duration. When you
/// choose Match video duration, MediaConvert pads the output audio streams with
/// silence or trims them to ensure that the total duration of each audio stream
/// is at least as long as the total duration of the video stream. After padding
/// or trimming, the audio stream duration is no more than one frame longer than
/// the video stream. MediaConvert applies audio padding or trimming only to the
/// end of the last segment of the output. For unsegmented outputs, MediaConvert
/// adds padding only to the end of the file. When you keep the default value,
/// any minor discrepancies between audio and video duration will depend on your
/// output audio codec.
enum CmfcAudioDuration {
  defaultCodecDuration('DEFAULT_CODEC_DURATION'),
  matchVideoDuration('MATCH_VIDEO_DURATION'),
  ;

  final String value;

  const CmfcAudioDuration(this.value);

  static CmfcAudioDuration fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum CmfcAudioDuration'));
}

/// Use this setting to control the values that MediaConvert puts in your HLS
/// parent playlist to control how the client player selects which audio track
/// to play. Choose Audio-only variant stream (AUDIO_ONLY_VARIANT_STREAM) for
/// any variant that you want to prohibit the client from playing with video.
/// This causes MediaConvert to represent the variant as an EXT-X-STREAM-INF in
/// the HLS manifest. The other options for this setting determine the values
/// that MediaConvert writes for the DEFAULT and AUTOSELECT attributes of the
/// EXT-X-MEDIA entry for the audio variant. For more information about these
/// attributes, see the Apple documentation article
/// https://developer.apple.com/documentation/http_live_streaming/example_playlists_for_http_live_streaming/adding_alternate_media_to_a_playlist.
/// Choose Alternate audio, auto select, default to set DEFAULT=YES and
/// AUTOSELECT=YES. Choose this value for only one variant in your output group.
/// Choose Alternate audio, auto select, not default to set DEFAULT=NO and
/// AUTOSELECT=YES. Choose Alternate Audio, Not Auto Select to set DEFAULT=NO
/// and AUTOSELECT=NO. When you don't specify a value for this setting,
/// MediaConvert defaults to Alternate audio, auto select, default. When there
/// is more than one variant in your output group, you must explicitly choose a
/// value for this setting.
enum CmfcAudioTrackType {
  alternateAudioAutoSelectDefault('ALTERNATE_AUDIO_AUTO_SELECT_DEFAULT'),
  alternateAudioAutoSelect('ALTERNATE_AUDIO_AUTO_SELECT'),
  alternateAudioNotAutoSelect('ALTERNATE_AUDIO_NOT_AUTO_SELECT'),
  audioOnlyVariantStream('AUDIO_ONLY_VARIANT_STREAM'),
  ;

  final String value;

  const CmfcAudioTrackType(this.value);

  static CmfcAudioTrackType fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum CmfcAudioTrackType'));
}

/// Specify whether to flag this audio track as descriptive video service (DVS)
/// in your HLS parent manifest. When you choose Flag, MediaConvert includes the
/// parameter CHARACTERISTICS="public.accessibility.describes-video" in the
/// EXT-X-MEDIA entry for this track. When you keep the default choice, Don't
/// flag, MediaConvert leaves this parameter out. The DVS flag can help with
/// accessibility on Apple devices. For more information, see the Apple
/// documentation.
enum CmfcDescriptiveVideoServiceFlag {
  dontFlag('DONT_FLAG'),
  flag('FLAG'),
  ;

  final String value;

  const CmfcDescriptiveVideoServiceFlag(this.value);

  static CmfcDescriptiveVideoServiceFlag fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CmfcDescriptiveVideoServiceFlag'));
}

/// Choose Include to have MediaConvert generate an HLS child manifest that
/// lists only the I-frames for this rendition, in addition to your regular
/// manifest for this rendition. You might use this manifest as part of a
/// workflow that creates preview functions for your video. MediaConvert adds
/// both the I-frame only child manifest and the regular child manifest to the
/// parent manifest. When you don't need the I-frame only child manifest, keep
/// the default value Exclude.
enum CmfcIFrameOnlyManifest {
  include('INCLUDE'),
  exclude('EXCLUDE'),
  ;

  final String value;

  const CmfcIFrameOnlyManifest(this.value);

  static CmfcIFrameOnlyManifest fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CmfcIFrameOnlyManifest'));
}

/// To include key-length-value metadata in this output: Set KLV metadata
/// insertion to Passthrough. MediaConvert reads KLV metadata present in your
/// input and writes each instance to a separate event message box in the
/// output, according to MISB ST1910.1. To exclude this KLV metadata: Set KLV
/// metadata insertion to None or leave blank.
enum CmfcKlvMetadata {
  passthrough('PASSTHROUGH'),
  none('NONE'),
  ;

  final String value;

  const CmfcKlvMetadata(this.value);

  static CmfcKlvMetadata fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum CmfcKlvMetadata'));
}

/// To add an InbandEventStream element in your output MPD manifest for each
/// type of event message, set Manifest metadata signaling to Enabled. For ID3
/// event messages, the InbandEventStream element schemeIdUri will be same value
/// that you specify for ID3 metadata scheme ID URI. For SCTE35 event messages,
/// the InbandEventStream element schemeIdUri will be
/// "urn:scte:scte35:2013:bin". To leave these elements out of your output MPD
/// manifest, set Manifest metadata signaling to Disabled. To enable Manifest
/// metadata signaling, you must also set SCTE-35 source to Passthrough, ESAM
/// SCTE-35 to insert, or ID3 metadata to Passthrough.
enum CmfcManifestMetadataSignaling {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const CmfcManifestMetadataSignaling(this.value);

  static CmfcManifestMetadataSignaling fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CmfcManifestMetadataSignaling'));
}

/// Use this setting only when you specify SCTE-35 markers from ESAM. Choose
/// INSERT to put SCTE-35 markers in this output at the insertion points that
/// you specify in an ESAM XML document. Provide the document in the setting SCC
/// XML.
enum CmfcScte35Esam {
  insert('INSERT'),
  none('NONE'),
  ;

  final String value;

  const CmfcScte35Esam(this.value);

  static CmfcScte35Esam fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum CmfcScte35Esam'));
}

/// Ignore this setting unless you have SCTE-35 markers in your input video
/// file. Choose Passthrough if you want SCTE-35 markers that appear in your
/// input to also appear in this output. Choose None if you don't want those
/// SCTE-35 markers in this output.
enum CmfcScte35Source {
  passthrough('PASSTHROUGH'),
  none('NONE'),
  ;

  final String value;

  const CmfcScte35Source(this.value);

  static CmfcScte35Source fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum CmfcScte35Source'));
}

/// These settings relate to the fragmented MP4 container for the segments in
/// your CMAF outputs.
class CmfcSettings {
  /// Specify this setting only when your output will be consumed by a downstream
  /// repackaging workflow that is sensitive to very small duration differences
  /// between video and audio. For this situation, choose Match video duration. In
  /// all other cases, keep the default value, Default codec duration. When you
  /// choose Match video duration, MediaConvert pads the output audio streams with
  /// silence or trims them to ensure that the total duration of each audio stream
  /// is at least as long as the total duration of the video stream. After padding
  /// or trimming, the audio stream duration is no more than one frame longer than
  /// the video stream. MediaConvert applies audio padding or trimming only to the
  /// end of the last segment of the output. For unsegmented outputs, MediaConvert
  /// adds padding only to the end of the file. When you keep the default value,
  /// any minor discrepancies between audio and video duration will depend on your
  /// output audio codec.
  final CmfcAudioDuration? audioDuration;

  /// Specify the audio rendition group for this audio rendition. Specify up to
  /// one value for each audio output in your output group. This value appears in
  /// your HLS parent manifest in the EXT-X-MEDIA tag of TYPE=AUDIO, as the value
  /// for the GROUP-ID attribute. For example, if you specify "audio_aac_1" for
  /// Audio group ID, it appears in your manifest like this:
  /// #EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID="audio_aac_1". Related setting: To
  /// associate the rendition group that this audio track belongs to with a video
  /// rendition, include the same value that you provide here for that video
  /// output's setting Audio rendition sets.
  final String? audioGroupId;

  /// List the audio rendition groups that you want included with this video
  /// rendition. Use a comma-separated list. For example, say you want to include
  /// the audio rendition groups that have the audio group IDs "audio_aac_1" and
  /// "audio_dolby". Then you would specify this value: "audio_aac_1,audio_dolby".
  /// Related setting: The rendition groups that you include in your
  /// comma-separated list should all match values that you specify in the setting
  /// Audio group ID for audio renditions in the same output group as this video
  /// rendition. Default behavior: If you don't specify anything here and for
  /// Audio group ID, MediaConvert puts each audio variant in its own audio
  /// rendition group and associates it with every video variant. Each value in
  /// your list appears in your HLS parent manifest in the EXT-X-STREAM-INF tag as
  /// the value for the AUDIO attribute. To continue the previous example, say
  /// that the file name for the child manifest for your video rendition is
  /// "amazing_video_1.m3u8". Then, in your parent manifest, each value will
  /// appear on separate lines, like this:
  /// #EXT-X-STREAM-INF:AUDIO="audio_aac_1"... amazing_video_1.m3u8
  /// #EXT-X-STREAM-INF:AUDIO="audio_dolby"... amazing_video_1.m3u8
  final String? audioRenditionSets;

  /// Use this setting to control the values that MediaConvert puts in your HLS
  /// parent playlist to control how the client player selects which audio track
  /// to play. Choose Audio-only variant stream (AUDIO_ONLY_VARIANT_STREAM) for
  /// any variant that you want to prohibit the client from playing with video.
  /// This causes MediaConvert to represent the variant as an EXT-X-STREAM-INF in
  /// the HLS manifest. The other options for this setting determine the values
  /// that MediaConvert writes for the DEFAULT and AUTOSELECT attributes of the
  /// EXT-X-MEDIA entry for the audio variant. For more information about these
  /// attributes, see the Apple documentation article
  /// https://developer.apple.com/documentation/http_live_streaming/example_playlists_for_http_live_streaming/adding_alternate_media_to_a_playlist.
  /// Choose Alternate audio, auto select, default to set DEFAULT=YES and
  /// AUTOSELECT=YES. Choose this value for only one variant in your output group.
  /// Choose Alternate audio, auto select, not default to set DEFAULT=NO and
  /// AUTOSELECT=YES. Choose Alternate Audio, Not Auto Select to set DEFAULT=NO
  /// and AUTOSELECT=NO. When you don't specify a value for this setting,
  /// MediaConvert defaults to Alternate audio, auto select, default. When there
  /// is more than one variant in your output group, you must explicitly choose a
  /// value for this setting.
  final CmfcAudioTrackType? audioTrackType;

  /// Specify whether to flag this audio track as descriptive video service (DVS)
  /// in your HLS parent manifest. When you choose Flag, MediaConvert includes the
  /// parameter CHARACTERISTICS="public.accessibility.describes-video" in the
  /// EXT-X-MEDIA entry for this track. When you keep the default choice, Don't
  /// flag, MediaConvert leaves this parameter out. The DVS flag can help with
  /// accessibility on Apple devices. For more information, see the Apple
  /// documentation.
  final CmfcDescriptiveVideoServiceFlag? descriptiveVideoServiceFlag;

  /// Choose Include to have MediaConvert generate an HLS child manifest that
  /// lists only the I-frames for this rendition, in addition to your regular
  /// manifest for this rendition. You might use this manifest as part of a
  /// workflow that creates preview functions for your video. MediaConvert adds
  /// both the I-frame only child manifest and the regular child manifest to the
  /// parent manifest. When you don't need the I-frame only child manifest, keep
  /// the default value Exclude.
  final CmfcIFrameOnlyManifest? iFrameOnlyManifest;

  /// To include key-length-value metadata in this output: Set KLV metadata
  /// insertion to Passthrough. MediaConvert reads KLV metadata present in your
  /// input and writes each instance to a separate event message box in the
  /// output, according to MISB ST1910.1. To exclude this KLV metadata: Set KLV
  /// metadata insertion to None or leave blank.
  final CmfcKlvMetadata? klvMetadata;

  /// To add an InbandEventStream element in your output MPD manifest for each
  /// type of event message, set Manifest metadata signaling to Enabled. For ID3
  /// event messages, the InbandEventStream element schemeIdUri will be same value
  /// that you specify for ID3 metadata scheme ID URI. For SCTE35 event messages,
  /// the InbandEventStream element schemeIdUri will be
  /// "urn:scte:scte35:2013:bin". To leave these elements out of your output MPD
  /// manifest, set Manifest metadata signaling to Disabled. To enable Manifest
  /// metadata signaling, you must also set SCTE-35 source to Passthrough, ESAM
  /// SCTE-35 to insert, or ID3 metadata to Passthrough.
  final CmfcManifestMetadataSignaling? manifestMetadataSignaling;

  /// Use this setting only when you specify SCTE-35 markers from ESAM. Choose
  /// INSERT to put SCTE-35 markers in this output at the insertion points that
  /// you specify in an ESAM XML document. Provide the document in the setting SCC
  /// XML.
  final CmfcScte35Esam? scte35Esam;

  /// Ignore this setting unless you have SCTE-35 markers in your input video
  /// file. Choose Passthrough if you want SCTE-35 markers that appear in your
  /// input to also appear in this output. Choose None if you don't want those
  /// SCTE-35 markers in this output.
  final CmfcScte35Source? scte35Source;

  /// To include ID3 metadata in this output: Set ID3 metadata to Passthrough.
  /// Specify this ID3 metadata in Custom ID3 metadata inserter. MediaConvert
  /// writes each instance of ID3 metadata in a separate Event Message (eMSG) box.
  /// To exclude this ID3 metadata: Set ID3 metadata to None or leave blank.
  final CmfcTimedMetadata? timedMetadata;

  /// Specify the event message box (eMSG) version for ID3 timed metadata in your
  /// output.
  /// For more information, see ISO/IEC 23009-1:2022 section 5.10.3.3.3 Syntax.
  /// Leave blank to use the default value Version 0.
  /// When you specify Version 1, you must also set ID3 metadata to Passthrough.
  final CmfcTimedMetadataBoxVersion? timedMetadataBoxVersion;

  /// Specify the event message box (eMSG) scheme ID URI for ID3 timed metadata in
  /// your output. For more information, see ISO/IEC 23009-1:2022 section
  /// 5.10.3.3.4 Semantics. Leave blank to use the default value:
  /// https://aomedia.org/emsg/ID3 When you specify a value for ID3 metadata
  /// scheme ID URI, you must also set ID3 metadata to Passthrough.
  final String? timedMetadataSchemeIdUri;

  /// Specify the event message box (eMSG) value for ID3 timed metadata in your
  /// output. For more information, see ISO/IEC 23009-1:2022 section 5.10.3.3.4
  /// Semantics. When you specify a value for ID3 Metadata Value, you must also
  /// set ID3 metadata to Passthrough.
  final String? timedMetadataValue;

  CmfcSettings({
    this.audioDuration,
    this.audioGroupId,
    this.audioRenditionSets,
    this.audioTrackType,
    this.descriptiveVideoServiceFlag,
    this.iFrameOnlyManifest,
    this.klvMetadata,
    this.manifestMetadataSignaling,
    this.scte35Esam,
    this.scte35Source,
    this.timedMetadata,
    this.timedMetadataBoxVersion,
    this.timedMetadataSchemeIdUri,
    this.timedMetadataValue,
  });

  factory CmfcSettings.fromJson(Map<String, dynamic> json) {
    return CmfcSettings(
      audioDuration:
          (json['audioDuration'] as String?)?.let(CmfcAudioDuration.fromString),
      audioGroupId: json['audioGroupId'] as String?,
      audioRenditionSets: json['audioRenditionSets'] as String?,
      audioTrackType: (json['audioTrackType'] as String?)
          ?.let(CmfcAudioTrackType.fromString),
      descriptiveVideoServiceFlag:
          (json['descriptiveVideoServiceFlag'] as String?)
              ?.let(CmfcDescriptiveVideoServiceFlag.fromString),
      iFrameOnlyManifest: (json['iFrameOnlyManifest'] as String?)
          ?.let(CmfcIFrameOnlyManifest.fromString),
      klvMetadata:
          (json['klvMetadata'] as String?)?.let(CmfcKlvMetadata.fromString),
      manifestMetadataSignaling: (json['manifestMetadataSignaling'] as String?)
          ?.let(CmfcManifestMetadataSignaling.fromString),
      scte35Esam:
          (json['scte35Esam'] as String?)?.let(CmfcScte35Esam.fromString),
      scte35Source:
          (json['scte35Source'] as String?)?.let(CmfcScte35Source.fromString),
      timedMetadata:
          (json['timedMetadata'] as String?)?.let(CmfcTimedMetadata.fromString),
      timedMetadataBoxVersion: (json['timedMetadataBoxVersion'] as String?)
          ?.let(CmfcTimedMetadataBoxVersion.fromString),
      timedMetadataSchemeIdUri: json['timedMetadataSchemeIdUri'] as String?,
      timedMetadataValue: json['timedMetadataValue'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final audioDuration = this.audioDuration;
    final audioGroupId = this.audioGroupId;
    final audioRenditionSets = this.audioRenditionSets;
    final audioTrackType = this.audioTrackType;
    final descriptiveVideoServiceFlag = this.descriptiveVideoServiceFlag;
    final iFrameOnlyManifest = this.iFrameOnlyManifest;
    final klvMetadata = this.klvMetadata;
    final manifestMetadataSignaling = this.manifestMetadataSignaling;
    final scte35Esam = this.scte35Esam;
    final scte35Source = this.scte35Source;
    final timedMetadata = this.timedMetadata;
    final timedMetadataBoxVersion = this.timedMetadataBoxVersion;
    final timedMetadataSchemeIdUri = this.timedMetadataSchemeIdUri;
    final timedMetadataValue = this.timedMetadataValue;
    return {
      if (audioDuration != null) 'audioDuration': audioDuration.value,
      if (audioGroupId != null) 'audioGroupId': audioGroupId,
      if (audioRenditionSets != null) 'audioRenditionSets': audioRenditionSets,
      if (audioTrackType != null) 'audioTrackType': audioTrackType.value,
      if (descriptiveVideoServiceFlag != null)
        'descriptiveVideoServiceFlag': descriptiveVideoServiceFlag.value,
      if (iFrameOnlyManifest != null)
        'iFrameOnlyManifest': iFrameOnlyManifest.value,
      if (klvMetadata != null) 'klvMetadata': klvMetadata.value,
      if (manifestMetadataSignaling != null)
        'manifestMetadataSignaling': manifestMetadataSignaling.value,
      if (scte35Esam != null) 'scte35Esam': scte35Esam.value,
      if (scte35Source != null) 'scte35Source': scte35Source.value,
      if (timedMetadata != null) 'timedMetadata': timedMetadata.value,
      if (timedMetadataBoxVersion != null)
        'timedMetadataBoxVersion': timedMetadataBoxVersion.value,
      if (timedMetadataSchemeIdUri != null)
        'timedMetadataSchemeIdUri': timedMetadataSchemeIdUri,
      if (timedMetadataValue != null) 'timedMetadataValue': timedMetadataValue,
    };
  }
}

/// To include ID3 metadata in this output: Set ID3 metadata to Passthrough.
/// Specify this ID3 metadata in Custom ID3 metadata inserter. MediaConvert
/// writes each instance of ID3 metadata in a separate Event Message (eMSG) box.
/// To exclude this ID3 metadata: Set ID3 metadata to None or leave blank.
enum CmfcTimedMetadata {
  passthrough('PASSTHROUGH'),
  none('NONE'),
  ;

  final String value;

  const CmfcTimedMetadata(this.value);

  static CmfcTimedMetadata fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum CmfcTimedMetadata'));
}

/// Specify the event message box (eMSG) version for ID3 timed metadata in your
/// output.
/// For more information, see ISO/IEC 23009-1:2022 section 5.10.3.3.3 Syntax.
/// Leave blank to use the default value Version 0.
/// When you specify Version 1, you must also set ID3 metadata to Passthrough.
enum CmfcTimedMetadataBoxVersion {
  version_0('VERSION_0'),
  version_1('VERSION_1'),
  ;

  final String value;

  const CmfcTimedMetadataBoxVersion(this.value);

  static CmfcTimedMetadataBoxVersion fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum CmfcTimedMetadataBoxVersion'));
}

/// Custom 3D lut settings
class ColorConversion3DLUTSetting {
  /// Specify the input file S3, HTTP, or HTTPS URL for your 3D LUT .cube file.
  /// Note that MediaConvert accepts 3D LUT files up to 8MB in size.
  final String? fileInput;

  /// Specify which inputs use this 3D LUT, according to their color space.
  final ColorSpace? inputColorSpace;

  /// Specify which inputs use this 3D LUT, according to their luminance. To apply
  /// this 3D LUT to HDR10 or P3D65 (HDR) inputs with a specific mastering
  /// luminance: Enter an integer from 0 to 2147483647, corresponding to the
  /// input's Maximum luminance value. To apply this 3D LUT to any input
  /// regardless of its luminance: Leave blank, or enter 0.
  final int? inputMasteringLuminance;

  /// Specify which outputs use this 3D LUT, according to their color space.
  final ColorSpace? outputColorSpace;

  /// Specify which outputs use this 3D LUT, according to their luminance. To
  /// apply this 3D LUT to HDR10 or P3D65 (HDR) outputs with a specific luminance:
  /// Enter an integer from 0 to 2147483647, corresponding to the output's
  /// luminance. To apply this 3D LUT to any output regardless of its luminance:
  /// Leave blank, or enter 0.
  final int? outputMasteringLuminance;

  ColorConversion3DLUTSetting({
    this.fileInput,
    this.inputColorSpace,
    this.inputMasteringLuminance,
    this.outputColorSpace,
    this.outputMasteringLuminance,
  });

  factory ColorConversion3DLUTSetting.fromJson(Map<String, dynamic> json) {
    return ColorConversion3DLUTSetting(
      fileInput: json['fileInput'] as String?,
      inputColorSpace:
          (json['inputColorSpace'] as String?)?.let(ColorSpace.fromString),
      inputMasteringLuminance: json['inputMasteringLuminance'] as int?,
      outputColorSpace:
          (json['outputColorSpace'] as String?)?.let(ColorSpace.fromString),
      outputMasteringLuminance: json['outputMasteringLuminance'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final fileInput = this.fileInput;
    final inputColorSpace = this.inputColorSpace;
    final inputMasteringLuminance = this.inputMasteringLuminance;
    final outputColorSpace = this.outputColorSpace;
    final outputMasteringLuminance = this.outputMasteringLuminance;
    return {
      if (fileInput != null) 'fileInput': fileInput,
      if (inputColorSpace != null) 'inputColorSpace': inputColorSpace.value,
      if (inputMasteringLuminance != null)
        'inputMasteringLuminance': inputMasteringLuminance,
      if (outputColorSpace != null) 'outputColorSpace': outputColorSpace.value,
      if (outputMasteringLuminance != null)
        'outputMasteringLuminance': outputMasteringLuminance,
    };
  }
}

/// Settings for color correction.
class ColorCorrector {
  /// Brightness level.
  final int? brightness;

  /// Specify YUV limits and RGB tolerances when you set Sample range conversion
  /// to Limited range clip.
  final ClipLimits? clipLimits;

  /// Specify the color space you want for this output. The service supports
  /// conversion between HDR formats, between SDR formats, from SDR to HDR, and
  /// from HDR to SDR. SDR to HDR conversion doesn't upgrade the dynamic range.
  /// The converted video has an HDR format, but visually appears the same as an
  /// unconverted output. HDR to SDR conversion uses tone mapping to approximate
  /// the outcome of manually regrading from HDR to SDR. When you specify an
  /// output color space, MediaConvert uses the following color space metadata,
  /// which includes color primaries, transfer characteristics, and matrix
  /// coefficients:
  /// * HDR 10: BT.2020, PQ, BT.2020 non-constant
  /// * HLG 2020: BT.2020, HLG, BT.2020 non-constant
  /// * P3DCI (Theater): DCIP3, SMPTE 428M, BT.709
  /// * P3D65 (SDR): Display P3, sRGB, BT.709
  /// * P3D65 (HDR): Display P3, PQ, BT.709
  final ColorSpaceConversion? colorSpaceConversion;

  /// Contrast level.
  final int? contrast;

  /// Use these settings when you convert to the HDR 10 color space. Specify the
  /// SMPTE ST 2086 Mastering Display Color Volume static metadata that you want
  /// signaled in the output. These values don't affect the pixel values that are
  /// encoded in the video stream. They are intended to help the downstream video
  /// player display content in a way that reflects the intentions of the the
  /// content creator. When you set Color space conversion to HDR 10, these
  /// settings are required. You must set values for Max frame average light level
  /// and Max content light level; these settings don't have a default value. The
  /// default values for the other HDR 10 metadata settings are defined by the
  /// P3D65 color space. For more information about MediaConvert HDR jobs, see
  /// https://docs.aws.amazon.com/console/mediaconvert/hdr.
  final Hdr10Metadata? hdr10Metadata;

  /// Specify how MediaConvert maps brightness and colors from your HDR input to
  /// your SDR output. The mode that you select represents a creative choice, with
  /// different tradeoffs in the details and tones of your output. To maintain
  /// details in bright or saturated areas of your output: Choose Preserve
  /// details. For some sources, your SDR output may look less bright and less
  /// saturated when compared to your HDR source. MediaConvert automatically
  /// applies this mode for HLG sources, regardless of your choice. For a bright
  /// and saturated output: Choose Vibrant. We recommend that you choose this mode
  /// when any of your source content is HDR10, and for the best results when it
  /// is mastered for 1000 nits. You may notice loss of details in bright or
  /// saturated areas of your output. HDR to SDR tone mapping has no effect when
  /// your input is SDR.
  final HDRToSDRToneMapper? hdrToSdrToneMapper;

  /// Hue in degrees.
  final int? hue;

  /// Specify the maximum mastering display luminance. Enter an integer from 0 to
  /// 2147483647, in units of 0.0001 nits. For example, enter 10000000 for 1000
  /// nits.
  final int? maxLuminance;

  /// Specify how MediaConvert limits the color sample range for this output. To
  /// create a limited range output from a full range input: Choose Limited range
  /// squeeze. For full range inputs, MediaConvert performs a linear offset to
  /// color samples equally across all pixels and frames. Color samples in 10-bit
  /// outputs are limited to 64 through 940, and 8-bit outputs are limited to 16
  /// through 235. Note: For limited range inputs, values for color samples are
  /// passed through to your output unchanged. MediaConvert does not limit the
  /// sample range. To correct pixels in your input that are out of range or out
  /// of gamut: Choose Limited range clip. Use for broadcast applications.
  /// MediaConvert conforms any pixels outside of the values that you specify
  /// under Minimum YUV and Maximum YUV to limited range bounds. MediaConvert also
  /// corrects any YUV values that, when converted to RGB, would be outside the
  /// bounds you specify under Minimum RGB tolerance and Maximum RGB tolerance.
  /// With either limited range conversion, MediaConvert writes the sample range
  /// metadata in the output.
  final SampleRangeConversion? sampleRangeConversion;

  /// Saturation level.
  final int? saturation;

  /// Specify the reference white level, in nits, for all of your SDR inputs. Use
  /// to correct brightness levels within HDR10 outputs. The following color
  /// metadata must be present in your SDR input: color primaries, transfer
  /// characteristics, and matrix coefficients. If your SDR input has missing
  /// color metadata, or if you want to correct input color metadata, manually
  /// specify a color space in the input video selector. For 1,000 nit peak
  /// brightness displays, we recommend that you set SDR reference white level to
  /// 203 (according to ITU-R BT.2408). Leave blank to use the default value of
  /// 100, or specify an integer from 100 to 1000.
  final int? sdrReferenceWhiteLevel;

  ColorCorrector({
    this.brightness,
    this.clipLimits,
    this.colorSpaceConversion,
    this.contrast,
    this.hdr10Metadata,
    this.hdrToSdrToneMapper,
    this.hue,
    this.maxLuminance,
    this.sampleRangeConversion,
    this.saturation,
    this.sdrReferenceWhiteLevel,
  });

  factory ColorCorrector.fromJson(Map<String, dynamic> json) {
    return ColorCorrector(
      brightness: json['brightness'] as int?,
      clipLimits: json['clipLimits'] != null
          ? ClipLimits.fromJson(json['clipLimits'] as Map<String, dynamic>)
          : null,
      colorSpaceConversion: (json['colorSpaceConversion'] as String?)
          ?.let(ColorSpaceConversion.fromString),
      contrast: json['contrast'] as int?,
      hdr10Metadata: json['hdr10Metadata'] != null
          ? Hdr10Metadata.fromJson(
              json['hdr10Metadata'] as Map<String, dynamic>)
          : null,
      hdrToSdrToneMapper: (json['hdrToSdrToneMapper'] as String?)
          ?.let(HDRToSDRToneMapper.fromString),
      hue: json['hue'] as int?,
      maxLuminance: json['maxLuminance'] as int?,
      sampleRangeConversion: (json['sampleRangeConversion'] as String?)
          ?.let(SampleRangeConversion.fromString),
      saturation: json['saturation'] as int?,
      sdrReferenceWhiteLevel: json['sdrReferenceWhiteLevel'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final brightness = this.brightness;
    final clipLimits = this.clipLimits;
    final colorSpaceConversion = this.colorSpaceConversion;
    final contrast = this.contrast;
    final hdr10Metadata = this.hdr10Metadata;
    final hdrToSdrToneMapper = this.hdrToSdrToneMapper;
    final hue = this.hue;
    final maxLuminance = this.maxLuminance;
    final sampleRangeConversion = this.sampleRangeConversion;
    final saturation = this.saturation;
    final sdrReferenceWhiteLevel = this.sdrReferenceWhiteLevel;
    return {
      if (brightness != null) 'brightness': brightness,
      if (clipLimits != null) 'clipLimits': clipLimits,
      if (colorSpaceConversion != null)
        'colorSpaceConversion': colorSpaceConversion.value,
      if (contrast != null) 'contrast': contrast,
      if (hdr10Metadata != null) 'hdr10Metadata': hdr10Metadata,
      if (hdrToSdrToneMapper != null)
        'hdrToSdrToneMapper': hdrToSdrToneMapper.value,
      if (hue != null) 'hue': hue,
      if (maxLuminance != null) 'maxLuminance': maxLuminance,
      if (sampleRangeConversion != null)
        'sampleRangeConversion': sampleRangeConversion.value,
      if (saturation != null) 'saturation': saturation,
      if (sdrReferenceWhiteLevel != null)
        'sdrReferenceWhiteLevel': sdrReferenceWhiteLevel,
    };
  }
}

/// Choose Insert for this setting to include color metadata in this output.
/// Choose Ignore to exclude color metadata from this output. If you don't
/// specify a value, the service sets this to Insert by default.
enum ColorMetadata {
  ignore('IGNORE'),
  insert('INSERT'),
  ;

  final String value;

  const ColorMetadata(this.value);

  static ColorMetadata fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum ColorMetadata'));
}

/// If your input video has accurate color space metadata, or if you don't know
/// about color space: Keep the default value, Follow. MediaConvert will
/// automatically detect your input color space. If your input video has
/// metadata indicating the wrong color space, or has missing metadata: Specify
/// the accurate color space here. If your input video is HDR 10 and the SMPTE
/// ST 2086 Mastering Display Color Volume static metadata isn't present in your
/// video stream, or if that metadata is present but not accurate: Choose Force
/// HDR 10. Specify correct values in the input HDR 10 metadata settings. For
/// more information about HDR jobs, see
/// https://docs.aws.amazon.com/console/mediaconvert/hdr. When you specify an
/// input color space, MediaConvert uses the following color space metadata,
/// which includes color primaries, transfer characteristics, and matrix
/// coefficients:
/// * HDR 10: BT.2020, PQ, BT.2020 non-constant
/// * HLG 2020: BT.2020, HLG, BT.2020 non-constant
/// * P3DCI (Theater): DCIP3, SMPTE 428M, BT.709
/// * P3D65 (SDR): Display P3, sRGB, BT.709
/// * P3D65 (HDR): Display P3, PQ, BT.709
enum ColorSpace {
  follow('FOLLOW'),
  rec_601('REC_601'),
  rec_709('REC_709'),
  hdr10('HDR10'),
  hlg_2020('HLG_2020'),
  p3dci('P3DCI'),
  p3d65Sdr('P3D65_SDR'),
  p3d65Hdr('P3D65_HDR'),
  ;

  final String value;

  const ColorSpace(this.value);

  static ColorSpace fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum ColorSpace'));
}

/// Specify the color space you want for this output. The service supports
/// conversion between HDR formats, between SDR formats, from SDR to HDR, and
/// from HDR to SDR. SDR to HDR conversion doesn't upgrade the dynamic range.
/// The converted video has an HDR format, but visually appears the same as an
/// unconverted output. HDR to SDR conversion uses tone mapping to approximate
/// the outcome of manually regrading from HDR to SDR. When you specify an
/// output color space, MediaConvert uses the following color space metadata,
/// which includes color primaries, transfer characteristics, and matrix
/// coefficients:
/// * HDR 10: BT.2020, PQ, BT.2020 non-constant
/// * HLG 2020: BT.2020, HLG, BT.2020 non-constant
/// * P3DCI (Theater): DCIP3, SMPTE 428M, BT.709
/// * P3D65 (SDR): Display P3, sRGB, BT.709
/// * P3D65 (HDR): Display P3, PQ, BT.709
enum ColorSpaceConversion {
  none('NONE'),
  force_601('FORCE_601'),
  force_709('FORCE_709'),
  forceHdr10('FORCE_HDR10'),
  forceHlg_2020('FORCE_HLG_2020'),
  forceP3dci('FORCE_P3DCI'),
  forceP3d65Sdr('FORCE_P3D65_SDR'),
  forceP3d65Hdr('FORCE_P3D65_HDR'),
  ;

  final String value;

  const ColorSpaceConversion(this.value);

  static ColorSpaceConversion fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum ColorSpaceConversion'));
}

/// There are two sources for color metadata, the input file and the job input
/// settings Color space and HDR master display information settings. The Color
/// space usage setting determines which takes precedence. Choose Force to use
/// color metadata from the input job settings. If you don't specify values for
/// those settings, the service defaults to using metadata from your input.
/// FALLBACK - Choose Fallback to use color metadata from the source when it is
/// present. If there's no color metadata in your input file, the service
/// defaults to using values you specify in the input settings.
enum ColorSpaceUsage {
  force('FORCE'),
  fallback('FALLBACK'),
  ;

  final String value;

  const ColorSpaceUsage(this.value);

  static ColorSpaceUsage fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum ColorSpaceUsage'));
}

/// The length of the term of your reserved queue pricing plan commitment.
enum Commitment {
  oneYear('ONE_YEAR'),
  ;

  final String value;

  const Commitment(this.value);

  static Commitment fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum Commitment'));
}

/// Container specific settings.
class ContainerSettings {
  /// These settings relate to the fragmented MP4 container for the segments in
  /// your CMAF outputs.
  final CmfcSettings? cmfcSettings;

  /// Container for this output. Some containers require a container settings
  /// object. If not specified, the default object will be created.
  final ContainerType? container;

  /// Settings for F4v container
  final F4vSettings? f4vSettings;

  /// MPEG-2 TS container settings. These apply to outputs in a File output group
  /// when the output's container is MPEG-2 Transport Stream (M2TS). In these
  /// assets, data is organized by the program map table (PMT). Each transport
  /// stream program contains subsets of data, including audio, video, and
  /// metadata. Each of these subsets of data has a numerical label called a
  /// packet identifier (PID). Each transport stream program corresponds to one
  /// MediaConvert output. The PMT lists the types of data in a program along with
  /// their PID. Downstream systems and players use the program map table to look
  /// up the PID for each type of data it accesses and then uses the PIDs to
  /// locate specific data within the asset.
  final M2tsSettings? m2tsSettings;

  /// These settings relate to the MPEG-2 transport stream (MPEG2-TS) container
  /// for the MPEG2-TS segments in your HLS outputs.
  final M3u8Settings? m3u8Settings;

  /// These settings relate to your QuickTime MOV output container.
  final MovSettings? movSettings;

  /// These settings relate to your MP4 output container. You can create audio
  /// only outputs with this container. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/supported-codecs-containers-audio-only.html#output-codecs-and-containers-supported-for-audio-only.
  final Mp4Settings? mp4Settings;

  /// These settings relate to the fragmented MP4 container for the segments in
  /// your DASH outputs.
  final MpdSettings? mpdSettings;

  /// These settings relate to your MXF output container.
  final MxfSettings? mxfSettings;

  ContainerSettings({
    this.cmfcSettings,
    this.container,
    this.f4vSettings,
    this.m2tsSettings,
    this.m3u8Settings,
    this.movSettings,
    this.mp4Settings,
    this.mpdSettings,
    this.mxfSettings,
  });

  factory ContainerSettings.fromJson(Map<String, dynamic> json) {
    return ContainerSettings(
      cmfcSettings: json['cmfcSettings'] != null
          ? CmfcSettings.fromJson(json['cmfcSettings'] as Map<String, dynamic>)
          : null,
      container: (json['container'] as String?)?.let(ContainerType.fromString),
      f4vSettings: json['f4vSettings'] != null
          ? F4vSettings.fromJson(json['f4vSettings'] as Map<String, dynamic>)
          : null,
      m2tsSettings: json['m2tsSettings'] != null
          ? M2tsSettings.fromJson(json['m2tsSettings'] as Map<String, dynamic>)
          : null,
      m3u8Settings: json['m3u8Settings'] != null
          ? M3u8Settings.fromJson(json['m3u8Settings'] as Map<String, dynamic>)
          : null,
      movSettings: json['movSettings'] != null
          ? MovSettings.fromJson(json['movSettings'] as Map<String, dynamic>)
          : null,
      mp4Settings: json['mp4Settings'] != null
          ? Mp4Settings.fromJson(json['mp4Settings'] as Map<String, dynamic>)
          : null,
      mpdSettings: json['mpdSettings'] != null
          ? MpdSettings.fromJson(json['mpdSettings'] as Map<String, dynamic>)
          : null,
      mxfSettings: json['mxfSettings'] != null
          ? MxfSettings.fromJson(json['mxfSettings'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final cmfcSettings = this.cmfcSettings;
    final container = this.container;
    final f4vSettings = this.f4vSettings;
    final m2tsSettings = this.m2tsSettings;
    final m3u8Settings = this.m3u8Settings;
    final movSettings = this.movSettings;
    final mp4Settings = this.mp4Settings;
    final mpdSettings = this.mpdSettings;
    final mxfSettings = this.mxfSettings;
    return {
      if (cmfcSettings != null) 'cmfcSettings': cmfcSettings,
      if (container != null) 'container': container.value,
      if (f4vSettings != null) 'f4vSettings': f4vSettings,
      if (m2tsSettings != null) 'm2tsSettings': m2tsSettings,
      if (m3u8Settings != null) 'm3u8Settings': m3u8Settings,
      if (movSettings != null) 'movSettings': movSettings,
      if (mp4Settings != null) 'mp4Settings': mp4Settings,
      if (mpdSettings != null) 'mpdSettings': mpdSettings,
      if (mxfSettings != null) 'mxfSettings': mxfSettings,
    };
  }
}

/// Container for this output. Some containers require a container settings
/// object. If not specified, the default object will be created.
enum ContainerType {
  f4v('F4V'),
  ismv('ISMV'),
  m2ts('M2TS'),
  m3u8('M3U8'),
  cmfc('CMFC'),
  mov('MOV'),
  mp4('MP4'),
  mpd('MPD'),
  mxf('MXF'),
  webm('WEBM'),
  raw('RAW'),
  y4m('Y4M'),
  ;

  final String value;

  const ContainerType(this.value);

  static ContainerType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum ContainerType'));
}

/// The action to take on copy and redistribution control XDS packets. If you
/// select PASSTHROUGH, packets will not be changed. If you select STRIP, any
/// packets will be removed in output captions.
enum CopyProtectionAction {
  passthrough('PASSTHROUGH'),
  strip('STRIP'),
  ;

  final String value;

  const CopyProtectionAction(this.value);

  static CopyProtectionAction fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum CopyProtectionAction'));
}

class CreateJobResponse {
  /// Each job converts an input file into an output file or files. For more
  /// information, see the User Guide at
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/what-is.html
  final Job? job;

  CreateJobResponse({
    this.job,
  });

  factory CreateJobResponse.fromJson(Map<String, dynamic> json) {
    return CreateJobResponse(
      job: json['job'] != null
          ? Job.fromJson(json['job'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final job = this.job;
    return {
      if (job != null) 'job': job,
    };
  }
}

class CreateJobTemplateResponse {
  /// A job template is a pre-made set of encoding instructions that you can use
  /// to quickly create a job.
  final JobTemplate? jobTemplate;

  CreateJobTemplateResponse({
    this.jobTemplate,
  });

  factory CreateJobTemplateResponse.fromJson(Map<String, dynamic> json) {
    return CreateJobTemplateResponse(
      jobTemplate: json['jobTemplate'] != null
          ? JobTemplate.fromJson(json['jobTemplate'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final jobTemplate = this.jobTemplate;
    return {
      if (jobTemplate != null) 'jobTemplate': jobTemplate,
    };
  }
}

class CreatePresetResponse {
  /// A preset is a collection of preconfigured media conversion settings that you
  /// want MediaConvert to apply to the output during the conversion process.
  final Preset? preset;

  CreatePresetResponse({
    this.preset,
  });

  factory CreatePresetResponse.fromJson(Map<String, dynamic> json) {
    return CreatePresetResponse(
      preset: json['preset'] != null
          ? Preset.fromJson(json['preset'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final preset = this.preset;
    return {
      if (preset != null) 'preset': preset,
    };
  }
}

class CreateQueueResponse {
  /// You can use queues to manage the resources that are available to your AWS
  /// account for running multiple transcoding jobs at the same time. If you don't
  /// specify a queue, the service sends all jobs through the default queue. For
  /// more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/working-with-queues.html.
  final Queue? queue;

  CreateQueueResponse({
    this.queue,
  });

  factory CreateQueueResponse.fromJson(Map<String, dynamic> json) {
    return CreateQueueResponse(
      queue: json['queue'] != null
          ? Queue.fromJson(json['queue'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final queue = this.queue;
    return {
      if (queue != null) 'queue': queue,
    };
  }
}

/// Specify the details for each additional DASH manifest that you want the
/// service to generate for this output group. Each manifest can reference a
/// different subset of outputs in the group.
class DashAdditionalManifest {
  /// Specify a name modifier that the service adds to the name of this manifest
  /// to make it different from the file names of the other main manifests in the
  /// output group. For example, say that the default main manifest for your DASH
  /// group is film-name.mpd. If you enter "-no-premium" for this setting, then
  /// the file name the service generates for this top-level manifest is
  /// film-name-no-premium.mpd.
  final String? manifestNameModifier;

  /// Specify the outputs that you want this additional top-level manifest to
  /// reference.
  final List<String>? selectedOutputs;

  DashAdditionalManifest({
    this.manifestNameModifier,
    this.selectedOutputs,
  });

  factory DashAdditionalManifest.fromJson(Map<String, dynamic> json) {
    return DashAdditionalManifest(
      manifestNameModifier: json['manifestNameModifier'] as String?,
      selectedOutputs: (json['selectedOutputs'] as List?)
          ?.nonNulls
          .map((e) => e as String)
          .toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final manifestNameModifier = this.manifestNameModifier;
    final selectedOutputs = this.selectedOutputs;
    return {
      if (manifestNameModifier != null)
        'manifestNameModifier': manifestNameModifier,
      if (selectedOutputs != null) 'selectedOutputs': selectedOutputs,
    };
  }
}

/// Specifies DRM settings for DASH outputs.
class DashIsoEncryptionSettings {
  /// This setting can improve the compatibility of your output with video players
  /// on obsolete devices. It applies only to DASH H.264 outputs with DRM
  /// encryption. Choose Unencrypted SEI only to correct problems with playback on
  /// older devices. Otherwise, keep the default setting CENC v1. If you choose
  /// Unencrypted SEI, for that output, the service will exclude the access unit
  /// delimiter and will leave the SEI NAL units unencrypted.
  final DashIsoPlaybackDeviceCompatibility? playbackDeviceCompatibility;

  /// If your output group type is HLS, DASH, or Microsoft Smooth, use these
  /// settings when doing DRM encryption with a SPEKE-compliant key provider. If
  /// your output group type is CMAF, use the SpekeKeyProviderCmaf settings
  /// instead.
  final SpekeKeyProvider? spekeKeyProvider;

  DashIsoEncryptionSettings({
    this.playbackDeviceCompatibility,
    this.spekeKeyProvider,
  });

  factory DashIsoEncryptionSettings.fromJson(Map<String, dynamic> json) {
    return DashIsoEncryptionSettings(
      playbackDeviceCompatibility:
          (json['playbackDeviceCompatibility'] as String?)
              ?.let(DashIsoPlaybackDeviceCompatibility.fromString),
      spekeKeyProvider: json['spekeKeyProvider'] != null
          ? SpekeKeyProvider.fromJson(
              json['spekeKeyProvider'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final playbackDeviceCompatibility = this.playbackDeviceCompatibility;
    final spekeKeyProvider = this.spekeKeyProvider;
    return {
      if (playbackDeviceCompatibility != null)
        'playbackDeviceCompatibility': playbackDeviceCompatibility.value,
      if (spekeKeyProvider != null) 'spekeKeyProvider': spekeKeyProvider,
    };
  }
}

/// Use this setting only when your audio codec is a Dolby one (AC3, EAC3, or
/// Atmos) and your downstream workflow requires that your DASH manifest use the
/// Dolby channel configuration tag, rather than the MPEG one. For example, you
/// might need to use this to make dynamic ad insertion work. Specify which
/// audio channel configuration scheme ID URI MediaConvert writes in your DASH
/// manifest. Keep the default value, MPEG channel configuration, to have
/// MediaConvert write this: urn:mpeg:mpegB:cicp:ChannelConfiguration. Choose
/// Dolby channel configuration to have MediaConvert write this instead:
/// tag:dolby.com,2014:dash:audio_channel_configuration:2011.
enum DashIsoGroupAudioChannelConfigSchemeIdUri {
  mpegChannelConfiguration('MPEG_CHANNEL_CONFIGURATION'),
  dolbyChannelConfiguration('DOLBY_CHANNEL_CONFIGURATION'),
  ;

  final String value;

  const DashIsoGroupAudioChannelConfigSchemeIdUri(this.value);

  static DashIsoGroupAudioChannelConfigSchemeIdUri fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum DashIsoGroupAudioChannelConfigSchemeIdUri'));
}

/// Settings related to your DASH output package. For more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/outputs-file-ABR.html.
class DashIsoGroupSettings {
  /// By default, the service creates one .mpd DASH manifest for each DASH ISO
  /// output group in your job. This default manifest references every output in
  /// the output group. To create additional DASH manifests that reference a
  /// subset of the outputs in the output group, specify a list of them here.
  final List<DashAdditionalManifest>? additionalManifests;

  /// Use this setting only when your audio codec is a Dolby one (AC3, EAC3, or
  /// Atmos) and your downstream workflow requires that your DASH manifest use the
  /// Dolby channel configuration tag, rather than the MPEG one. For example, you
  /// might need to use this to make dynamic ad insertion work. Specify which
  /// audio channel configuration scheme ID URI MediaConvert writes in your DASH
  /// manifest. Keep the default value, MPEG channel configuration, to have
  /// MediaConvert write this: urn:mpeg:mpegB:cicp:ChannelConfiguration. Choose
  /// Dolby channel configuration to have MediaConvert write this instead:
  /// tag:dolby.com,2014:dash:audio_channel_configuration:2011.
  final DashIsoGroupAudioChannelConfigSchemeIdUri?
      audioChannelConfigSchemeIdUri;

  /// A partial URI prefix that will be put in the manifest (.mpd) file at the top
  /// level BaseURL element. Can be used if streams are delivered from a different
  /// URL than the manifest file.
  final String? baseUrl;

  /// Specify whether MediaConvert generates I-frame only video segments for DASH
  /// trick play, also known as trick mode. When specified, the I-frame only video
  /// segments are included within an additional AdaptationSet in your DASH output
  /// manifest. To generate I-frame only video segments: Enter a name as a text
  /// string, up to 256 character long. This name is appended to the end of this
  /// output group's base filename, that you specify as part of your destination
  /// URI, and used for the I-frame only video segment files. You may also include
  /// format identifiers. For more information, see:
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/using-variables-in-your-job-settings.html#using-settings-variables-with-streaming-outputs
  /// To not generate I-frame only video segments: Leave blank.
  final String? dashIFrameTrickPlayNameModifier;

  /// Specify how MediaConvert writes SegmentTimeline in your output DASH
  /// manifest. To write a SegmentTimeline in each video Representation: Keep the
  /// default value, Basic. To write a common SegmentTimeline in the video
  /// AdaptationSet: Choose Compact. Note that MediaConvert will still write a
  /// SegmentTimeline in any Representation that does not share a common timeline.
  /// To write a video AdaptationSet for each different output framerate, and a
  /// common SegmentTimeline in each AdaptationSet: Choose Distinct.
  final DashManifestStyle? dashManifestStyle;

  /// Use Destination to specify the S3 output location and the output filename
  /// base. Destination accepts format identifiers. If you do not specify the base
  /// filename in the URI, the service will use the filename of the input file. If
  /// your job has multiple inputs, the service uses the filename of the first
  /// input file.
  final String? destination;

  /// Settings associated with the destination. Will vary based on the type of
  /// destination
  final DestinationSettings? destinationSettings;

  /// DRM settings.
  final DashIsoEncryptionSettings? encryption;

  /// Length of fragments to generate (in seconds). Fragment length must be
  /// compatible with GOP size and Framerate. Note that fragments will end on the
  /// next keyframe after this number of seconds, so actual fragment length may be
  /// longer. When Emit Single File is checked, the fragmentation is internal to a
  /// single output file and it does not cause the creation of many output files
  /// as in other output types.
  final int? fragmentLength;

  /// Supports HbbTV specification as indicated
  final DashIsoHbbtvCompliance? hbbtvCompliance;

  /// Specify whether MediaConvert generates images for trick play. Keep the
  /// default value, None, to not generate any images. Choose Thumbnail to
  /// generate tiled thumbnails. Choose Thumbnail and full frame to generate tiled
  /// thumbnails and full-resolution images of single frames. MediaConvert adds an
  /// entry in the .mpd manifest for each set of images that you generate. A
  /// common application for these images is Roku trick mode. The thumbnails and
  /// full-frame images that MediaConvert creates with this feature are compatible
  /// with this Roku specification:
  /// https://developer.roku.com/docs/developer-program/media-playback/trick-mode/hls-and-dash.md
  final DashIsoImageBasedTrickPlay? imageBasedTrickPlay;

  /// Tile and thumbnail settings applicable when imageBasedTrickPlay is ADVANCED
  final DashIsoImageBasedTrickPlaySettings? imageBasedTrickPlaySettings;

  /// Minimum time of initially buffered media that is needed to ensure smooth
  /// playout.
  final int? minBufferTime;

  /// Keep this setting at the default value of 0, unless you are troubleshooting
  /// a problem with how devices play back the end of your video asset. If you
  /// know that player devices are hanging on the final segment of your video
  /// because the length of your final segment is too short, use this setting to
  /// specify a minimum final segment length, in seconds. Choose a value that is
  /// greater than or equal to 1 and less than your segment length. When you
  /// specify a value for this setting, the encoder will combine any final segment
  /// that is shorter than the length that you specify with the previous segment.
  /// For example, your segment length is 3 seconds and your final segment is .5
  /// seconds without a minimum final segment length; when you set the minimum
  /// final segment length to 1, your final segment is 3.5 seconds.
  final double? minFinalSegmentLength;

  /// Specify how the value for bandwidth is determined for each video
  /// Representation in your output MPD manifest. We recommend that you choose a
  /// MPD manifest bandwidth type that is compatible with your downstream player
  /// configuration. Max: Use the same value that you specify for Max bitrate in
  /// the video output, in bits per second. Average: Use the calculated average
  /// bitrate of the encoded video output, in bits per second.
  final DashIsoMpdManifestBandwidthType? mpdManifestBandwidthType;

  /// Specify whether your DASH profile is on-demand or main. When you choose Main
  /// profile, the service signals urn:mpeg:dash:profile:isoff-main:2011 in your
  /// .mpd DASH manifest. When you choose On-demand, the service signals
  /// urn:mpeg:dash:profile:isoff-on-demand:2011 in your .mpd. When you choose
  /// On-demand, you must also set the output group setting Segment control to
  /// Single file.
  final DashIsoMpdProfile? mpdProfile;

  /// Use this setting only when your output video stream has B-frames, which
  /// causes the initial presentation time stamp (PTS) to be offset from the
  /// initial decode time stamp (DTS). Specify how MediaConvert handles PTS when
  /// writing time stamps in output DASH manifests. Choose Match initial PTS when
  /// you want MediaConvert to use the initial PTS as the first time stamp in the
  /// manifest. Choose Zero-based to have MediaConvert ignore the initial PTS in
  /// the video stream and instead write the initial time stamp as zero in the
  /// manifest. For outputs that don't have B-frames, the time stamps in your DASH
  /// manifests start at zero regardless of your choice here.
  final DashIsoPtsOffsetHandlingForBFrames? ptsOffsetHandlingForBFrames;

  /// When set to SINGLE_FILE, a single output file is generated, which is
  /// internally segmented using the Fragment Length and Segment Length. When set
  /// to SEGMENTED_FILES, separate segment files will be created.
  final DashIsoSegmentControl? segmentControl;

  /// Specify the length, in whole seconds, of each segment. When you don't
  /// specify a value, MediaConvert defaults to 30. Related settings: Use Segment
  /// length control to specify whether the encoder enforces this value strictly.
  /// Use Segment control to specify whether MediaConvert creates separate segment
  /// files or one content file that has metadata to mark the segment boundaries.
  final int? segmentLength;

  /// Specify how you want MediaConvert to determine the segment length. Choose
  /// Exact to have the encoder use the exact length that you specify with the
  /// setting Segment length. This might result in extra I-frames. Choose Multiple
  /// of GOP to have the encoder round up the segment lengths to match the next
  /// GOP boundary.
  final DashIsoSegmentLengthControl? segmentLengthControl;

  /// Specify the video sample composition time offset mode in the output fMP4
  /// TRUN box. For wider player compatibility, set Video composition offsets to
  /// Unsigned or leave blank. The earliest presentation time may be greater than
  /// zero, and sample composition time offsets will increment using unsigned
  /// integers. For strict fMP4 video and audio timing, set Video composition
  /// offsets to Signed. The earliest presentation time will be equal to zero, and
  /// sample composition time offsets will increment using signed integers.
  final DashIsoVideoCompositionOffsets? videoCompositionOffsets;

  /// If you get an HTTP error in the 400 range when you play back your DASH
  /// output, enable this setting and run your transcoding job again. When you
  /// enable this setting, the service writes precise segment durations in the
  /// DASH manifest. The segment duration information appears inside the
  /// SegmentTimeline element, inside SegmentTemplate at the Representation level.
  /// When you don't enable this setting, the service writes approximate segment
  /// durations in your DASH manifest.
  final DashIsoWriteSegmentTimelineInRepresentation?
      writeSegmentTimelineInRepresentation;

  DashIsoGroupSettings({
    this.additionalManifests,
    this.audioChannelConfigSchemeIdUri,
    this.baseUrl,
    this.dashIFrameTrickPlayNameModifier,
    this.dashManifestStyle,
    this.destination,
    this.destinationSettings,
    this.encryption,
    this.fragmentLength,
    this.hbbtvCompliance,
    this.imageBasedTrickPlay,
    this.imageBasedTrickPlaySettings,
    this.minBufferTime,
    this.minFinalSegmentLength,
    this.mpdManifestBandwidthType,
    this.mpdProfile,
    this.ptsOffsetHandlingForBFrames,
    this.segmentControl,
    this.segmentLength,
    this.segmentLengthControl,
    this.videoCompositionOffsets,
    this.writeSegmentTimelineInRepresentation,
  });

  factory DashIsoGroupSettings.fromJson(Map<String, dynamic> json) {
    return DashIsoGroupSettings(
      additionalManifests: (json['additionalManifests'] as List?)
          ?.nonNulls
          .map(
              (e) => DashAdditionalManifest.fromJson(e as Map<String, dynamic>))
          .toList(),
      audioChannelConfigSchemeIdUri:
          (json['audioChannelConfigSchemeIdUri'] as String?)
              ?.let(DashIsoGroupAudioChannelConfigSchemeIdUri.fromString),
      baseUrl: json['baseUrl'] as String?,
      dashIFrameTrickPlayNameModifier:
          json['dashIFrameTrickPlayNameModifier'] as String?,
      dashManifestStyle: (json['dashManifestStyle'] as String?)
          ?.let(DashManifestStyle.fromString),
      destination: json['destination'] as String?,
      destinationSettings: json['destinationSettings'] != null
          ? DestinationSettings.fromJson(
              json['destinationSettings'] as Map<String, dynamic>)
          : null,
      encryption: json['encryption'] != null
          ? DashIsoEncryptionSettings.fromJson(
              json['encryption'] as Map<String, dynamic>)
          : null,
      fragmentLength: json['fragmentLength'] as int?,
      hbbtvCompliance: (json['hbbtvCompliance'] as String?)
          ?.let(DashIsoHbbtvCompliance.fromString),
      imageBasedTrickPlay: (json['imageBasedTrickPlay'] as String?)
          ?.let(DashIsoImageBasedTrickPlay.fromString),
      imageBasedTrickPlaySettings: json['imageBasedTrickPlaySettings'] != null
          ? DashIsoImageBasedTrickPlaySettings.fromJson(
              json['imageBasedTrickPlaySettings'] as Map<String, dynamic>)
          : null,
      minBufferTime: json['minBufferTime'] as int?,
      minFinalSegmentLength: json['minFinalSegmentLength'] as double?,
      mpdManifestBandwidthType: (json['mpdManifestBandwidthType'] as String?)
          ?.let(DashIsoMpdManifestBandwidthType.fromString),
      mpdProfile:
          (json['mpdProfile'] as String?)?.let(DashIsoMpdProfile.fromString),
      ptsOffsetHandlingForBFrames:
          (json['ptsOffsetHandlingForBFrames'] as String?)
              ?.let(DashIsoPtsOffsetHandlingForBFrames.fromString),
      segmentControl: (json['segmentControl'] as String?)
          ?.let(DashIsoSegmentControl.fromString),
      segmentLength: json['segmentLength'] as int?,
      segmentLengthControl: (json['segmentLengthControl'] as String?)
          ?.let(DashIsoSegmentLengthControl.fromString),
      videoCompositionOffsets: (json['videoCompositionOffsets'] as String?)
          ?.let(DashIsoVideoCompositionOffsets.fromString),
      writeSegmentTimelineInRepresentation:
          (json['writeSegmentTimelineInRepresentation'] as String?)
              ?.let(DashIsoWriteSegmentTimelineInRepresentation.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final additionalManifests = this.additionalManifests;
    final audioChannelConfigSchemeIdUri = this.audioChannelConfigSchemeIdUri;
    final baseUrl = this.baseUrl;
    final dashIFrameTrickPlayNameModifier =
        this.dashIFrameTrickPlayNameModifier;
    final dashManifestStyle = this.dashManifestStyle;
    final destination = this.destination;
    final destinationSettings = this.destinationSettings;
    final encryption = this.encryption;
    final fragmentLength = this.fragmentLength;
    final hbbtvCompliance = this.hbbtvCompliance;
    final imageBasedTrickPlay = this.imageBasedTrickPlay;
    final imageBasedTrickPlaySettings = this.imageBasedTrickPlaySettings;
    final minBufferTime = this.minBufferTime;
    final minFinalSegmentLength = this.minFinalSegmentLength;
    final mpdManifestBandwidthType = this.mpdManifestBandwidthType;
    final mpdProfile = this.mpdProfile;
    final ptsOffsetHandlingForBFrames = this.ptsOffsetHandlingForBFrames;
    final segmentControl = this.segmentControl;
    final segmentLength = this.segmentLength;
    final segmentLengthControl = this.segmentLengthControl;
    final videoCompositionOffsets = this.videoCompositionOffsets;
    final writeSegmentTimelineInRepresentation =
        this.writeSegmentTimelineInRepresentation;
    return {
      if (additionalManifests != null)
        'additionalManifests': additionalManifests,
      if (audioChannelConfigSchemeIdUri != null)
        'audioChannelConfigSchemeIdUri': audioChannelConfigSchemeIdUri.value,
      if (baseUrl != null) 'baseUrl': baseUrl,
      if (dashIFrameTrickPlayNameModifier != null)
        'dashIFrameTrickPlayNameModifier': dashIFrameTrickPlayNameModifier,
      if (dashManifestStyle != null)
        'dashManifestStyle': dashManifestStyle.value,
      if (destination != null) 'destination': destination,
      if (destinationSettings != null)
        'destinationSettings': destinationSettings,
      if (encryption != null) 'encryption': encryption,
      if (fragmentLength != null) 'fragmentLength': fragmentLength,
      if (hbbtvCompliance != null) 'hbbtvCompliance': hbbtvCompliance.value,
      if (imageBasedTrickPlay != null)
        'imageBasedTrickPlay': imageBasedTrickPlay.value,
      if (imageBasedTrickPlaySettings != null)
        'imageBasedTrickPlaySettings': imageBasedTrickPlaySettings,
      if (minBufferTime != null) 'minBufferTime': minBufferTime,
      if (minFinalSegmentLength != null)
        'minFinalSegmentLength': minFinalSegmentLength,
      if (mpdManifestBandwidthType != null)
        'mpdManifestBandwidthType': mpdManifestBandwidthType.value,
      if (mpdProfile != null) 'mpdProfile': mpdProfile.value,
      if (ptsOffsetHandlingForBFrames != null)
        'ptsOffsetHandlingForBFrames': ptsOffsetHandlingForBFrames.value,
      if (segmentControl != null) 'segmentControl': segmentControl.value,
      if (segmentLength != null) 'segmentLength': segmentLength,
      if (segmentLengthControl != null)
        'segmentLengthControl': segmentLengthControl.value,
      if (videoCompositionOffsets != null)
        'videoCompositionOffsets': videoCompositionOffsets.value,
      if (writeSegmentTimelineInRepresentation != null)
        'writeSegmentTimelineInRepresentation':
            writeSegmentTimelineInRepresentation.value,
    };
  }
}

/// Supports HbbTV specification as indicated
enum DashIsoHbbtvCompliance {
  hbbtv_1_5('HBBTV_1_5'),
  none('NONE'),
  ;

  final String value;

  const DashIsoHbbtvCompliance(this.value);

  static DashIsoHbbtvCompliance fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum DashIsoHbbtvCompliance'));
}

/// Specify whether MediaConvert generates images for trick play. Keep the
/// default value, None, to not generate any images. Choose Thumbnail to
/// generate tiled thumbnails. Choose Thumbnail and full frame to generate tiled
/// thumbnails and full-resolution images of single frames. MediaConvert adds an
/// entry in the .mpd manifest for each set of images that you generate. A
/// common application for these images is Roku trick mode. The thumbnails and
/// full-frame images that MediaConvert creates with this feature are compatible
/// with this Roku specification:
/// https://developer.roku.com/docs/developer-program/media-playback/trick-mode/hls-and-dash.md
enum DashIsoImageBasedTrickPlay {
  none('NONE'),
  thumbnail('THUMBNAIL'),
  thumbnailAndFullframe('THUMBNAIL_AND_FULLFRAME'),
  advanced('ADVANCED'),
  ;

  final String value;

  const DashIsoImageBasedTrickPlay(this.value);

  static DashIsoImageBasedTrickPlay fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum DashIsoImageBasedTrickPlay'));
}

/// Tile and thumbnail settings applicable when imageBasedTrickPlay is ADVANCED
class DashIsoImageBasedTrickPlaySettings {
  /// The cadence MediaConvert follows for generating thumbnails. If set to
  /// FOLLOW_IFRAME, MediaConvert generates thumbnails for each IDR frame in the
  /// output (matching the GOP cadence). If set to FOLLOW_CUSTOM, MediaConvert
  /// generates thumbnails according to the interval you specify in
  /// thumbnailInterval.
  final DashIsoIntervalCadence? intervalCadence;

  /// Height of each thumbnail within each tile image, in pixels. Leave blank to
  /// maintain aspect ratio with thumbnail width. If following the aspect ratio
  /// would lead to a total tile height greater than 4096, then the job will be
  /// rejected. Must be divisible by 2.
  final int? thumbnailHeight;

  /// Enter the interval, in seconds, that MediaConvert uses to generate
  /// thumbnails. If the interval you enter doesn't align with the output frame
  /// rate, MediaConvert automatically rounds the interval to align with the
  /// output frame rate. For example, if the output frame rate is 29.97 frames per
  /// second and you enter 5, MediaConvert uses a 150 frame interval to generate
  /// thumbnails.
  final double? thumbnailInterval;

  /// Width of each thumbnail within each tile image, in pixels. Default is 312.
  /// Must be divisible by 8.
  final int? thumbnailWidth;

  /// Number of thumbnails in each column of a tile image. Set a value between 2
  /// and 2048. Must be divisible by 2.
  final int? tileHeight;

  /// Number of thumbnails in each row of a tile image. Set a value between 1 and
  /// 512.
  final int? tileWidth;

  DashIsoImageBasedTrickPlaySettings({
    this.intervalCadence,
    this.thumbnailHeight,
    this.thumbnailInterval,
    this.thumbnailWidth,
    this.tileHeight,
    this.tileWidth,
  });

  factory DashIsoImageBasedTrickPlaySettings.fromJson(
      Map<String, dynamic> json) {
    return DashIsoImageBasedTrickPlaySettings(
      intervalCadence: (json['intervalCadence'] as String?)
          ?.let(DashIsoIntervalCadence.fromString),
      thumbnailHeight: json['thumbnailHeight'] as int?,
      thumbnailInterval: json['thumbnailInterval'] as double?,
      thumbnailWidth: json['thumbnailWidth'] as int?,
      tileHeight: json['tileHeight'] as int?,
      tileWidth: json['tileWidth'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final intervalCadence = this.intervalCadence;
    final thumbnailHeight = this.thumbnailHeight;
    final thumbnailInterval = this.thumbnailInterval;
    final thumbnailWidth = this.thumbnailWidth;
    final tileHeight = this.tileHeight;
    final tileWidth = this.tileWidth;
    return {
      if (intervalCadence != null) 'intervalCadence': intervalCadence.value,
      if (thumbnailHeight != null) 'thumbnailHeight': thumbnailHeight,
      if (thumbnailInterval != null) 'thumbnailInterval': thumbnailInterval,
      if (thumbnailWidth != null) 'thumbnailWidth': thumbnailWidth,
      if (tileHeight != null) 'tileHeight': tileHeight,
      if (tileWidth != null) 'tileWidth': tileWidth,
    };
  }
}

/// The cadence MediaConvert follows for generating thumbnails. If set to
/// FOLLOW_IFRAME, MediaConvert generates thumbnails for each IDR frame in the
/// output (matching the GOP cadence). If set to FOLLOW_CUSTOM, MediaConvert
/// generates thumbnails according to the interval you specify in
/// thumbnailInterval.
enum DashIsoIntervalCadence {
  followIframe('FOLLOW_IFRAME'),
  followCustom('FOLLOW_CUSTOM'),
  ;

  final String value;

  const DashIsoIntervalCadence(this.value);

  static DashIsoIntervalCadence fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum DashIsoIntervalCadence'));
}

/// Specify how the value for bandwidth is determined for each video
/// Representation in your output MPD manifest. We recommend that you choose a
/// MPD manifest bandwidth type that is compatible with your downstream player
/// configuration. Max: Use the same value that you specify for Max bitrate in
/// the video output, in bits per second. Average: Use the calculated average
/// bitrate of the encoded video output, in bits per second.
enum DashIsoMpdManifestBandwidthType {
  average('AVERAGE'),
  max('MAX'),
  ;

  final String value;

  const DashIsoMpdManifestBandwidthType(this.value);

  static DashIsoMpdManifestBandwidthType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum DashIsoMpdManifestBandwidthType'));
}

/// Specify whether your DASH profile is on-demand or main. When you choose Main
/// profile, the service signals urn:mpeg:dash:profile:isoff-main:2011 in your
/// .mpd DASH manifest. When you choose On-demand, the service signals
/// urn:mpeg:dash:profile:isoff-on-demand:2011 in your .mpd. When you choose
/// On-demand, you must also set the output group setting Segment control to
/// Single file.
enum DashIsoMpdProfile {
  mainProfile('MAIN_PROFILE'),
  onDemandProfile('ON_DEMAND_PROFILE'),
  ;

  final String value;

  const DashIsoMpdProfile(this.value);

  static DashIsoMpdProfile fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum DashIsoMpdProfile'));
}

/// This setting can improve the compatibility of your output with video players
/// on obsolete devices. It applies only to DASH H.264 outputs with DRM
/// encryption. Choose Unencrypted SEI only to correct problems with playback on
/// older devices. Otherwise, keep the default setting CENC v1. If you choose
/// Unencrypted SEI, for that output, the service will exclude the access unit
/// delimiter and will leave the SEI NAL units unencrypted.
enum DashIsoPlaybackDeviceCompatibility {
  cencV1('CENC_V1'),
  unencryptedSei('UNENCRYPTED_SEI'),
  ;

  final String value;

  const DashIsoPlaybackDeviceCompatibility(this.value);

  static DashIsoPlaybackDeviceCompatibility fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum DashIsoPlaybackDeviceCompatibility'));
}

/// Use this setting only when your output video stream has B-frames, which
/// causes the initial presentation time stamp (PTS) to be offset from the
/// initial decode time stamp (DTS). Specify how MediaConvert handles PTS when
/// writing time stamps in output DASH manifests. Choose Match initial PTS when
/// you want MediaConvert to use the initial PTS as the first time stamp in the
/// manifest. Choose Zero-based to have MediaConvert ignore the initial PTS in
/// the video stream and instead write the initial time stamp as zero in the
/// manifest. For outputs that don't have B-frames, the time stamps in your DASH
/// manifests start at zero regardless of your choice here.
enum DashIsoPtsOffsetHandlingForBFrames {
  zeroBased('ZERO_BASED'),
  matchInitialPts('MATCH_INITIAL_PTS'),
  ;

  final String value;

  const DashIsoPtsOffsetHandlingForBFrames(this.value);

  static DashIsoPtsOffsetHandlingForBFrames fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum DashIsoPtsOffsetHandlingForBFrames'));
}

/// When set to SINGLE_FILE, a single output file is generated, which is
/// internally segmented using the Fragment Length and Segment Length. When set
/// to SEGMENTED_FILES, separate segment files will be created.
enum DashIsoSegmentControl {
  singleFile('SINGLE_FILE'),
  segmentedFiles('SEGMENTED_FILES'),
  ;

  final String value;

  const DashIsoSegmentControl(this.value);

  static DashIsoSegmentControl fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum DashIsoSegmentControl'));
}

/// Specify how you want MediaConvert to determine the segment length. Choose
/// Exact to have the encoder use the exact length that you specify with the
/// setting Segment length. This might result in extra I-frames. Choose Multiple
/// of GOP to have the encoder round up the segment lengths to match the next
/// GOP boundary.
enum DashIsoSegmentLengthControl {
  exact('EXACT'),
  gopMultiple('GOP_MULTIPLE'),
  ;

  final String value;

  const DashIsoSegmentLengthControl(this.value);

  static DashIsoSegmentLengthControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum DashIsoSegmentLengthControl'));
}

/// Specify the video sample composition time offset mode in the output fMP4
/// TRUN box. For wider player compatibility, set Video composition offsets to
/// Unsigned or leave blank. The earliest presentation time may be greater than
/// zero, and sample composition time offsets will increment using unsigned
/// integers. For strict fMP4 video and audio timing, set Video composition
/// offsets to Signed. The earliest presentation time will be equal to zero, and
/// sample composition time offsets will increment using signed integers.
enum DashIsoVideoCompositionOffsets {
  signed('SIGNED'),
  unsigned('UNSIGNED'),
  ;

  final String value;

  const DashIsoVideoCompositionOffsets(this.value);

  static DashIsoVideoCompositionOffsets fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum DashIsoVideoCompositionOffsets'));
}

/// When you enable Precise segment duration in manifests, your DASH manifest
/// shows precise segment durations. The segment duration information appears
/// inside the SegmentTimeline element, inside SegmentTemplate at the
/// Representation level. When this feature isn't enabled, the segment durations
/// in your DASH manifest are approximate. The segment duration information
/// appears in the duration attribute of the SegmentTemplate element.
enum DashIsoWriteSegmentTimelineInRepresentation {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const DashIsoWriteSegmentTimelineInRepresentation(this.value);

  static DashIsoWriteSegmentTimelineInRepresentation fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum DashIsoWriteSegmentTimelineInRepresentation'));
}

/// Specify how MediaConvert writes SegmentTimeline in your output DASH
/// manifest. To write a SegmentTimeline in each video Representation: Keep the
/// default value, Basic. To write a common SegmentTimeline in the video
/// AdaptationSet: Choose Compact. Note that MediaConvert will still write a
/// SegmentTimeline in any Representation that does not share a common timeline.
/// To write a video AdaptationSet for each different output framerate, and a
/// common SegmentTimeline in each AdaptationSet: Choose Distinct.
enum DashManifestStyle {
  basic('BASIC'),
  compact('COMPACT'),
  distinct('DISTINCT'),
  ;

  final String value;

  const DashManifestStyle(this.value);

  static DashManifestStyle fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum DashManifestStyle'));
}

/// Specify the encryption mode that you used to encrypt your input files.
enum DecryptionMode {
  aesCtr('AES_CTR'),
  aesCbc('AES_CBC'),
  aesGcm('AES_GCM'),
  ;

  final String value;

  const DecryptionMode(this.value);

  static DecryptionMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum DecryptionMode'));
}

/// Only applies when you set Deinterlace mode to Deinterlace or Adaptive.
/// Interpolate produces sharper pictures, while blend produces smoother motion.
/// If your source file includes a ticker, such as a scrolling headline at the
/// bottom of the frame: Choose Interpolate ticker or Blend ticker. To apply
/// field doubling: Choose Linear interpolation. Note that Linear interpolation
/// may introduce video artifacts into your output.
enum DeinterlaceAlgorithm {
  interpolate('INTERPOLATE'),
  interpolateTicker('INTERPOLATE_TICKER'),
  blend('BLEND'),
  blendTicker('BLEND_TICKER'),
  linearInterpolation('LINEAR_INTERPOLATION'),
  ;

  final String value;

  const DeinterlaceAlgorithm(this.value);

  static DeinterlaceAlgorithm fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum DeinterlaceAlgorithm'));
}

/// Settings for deinterlacer
class Deinterlacer {
  /// Only applies when you set Deinterlace mode to Deinterlace or Adaptive.
  /// Interpolate produces sharper pictures, while blend produces smoother motion.
  /// If your source file includes a ticker, such as a scrolling headline at the
  /// bottom of the frame: Choose Interpolate ticker or Blend ticker. To apply
  /// field doubling: Choose Linear interpolation. Note that Linear interpolation
  /// may introduce video artifacts into your output.
  final DeinterlaceAlgorithm? algorithm;

  /// - When set to NORMAL (default), the deinterlacer does not convert frames
  /// that are tagged in metadata as progressive. It will only convert those that
  /// are tagged as some other type. - When set to FORCE_ALL_FRAMES, the
  /// deinterlacer converts every frame to progressive - even those that are
  /// already tagged as progressive. Turn Force mode on only if there is a good
  /// chance that the metadata has tagged frames as progressive when they are not
  /// progressive. Do not turn on otherwise; processing frames that are already
  /// progressive into progressive will probably result in lower quality video.
  final DeinterlacerControl? control;

  /// Use Deinterlacer to choose how the service will do deinterlacing. Default is
  /// Deinterlace.
  /// - Deinterlace converts interlaced to progressive.
  /// - Inverse telecine converts Hard Telecine 29.97i to progressive 23.976p.
  /// - Adaptive auto-detects and converts to progressive.
  final DeinterlacerMode? mode;

  Deinterlacer({
    this.algorithm,
    this.control,
    this.mode,
  });

  factory Deinterlacer.fromJson(Map<String, dynamic> json) {
    return Deinterlacer(
      algorithm:
          (json['algorithm'] as String?)?.let(DeinterlaceAlgorithm.fromString),
      control:
          (json['control'] as String?)?.let(DeinterlacerControl.fromString),
      mode: (json['mode'] as String?)?.let(DeinterlacerMode.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final algorithm = this.algorithm;
    final control = this.control;
    final mode = this.mode;
    return {
      if (algorithm != null) 'algorithm': algorithm.value,
      if (control != null) 'control': control.value,
      if (mode != null) 'mode': mode.value,
    };
  }
}

/// - When set to NORMAL (default), the deinterlacer does not convert frames
/// that are tagged in metadata as progressive. It will only convert those that
/// are tagged as some other type. - When set to FORCE_ALL_FRAMES, the
/// deinterlacer converts every frame to progressive - even those that are
/// already tagged as progressive. Turn Force mode on only if there is a good
/// chance that the metadata has tagged frames as progressive when they are not
/// progressive. Do not turn on otherwise; processing frames that are already
/// progressive into progressive will probably result in lower quality video.
enum DeinterlacerControl {
  forceAllFrames('FORCE_ALL_FRAMES'),
  normal('NORMAL'),
  ;

  final String value;

  const DeinterlacerControl(this.value);

  static DeinterlacerControl fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum DeinterlacerControl'));
}

/// Use Deinterlacer to choose how the service will do deinterlacing. Default is
/// Deinterlace.
/// - Deinterlace converts interlaced to progressive.
/// - Inverse telecine converts Hard Telecine 29.97i to progressive 23.976p.
/// - Adaptive auto-detects and converts to progressive.
enum DeinterlacerMode {
  deinterlace('DEINTERLACE'),
  inverseTelecine('INVERSE_TELECINE'),
  adaptive('ADAPTIVE'),
  ;

  final String value;

  const DeinterlacerMode(this.value);

  static DeinterlacerMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum DeinterlacerMode'));
}

class DeleteJobTemplateResponse {
  DeleteJobTemplateResponse();

  factory DeleteJobTemplateResponse.fromJson(Map<String, dynamic> _) {
    return DeleteJobTemplateResponse();
  }

  Map<String, dynamic> toJson() {
    return {};
  }
}

class DeletePolicyResponse {
  DeletePolicyResponse();

  factory DeletePolicyResponse.fromJson(Map<String, dynamic> _) {
    return DeletePolicyResponse();
  }

  Map<String, dynamic> toJson() {
    return {};
  }
}

class DeletePresetResponse {
  DeletePresetResponse();

  factory DeletePresetResponse.fromJson(Map<String, dynamic> _) {
    return DeletePresetResponse();
  }

  Map<String, dynamic> toJson() {
    return {};
  }
}

class DeleteQueueResponse {
  DeleteQueueResponse();

  factory DeleteQueueResponse.fromJson(Map<String, dynamic> _) {
    return DeleteQueueResponse();
  }

  Map<String, dynamic> toJson() {
    return {};
  }
}

/// Optional field, defaults to DEFAULT. Specify DEFAULT for this operation to
/// return your endpoints if any exist, or to create an endpoint for you and
/// return it if one doesn't already exist. Specify GET_ONLY to return your
/// endpoints if any exist, or an empty list if none exist.
@Deprecated(
    'DescribeEndpoints and account specific endpoints are no longer required. We recommend that you send your requests directly to the regional endpoint instead.')
enum DescribeEndpointsMode {
  $default('DEFAULT'),
  getOnly('GET_ONLY'),
  ;

  final String value;

  const DescribeEndpointsMode(this.value);

  static DescribeEndpointsMode fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum DescribeEndpointsMode'));
}

@Deprecated(
    'DescribeEndpoints and account specific endpoints are no longer required. We recommend that you send your requests directly to the regional endpoint instead.')
class DescribeEndpointsResponse {
  /// List of endpoints
  final List<Endpoint>? endpoints;

  /// Use this string to request the next batch of endpoints.
  final String? nextToken;

  DescribeEndpointsResponse({
    this.endpoints,
    this.nextToken,
  });

  factory DescribeEndpointsResponse.fromJson(Map<String, dynamic> json) {
    return DescribeEndpointsResponse(
      endpoints: (json['endpoints'] as List?)
          ?.nonNulls
          .map((e) => Endpoint.fromJson(e as Map<String, dynamic>))
          .toList(),
      nextToken: json['nextToken'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final endpoints = this.endpoints;
    final nextToken = this.nextToken;
    return {
      if (endpoints != null) 'endpoints': endpoints,
      if (nextToken != null) 'nextToken': nextToken,
    };
  }
}

/// Settings associated with the destination. Will vary based on the type of
/// destination
class DestinationSettings {
  /// Settings associated with S3 destination
  final S3DestinationSettings? s3Settings;

  DestinationSettings({
    this.s3Settings,
  });

  factory DestinationSettings.fromJson(Map<String, dynamic> json) {
    return DestinationSettings(
      s3Settings: json['s3Settings'] != null
          ? S3DestinationSettings.fromJson(
              json['s3Settings'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final s3Settings = this.s3Settings;
    return {
      if (s3Settings != null) 's3Settings': s3Settings,
    };
  }
}

class DisassociateCertificateResponse {
  DisassociateCertificateResponse();

  factory DisassociateCertificateResponse.fromJson(Map<String, dynamic> _) {
    return DisassociateCertificateResponse();
  }

  Map<String, dynamic> toJson() {
    return {};
  }
}

/// Create Dolby Vision Profile 5 or Profile 8.1 compatible video output.
class DolbyVision {
  /// Use these settings when you set DolbyVisionLevel6Mode to SPECIFY to override
  /// the MaxCLL and MaxFALL values in your input with new values.
  final DolbyVisionLevel6Metadata? l6Metadata;

  /// Use Dolby Vision Mode to choose how the service will handle Dolby Vision
  /// MaxCLL and MaxFALL properies.
  final DolbyVisionLevel6Mode? l6Mode;

  /// Required when you set Dolby Vision Profile to Profile 8.1. When you set
  /// Content mapping to None, content mapping is not applied to the
  /// HDR10-compatible signal. Depending on the source peak nit level, clipping
  /// might occur on HDR devices without Dolby Vision. When you set Content
  /// mapping to HDR10 1000, the transcoder creates a 1,000 nits peak
  /// HDR10-compatible signal by applying static content mapping to the source.
  /// This mode is speed-optimized for PQ10 sources with metadata that is created
  /// from analysis. For graded Dolby Vision content, be aware that creative
  /// intent might not be guaranteed with extreme 1,000 nits trims.
  final DolbyVisionMapping? mapping;

  /// Required when you enable Dolby Vision. Use Profile 5 to include
  /// frame-interleaved Dolby Vision metadata in your output. Your input must
  /// include Dolby Vision metadata or an HDR10 YUV color space. Use Profile 8.1
  /// to include frame-interleaved Dolby Vision metadata and HDR10 metadata in
  /// your output. Your input must include Dolby Vision metadata.
  final DolbyVisionProfile? profile;

  DolbyVision({
    this.l6Metadata,
    this.l6Mode,
    this.mapping,
    this.profile,
  });

  factory DolbyVision.fromJson(Map<String, dynamic> json) {
    return DolbyVision(
      l6Metadata: json['l6Metadata'] != null
          ? DolbyVisionLevel6Metadata.fromJson(
              json['l6Metadata'] as Map<String, dynamic>)
          : null,
      l6Mode:
          (json['l6Mode'] as String?)?.let(DolbyVisionLevel6Mode.fromString),
      mapping: (json['mapping'] as String?)?.let(DolbyVisionMapping.fromString),
      profile: (json['profile'] as String?)?.let(DolbyVisionProfile.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final l6Metadata = this.l6Metadata;
    final l6Mode = this.l6Mode;
    final mapping = this.mapping;
    final profile = this.profile;
    return {
      if (l6Metadata != null) 'l6Metadata': l6Metadata,
      if (l6Mode != null) 'l6Mode': l6Mode.value,
      if (mapping != null) 'mapping': mapping.value,
      if (profile != null) 'profile': profile.value,
    };
  }
}

/// Use these settings when you set DolbyVisionLevel6Mode to SPECIFY to override
/// the MaxCLL and MaxFALL values in your input with new values.
class DolbyVisionLevel6Metadata {
  /// Maximum Content Light Level. Static HDR metadata that corresponds to the
  /// brightest pixel in the entire stream. Measured in nits.
  final int? maxCll;

  /// Maximum Frame-Average Light Level. Static HDR metadata that corresponds to
  /// the highest frame-average brightness in the entire stream. Measured in nits.
  final int? maxFall;

  DolbyVisionLevel6Metadata({
    this.maxCll,
    this.maxFall,
  });

  factory DolbyVisionLevel6Metadata.fromJson(Map<String, dynamic> json) {
    return DolbyVisionLevel6Metadata(
      maxCll: json['maxCll'] as int?,
      maxFall: json['maxFall'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final maxCll = this.maxCll;
    final maxFall = this.maxFall;
    return {
      if (maxCll != null) 'maxCll': maxCll,
      if (maxFall != null) 'maxFall': maxFall,
    };
  }
}

/// Use Dolby Vision Mode to choose how the service will handle Dolby Vision
/// MaxCLL and MaxFALL properies.
enum DolbyVisionLevel6Mode {
  passthrough('PASSTHROUGH'),
  recalculate('RECALCULATE'),
  specify('SPECIFY'),
  ;

  final String value;

  const DolbyVisionLevel6Mode(this.value);

  static DolbyVisionLevel6Mode fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum DolbyVisionLevel6Mode'));
}

/// Required when you set Dolby Vision Profile to Profile 8.1. When you set
/// Content mapping to None, content mapping is not applied to the
/// HDR10-compatible signal. Depending on the source peak nit level, clipping
/// might occur on HDR devices without Dolby Vision. When you set Content
/// mapping to HDR10 1000, the transcoder creates a 1,000 nits peak
/// HDR10-compatible signal by applying static content mapping to the source.
/// This mode is speed-optimized for PQ10 sources with metadata that is created
/// from analysis. For graded Dolby Vision content, be aware that creative
/// intent might not be guaranteed with extreme 1,000 nits trims.
enum DolbyVisionMapping {
  hdr10Nomap('HDR10_NOMAP'),
  hdr10_1000('HDR10_1000'),
  ;

  final String value;

  const DolbyVisionMapping(this.value);

  static DolbyVisionMapping fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum DolbyVisionMapping'));
}

/// Required when you enable Dolby Vision. Use Profile 5 to include
/// frame-interleaved Dolby Vision metadata in your output. Your input must
/// include Dolby Vision metadata or an HDR10 YUV color space. Use Profile 8.1
/// to include frame-interleaved Dolby Vision metadata and HDR10 metadata in
/// your output. Your input must include Dolby Vision metadata.
enum DolbyVisionProfile {
  profile_5('PROFILE_5'),
  profile_8_1('PROFILE_8_1'),
  ;

  final String value;

  const DolbyVisionProfile(this.value);

  static DolbyVisionProfile fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum DolbyVisionProfile'));
}

/// Applies only to 29.97 fps outputs. When this feature is enabled, the service
/// will use drop-frame timecode on outputs. If it is not possible to use
/// drop-frame timecode, the system will fall back to non-drop-frame. This
/// setting is enabled by default when Timecode insertion is enabled.
enum DropFrameTimecode {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const DropFrameTimecode(this.value);

  static DropFrameTimecode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum DropFrameTimecode'));
}

/// Use these settings to insert a DVB Network Information Table (NIT) in the
/// transport stream of this output.
class DvbNitSettings {
  /// The numeric value placed in the Network Information Table (NIT).
  final int? networkId;

  /// The network name text placed in the network_name_descriptor inside the
  /// Network Information Table. Maximum length is 256 characters.
  final String? networkName;

  /// The number of milliseconds between instances of this table in the output
  /// transport stream.
  final int? nitInterval;

  DvbNitSettings({
    this.networkId,
    this.networkName,
    this.nitInterval,
  });

  factory DvbNitSettings.fromJson(Map<String, dynamic> json) {
    return DvbNitSettings(
      networkId: json['networkId'] as int?,
      networkName: json['networkName'] as String?,
      nitInterval: json['nitInterval'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final networkId = this.networkId;
    final networkName = this.networkName;
    final nitInterval = this.nitInterval;
    return {
      if (networkId != null) 'networkId': networkId,
      if (networkName != null) 'networkName': networkName,
      if (nitInterval != null) 'nitInterval': nitInterval,
    };
  }
}

/// Use these settings to insert a DVB Service Description Table (SDT) in the
/// transport stream of this output.
class DvbSdtSettings {
  /// Selects method of inserting SDT information into output stream. "Follow
  /// input SDT" copies SDT information from input stream to output stream.
  /// "Follow input SDT if present" copies SDT information from input stream to
  /// output stream if SDT information is present in the input, otherwise it will
  /// fall back on the user-defined values. Enter "SDT Manually" means user will
  /// enter the SDT information. "No SDT" means output stream will not contain SDT
  /// information.
  final OutputSdt? outputSdt;

  /// The number of milliseconds between instances of this table in the output
  /// transport stream.
  final int? sdtInterval;

  /// The service name placed in the service_descriptor in the Service Description
  /// Table. Maximum length is 256 characters.
  final String? serviceName;

  /// The service provider name placed in the service_descriptor in the Service
  /// Description Table. Maximum length is 256 characters.
  final String? serviceProviderName;

  DvbSdtSettings({
    this.outputSdt,
    this.sdtInterval,
    this.serviceName,
    this.serviceProviderName,
  });

  factory DvbSdtSettings.fromJson(Map<String, dynamic> json) {
    return DvbSdtSettings(
      outputSdt: (json['outputSdt'] as String?)?.let(OutputSdt.fromString),
      sdtInterval: json['sdtInterval'] as int?,
      serviceName: json['serviceName'] as String?,
      serviceProviderName: json['serviceProviderName'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final outputSdt = this.outputSdt;
    final sdtInterval = this.sdtInterval;
    final serviceName = this.serviceName;
    final serviceProviderName = this.serviceProviderName;
    return {
      if (outputSdt != null) 'outputSdt': outputSdt.value,
      if (sdtInterval != null) 'sdtInterval': sdtInterval,
      if (serviceName != null) 'serviceName': serviceName,
      if (serviceProviderName != null)
        'serviceProviderName': serviceProviderName,
    };
  }
}

/// Settings related to DVB-Sub captions. Set up DVB-Sub captions in the same
/// output as your video. For more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/dvb-sub-output-captions.html.
class DvbSubDestinationSettings {
  /// Specify the alignment of your captions. If no explicit x_position is
  /// provided, setting alignment to centered will placethe captions at the bottom
  /// center of the output. Similarly, setting a left alignment willalign captions
  /// to the bottom left of the output. If x and y positions are given in
  /// conjunction with the alignment parameter, the font will be justified (either
  /// left or centered) relative to those coordinates. Within your job settings,
  /// all of your DVB-Sub settings must be identical.
  final DvbSubtitleAlignment? alignment;

  /// Ignore this setting unless Style Passthrough is set to Enabled and Font
  /// color set to Black, Yellow, Red, Green, Blue, or Hex. Use Apply font color
  /// for additional font color controls. When you choose White text only, or
  /// leave blank, your font color setting only applies to white text in your
  /// input captions. For example, if your font color setting is Yellow, and your
  /// input captions have red and white text, your output captions will have red
  /// and yellow text. When you choose ALL_TEXT, your font color setting applies
  /// to all of your output captions text.
  final DvbSubtitleApplyFontColor? applyFontColor;

  /// Specify the color of the rectangle behind the captions. Leave background
  /// color blank and set Style passthrough to enabled to use the background color
  /// data from your input captions, if present.
  final DvbSubtitleBackgroundColor? backgroundColor;

  /// Specify the opacity of the background rectangle. Enter a value from 0 to
  /// 255, where 0 is transparent and 255 is opaque. If Style passthrough is set
  /// to enabled, leave blank to pass through the background style information in
  /// your input captions to your output captions. If Style passthrough is set to
  /// disabled, leave blank to use a value of 0 and remove all backgrounds from
  /// your output captions. Within your job settings, all of your DVB-Sub settings
  /// must be identical.
  final int? backgroundOpacity;

  /// Specify how MediaConvert handles the display definition segment (DDS). To
  /// exclude the DDS from this set of captions: Keep the default, None. To
  /// include the DDS: Choose Specified. When you do, also specify the offset
  /// coordinates of the display window with DDS x-coordinate and DDS
  /// y-coordinate. To include the DDS, but not include display window data:
  /// Choose No display window. When you do, you can write position metadata to
  /// the page composition segment (PCS) with DDS x-coordinate and DDS
  /// y-coordinate. For video resolutions with a height of 576 pixels or less,
  /// MediaConvert doesn't include the DDS, regardless of the value you choose for
  /// DDS handling. All burn-in and DVB-Sub font settings must match.
  final DvbddsHandling? ddsHandling;

  /// Use this setting, along with DDS y-coordinate, to specify the upper left
  /// corner of the display definition segment (DDS) display window. With this
  /// setting, specify the distance, in pixels, between the left side of the frame
  /// and the left side of the DDS display window. Keep the default value, 0, to
  /// have MediaConvert automatically choose this offset. Related setting: When
  /// you use this setting, you must set DDS handling to a value other than None.
  /// MediaConvert uses these values to determine whether to write page position
  /// data to the DDS or to the page composition segment. All burn-in and DVB-Sub
  /// font settings must match.
  final int? ddsXCoordinate;

  /// Use this setting, along with DDS x-coordinate, to specify the upper left
  /// corner of the display definition segment (DDS) display window. With this
  /// setting, specify the distance, in pixels, between the top of the frame and
  /// the top of the DDS display window. Keep the default value, 0, to have
  /// MediaConvert automatically choose this offset. Related setting: When you use
  /// this setting, you must set DDS handling to a value other than None.
  /// MediaConvert uses these values to determine whether to write page position
  /// data to the DDS or to the page composition segment (PCS). All burn-in and
  /// DVB-Sub font settings must match.
  final int? ddsYCoordinate;

  /// Specify the font that you want the service to use for your burn in captions
  /// when your input captions specify a font that MediaConvert doesn't support.
  /// When you set Fallback font to best match, or leave blank, MediaConvert uses
  /// a supported font that most closely matches the font that your input captions
  /// specify. When there are multiple unsupported fonts in your input captions,
  /// MediaConvert matches each font with the supported font that matches best.
  /// When you explicitly choose a replacement font, MediaConvert uses that font
  /// to replace all unsupported fonts from your input.
  final DvbSubSubtitleFallbackFont? fallbackFont;

  /// Specify the color of the captions text. Leave Font color blank and set Style
  /// passthrough to enabled to use the font color data from your input captions,
  /// if present. Within your job settings, all of your DVB-Sub settings must be
  /// identical.
  final DvbSubtitleFontColor? fontColor;

  /// Specify a bold TrueType font file to use when rendering your output
  /// captions. Enter an S3, HTTP, or HTTPS URL. When you do, you must also
  /// separately specify a regular, an italic, and a bold italic font file.
  final String? fontFileBold;

  /// Specify a bold italic TrueType font file to use when rendering your output
  /// captions.
  /// Enter an S3, HTTP, or HTTPS URL.
  /// When you do, you must also separately specify a regular, a bold, and an
  /// italic font file.
  final String? fontFileBoldItalic;

  /// Specify an italic TrueType font file to use when rendering your output
  /// captions. Enter an S3, HTTP, or HTTPS URL. When you do, you must also
  /// separately specify a regular, a bold, and a bold italic font file.
  final String? fontFileItalic;

  /// Specify a regular TrueType font file to use when rendering your output
  /// captions. Enter an S3, HTTP, or HTTPS URL. When you do, you must also
  /// separately specify a bold, an italic, and a bold italic font file.
  final String? fontFileRegular;

  /// Specify the opacity of the burned-in captions. 255 is opaque; 0 is
  /// transparent.
  /// Within your job settings, all of your DVB-Sub settings must be identical.
  final int? fontOpacity;

  /// Specify the Font resolution in DPI (dots per inch).
  /// Within your job settings, all of your DVB-Sub settings must be identical.
  final int? fontResolution;

  /// Set Font script to Automatically determined, or leave blank, to
  /// automatically determine the font script in your input captions. Otherwise,
  /// set to Simplified Chinese (HANS) or Traditional Chinese (HANT) if your input
  /// font script uses Simplified or Traditional Chinese. Within your job
  /// settings, all of your DVB-Sub settings must be identical.
  final FontScript? fontScript;

  /// Specify the Font size in pixels. Must be a positive integer. Set to 0, or
  /// leave blank, for automatic font size. Within your job settings, all of your
  /// DVB-Sub settings must be identical.
  final int? fontSize;

  /// Specify the height, in pixels, of this set of DVB-Sub captions. The default
  /// value is 576 pixels. Related setting: When you use this setting, you must
  /// set DDS handling to a value other than None. All burn-in and DVB-Sub font
  /// settings must match.
  final int? height;

  /// Ignore this setting unless your Font color is set to Hex. Enter either six
  /// or eight hexidecimal digits, representing red, green, and blue, with two
  /// optional extra digits for alpha. For example a value of 1122AABB is a red
  /// value of 0x11, a green value of 0x22, a blue value of 0xAA, and an alpha
  /// value of 0xBB.
  final String? hexFontColor;

  /// Specify font outline color. Leave Outline color blank and set Style
  /// passthrough to enabled to use the font outline color data from your input
  /// captions, if present. Within your job settings, all of your DVB-Sub settings
  /// must be identical.
  final DvbSubtitleOutlineColor? outlineColor;

  /// Specify the Outline size of the caption text, in pixels. Leave Outline size
  /// blank and set Style passthrough to enabled to use the outline size data from
  /// your input captions, if present. Within your job settings, all of your
  /// DVB-Sub settings must be identical.
  final int? outlineSize;

  /// Specify the color of the shadow cast by the captions. Leave Shadow color
  /// blank and set Style passthrough to enabled to use the shadow color data from
  /// your input captions, if present. Within your job settings, all of your
  /// DVB-Sub settings must be identical.
  final DvbSubtitleShadowColor? shadowColor;

  /// Specify the opacity of the shadow. Enter a value from 0 to 255, where 0 is
  /// transparent and 255 is opaque. If Style passthrough is set to Enabled, leave
  /// Shadow opacity blank to pass through the shadow style information in your
  /// input captions to your output captions. If Style passthrough is set to
  /// disabled, leave blank to use a value of 0 and remove all shadows from your
  /// output captions. Within your job settings, all of your DVB-Sub settings must
  /// be identical.
  final int? shadowOpacity;

  /// Specify the horizontal offset of the shadow, relative to the captions in
  /// pixels. A value of -2 would result in a shadow offset 2 pixels to the left.
  /// Within your job settings, all of your DVB-Sub settings must be identical.
  final int? shadowXOffset;

  /// Specify the vertical offset of the shadow relative to the captions in
  /// pixels. A value of -2 would result in a shadow offset 2 pixels above the
  /// text. Leave Shadow y-offset blank and set Style passthrough to enabled to
  /// use the shadow y-offset data from your input captions, if present. Within
  /// your job settings, all of your DVB-Sub settings must be identical.
  final int? shadowYOffset;

  /// To use the available style, color, and position information from your input
  /// captions: Set Style passthrough to Enabled. Note that MediaConvert uses
  /// default settings for any missing style or position information in your input
  /// captions To ignore the style and position information from your input
  /// captions and use default settings: Leave blank or keep the default value,
  /// Disabled. Default settings include white text with black outlining,
  /// bottom-center positioning, and automatic sizing. Whether you set Style
  /// passthrough to enabled or not, you can also choose to manually override any
  /// of the individual style and position settings. You can also override any
  /// fonts by manually specifying custom font files.
  final DvbSubtitleStylePassthrough? stylePassthrough;

  /// Specify whether your DVB subtitles are standard or for hearing impaired.
  /// Choose hearing impaired if your subtitles include audio descriptions and
  /// dialogue. Choose standard if your subtitles include only dialogue.
  final DvbSubtitlingType? subtitlingType;

  /// Specify whether the Text spacing in your captions is set by the captions
  /// grid, or varies depending on letter width. Choose fixed grid to conform to
  /// the spacing specified in the captions file more accurately. Choose
  /// proportional to make the text easier to read for closed captions. Within
  /// your job settings, all of your DVB-Sub settings must be identical.
  final DvbSubtitleTeletextSpacing? teletextSpacing;

  /// Specify the width, in pixels, of this set of DVB-Sub captions. The default
  /// value is 720 pixels. Related setting: When you use this setting, you must
  /// set DDS handling to a value other than None. All burn-in and DVB-Sub font
  /// settings must match.
  final int? width;

  /// Specify the horizontal position of the captions, relative to the left side
  /// of the output in pixels. A value of 10 would result in the captions starting
  /// 10 pixels from the left of the output. If no explicit x_position is
  /// provided, the horizontal caption position will be determined by the
  /// alignment parameter. Within your job settings, all of your DVB-Sub settings
  /// must be identical.
  final int? xPosition;

  /// Specify the vertical position of the captions, relative to the top of the
  /// output in pixels. A value of 10 would result in the captions starting 10
  /// pixels from the top of the output. If no explicit y_position is provided,
  /// the caption will be positioned towards the bottom of the output. Within your
  /// job settings, all of your DVB-Sub settings must be identical.
  final int? yPosition;

  DvbSubDestinationSettings({
    this.alignment,
    this.applyFontColor,
    this.backgroundColor,
    this.backgroundOpacity,
    this.ddsHandling,
    this.ddsXCoordinate,
    this.ddsYCoordinate,
    this.fallbackFont,
    this.fontColor,
    this.fontFileBold,
    this.fontFileBoldItalic,
    this.fontFileItalic,
    this.fontFileRegular,
    this.fontOpacity,
    this.fontResolution,
    this.fontScript,
    this.fontSize,
    this.height,
    this.hexFontColor,
    this.outlineColor,
    this.outlineSize,
    this.shadowColor,
    this.shadowOpacity,
    this.shadowXOffset,
    this.shadowYOffset,
    this.stylePassthrough,
    this.subtitlingType,
    this.teletextSpacing,
    this.width,
    this.xPosition,
    this.yPosition,
  });

  factory DvbSubDestinationSettings.fromJson(Map<String, dynamic> json) {
    return DvbSubDestinationSettings(
      alignment:
          (json['alignment'] as String?)?.let(DvbSubtitleAlignment.fromString),
      applyFontColor: (json['applyFontColor'] as String?)
          ?.let(DvbSubtitleApplyFontColor.fromString),
      backgroundColor: (json['backgroundColor'] as String?)
          ?.let(DvbSubtitleBackgroundColor.fromString),
      backgroundOpacity: json['backgroundOpacity'] as int?,
      ddsHandling:
          (json['ddsHandling'] as String?)?.let(DvbddsHandling.fromString),
      ddsXCoordinate: json['ddsXCoordinate'] as int?,
      ddsYCoordinate: json['ddsYCoordinate'] as int?,
      fallbackFont: (json['fallbackFont'] as String?)
          ?.let(DvbSubSubtitleFallbackFont.fromString),
      fontColor:
          (json['fontColor'] as String?)?.let(DvbSubtitleFontColor.fromString),
      fontFileBold: json['fontFileBold'] as String?,
      fontFileBoldItalic: json['fontFileBoldItalic'] as String?,
      fontFileItalic: json['fontFileItalic'] as String?,
      fontFileRegular: json['fontFileRegular'] as String?,
      fontOpacity: json['fontOpacity'] as int?,
      fontResolution: json['fontResolution'] as int?,
      fontScript: (json['fontScript'] as String?)?.let(FontScript.fromString),
      fontSize: json['fontSize'] as int?,
      height: json['height'] as int?,
      hexFontColor: json['hexFontColor'] as String?,
      outlineColor: (json['outlineColor'] as String?)
          ?.let(DvbSubtitleOutlineColor.fromString),
      outlineSize: json['outlineSize'] as int?,
      shadowColor: (json['shadowColor'] as String?)
          ?.let(DvbSubtitleShadowColor.fromString),
      shadowOpacity: json['shadowOpacity'] as int?,
      shadowXOffset: json['shadowXOffset'] as int?,
      shadowYOffset: json['shadowYOffset'] as int?,
      stylePassthrough: (json['stylePassthrough'] as String?)
          ?.let(DvbSubtitleStylePassthrough.fromString),
      subtitlingType: (json['subtitlingType'] as String?)
          ?.let(DvbSubtitlingType.fromString),
      teletextSpacing: (json['teletextSpacing'] as String?)
          ?.let(DvbSubtitleTeletextSpacing.fromString),
      width: json['width'] as int?,
      xPosition: json['xPosition'] as int?,
      yPosition: json['yPosition'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final alignment = this.alignment;
    final applyFontColor = this.applyFontColor;
    final backgroundColor = this.backgroundColor;
    final backgroundOpacity = this.backgroundOpacity;
    final ddsHandling = this.ddsHandling;
    final ddsXCoordinate = this.ddsXCoordinate;
    final ddsYCoordinate = this.ddsYCoordinate;
    final fallbackFont = this.fallbackFont;
    final fontColor = this.fontColor;
    final fontFileBold = this.fontFileBold;
    final fontFileBoldItalic = this.fontFileBoldItalic;
    final fontFileItalic = this.fontFileItalic;
    final fontFileRegular = this.fontFileRegular;
    final fontOpacity = this.fontOpacity;
    final fontResolution = this.fontResolution;
    final fontScript = this.fontScript;
    final fontSize = this.fontSize;
    final height = this.height;
    final hexFontColor = this.hexFontColor;
    final outlineColor = this.outlineColor;
    final outlineSize = this.outlineSize;
    final shadowColor = this.shadowColor;
    final shadowOpacity = this.shadowOpacity;
    final shadowXOffset = this.shadowXOffset;
    final shadowYOffset = this.shadowYOffset;
    final stylePassthrough = this.stylePassthrough;
    final subtitlingType = this.subtitlingType;
    final teletextSpacing = this.teletextSpacing;
    final width = this.width;
    final xPosition = this.xPosition;
    final yPosition = this.yPosition;
    return {
      if (alignment != null) 'alignment': alignment.value,
      if (applyFontColor != null) 'applyFontColor': applyFontColor.value,
      if (backgroundColor != null) 'backgroundColor': backgroundColor.value,
      if (backgroundOpacity != null) 'backgroundOpacity': backgroundOpacity,
      if (ddsHandling != null) 'ddsHandling': ddsHandling.value,
      if (ddsXCoordinate != null) 'ddsXCoordinate': ddsXCoordinate,
      if (ddsYCoordinate != null) 'ddsYCoordinate': ddsYCoordinate,
      if (fallbackFont != null) 'fallbackFont': fallbackFont.value,
      if (fontColor != null) 'fontColor': fontColor.value,
      if (fontFileBold != null) 'fontFileBold': fontFileBold,
      if (fontFileBoldItalic != null) 'fontFileBoldItalic': fontFileBoldItalic,
      if (fontFileItalic != null) 'fontFileItalic': fontFileItalic,
      if (fontFileRegular != null) 'fontFileRegular': fontFileRegular,
      if (fontOpacity != null) 'fontOpacity': fontOpacity,
      if (fontResolution != null) 'fontResolution': fontResolution,
      if (fontScript != null) 'fontScript': fontScript.value,
      if (fontSize != null) 'fontSize': fontSize,
      if (height != null) 'height': height,
      if (hexFontColor != null) 'hexFontColor': hexFontColor,
      if (outlineColor != null) 'outlineColor': outlineColor.value,
      if (outlineSize != null) 'outlineSize': outlineSize,
      if (shadowColor != null) 'shadowColor': shadowColor.value,
      if (shadowOpacity != null) 'shadowOpacity': shadowOpacity,
      if (shadowXOffset != null) 'shadowXOffset': shadowXOffset,
      if (shadowYOffset != null) 'shadowYOffset': shadowYOffset,
      if (stylePassthrough != null) 'stylePassthrough': stylePassthrough.value,
      if (subtitlingType != null) 'subtitlingType': subtitlingType.value,
      if (teletextSpacing != null) 'teletextSpacing': teletextSpacing.value,
      if (width != null) 'width': width,
      if (xPosition != null) 'xPosition': xPosition,
      if (yPosition != null) 'yPosition': yPosition,
    };
  }
}

/// DVB Sub Source Settings
class DvbSubSourceSettings {
  /// When using DVB-Sub with Burn-in, use this PID for the source content. Unused
  /// for DVB-Sub passthrough. All DVB-Sub content is passed through, regardless
  /// of selectors.
  final int? pid;

  DvbSubSourceSettings({
    this.pid,
  });

  factory DvbSubSourceSettings.fromJson(Map<String, dynamic> json) {
    return DvbSubSourceSettings(
      pid: json['pid'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final pid = this.pid;
    return {
      if (pid != null) 'pid': pid,
    };
  }
}

/// Specify the font that you want the service to use for your burn in captions
/// when your input captions specify a font that MediaConvert doesn't support.
/// When you set Fallback font to best match, or leave blank, MediaConvert uses
/// a supported font that most closely matches the font that your input captions
/// specify. When there are multiple unsupported fonts in your input captions,
/// MediaConvert matches each font with the supported font that matches best.
/// When you explicitly choose a replacement font, MediaConvert uses that font
/// to replace all unsupported fonts from your input.
enum DvbSubSubtitleFallbackFont {
  bestMatch('BEST_MATCH'),
  monospacedSansserif('MONOSPACED_SANSSERIF'),
  monospacedSerif('MONOSPACED_SERIF'),
  proportionalSansserif('PROPORTIONAL_SANSSERIF'),
  proportionalSerif('PROPORTIONAL_SERIF'),
  ;

  final String value;

  const DvbSubSubtitleFallbackFont(this.value);

  static DvbSubSubtitleFallbackFont fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum DvbSubSubtitleFallbackFont'));
}

/// Specify the alignment of your captions. If no explicit x_position is
/// provided, setting alignment to centered will placethe captions at the bottom
/// center of the output. Similarly, setting a left alignment willalign captions
/// to the bottom left of the output. If x and y positions are given in
/// conjunction with the alignment parameter, the font will be justified (either
/// left or centered) relative to those coordinates. Within your job settings,
/// all of your DVB-Sub settings must be identical.
enum DvbSubtitleAlignment {
  centered('CENTERED'),
  left('LEFT'),
  auto('AUTO'),
  ;

  final String value;

  const DvbSubtitleAlignment(this.value);

  static DvbSubtitleAlignment fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum DvbSubtitleAlignment'));
}

/// Ignore this setting unless Style Passthrough is set to Enabled and Font
/// color set to Black, Yellow, Red, Green, Blue, or Hex. Use Apply font color
/// for additional font color controls. When you choose White text only, or
/// leave blank, your font color setting only applies to white text in your
/// input captions. For example, if your font color setting is Yellow, and your
/// input captions have red and white text, your output captions will have red
/// and yellow text. When you choose ALL_TEXT, your font color setting applies
/// to all of your output captions text.
enum DvbSubtitleApplyFontColor {
  whiteTextOnly('WHITE_TEXT_ONLY'),
  allText('ALL_TEXT'),
  ;

  final String value;

  const DvbSubtitleApplyFontColor(this.value);

  static DvbSubtitleApplyFontColor fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum DvbSubtitleApplyFontColor'));
}

/// Specify the color of the rectangle behind the captions. Leave background
/// color blank and set Style passthrough to enabled to use the background color
/// data from your input captions, if present.
enum DvbSubtitleBackgroundColor {
  none('NONE'),
  black('BLACK'),
  white('WHITE'),
  auto('AUTO'),
  ;

  final String value;

  const DvbSubtitleBackgroundColor(this.value);

  static DvbSubtitleBackgroundColor fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum DvbSubtitleBackgroundColor'));
}

/// Specify the color of the captions text. Leave Font color blank and set Style
/// passthrough to enabled to use the font color data from your input captions,
/// if present. Within your job settings, all of your DVB-Sub settings must be
/// identical.
enum DvbSubtitleFontColor {
  white('WHITE'),
  black('BLACK'),
  yellow('YELLOW'),
  red('RED'),
  green('GREEN'),
  blue('BLUE'),
  hex('HEX'),
  auto('AUTO'),
  ;

  final String value;

  const DvbSubtitleFontColor(this.value);

  static DvbSubtitleFontColor fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum DvbSubtitleFontColor'));
}

/// Specify font outline color. Leave Outline color blank and set Style
/// passthrough to enabled to use the font outline color data from your input
/// captions, if present. Within your job settings, all of your DVB-Sub settings
/// must be identical.
enum DvbSubtitleOutlineColor {
  black('BLACK'),
  white('WHITE'),
  yellow('YELLOW'),
  red('RED'),
  green('GREEN'),
  blue('BLUE'),
  auto('AUTO'),
  ;

  final String value;

  const DvbSubtitleOutlineColor(this.value);

  static DvbSubtitleOutlineColor fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum DvbSubtitleOutlineColor'));
}

/// Specify the color of the shadow cast by the captions. Leave Shadow color
/// blank and set Style passthrough to enabled to use the shadow color data from
/// your input captions, if present. Within your job settings, all of your
/// DVB-Sub settings must be identical.
enum DvbSubtitleShadowColor {
  none('NONE'),
  black('BLACK'),
  white('WHITE'),
  auto('AUTO'),
  ;

  final String value;

  const DvbSubtitleShadowColor(this.value);

  static DvbSubtitleShadowColor fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum DvbSubtitleShadowColor'));
}

/// To use the available style, color, and position information from your input
/// captions: Set Style passthrough to Enabled. Note that MediaConvert uses
/// default settings for any missing style or position information in your input
/// captions To ignore the style and position information from your input
/// captions and use default settings: Leave blank or keep the default value,
/// Disabled. Default settings include white text with black outlining,
/// bottom-center positioning, and automatic sizing. Whether you set Style
/// passthrough to enabled or not, you can also choose to manually override any
/// of the individual style and position settings. You can also override any
/// fonts by manually specifying custom font files.
enum DvbSubtitleStylePassthrough {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const DvbSubtitleStylePassthrough(this.value);

  static DvbSubtitleStylePassthrough fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum DvbSubtitleStylePassthrough'));
}

/// Specify whether the Text spacing in your captions is set by the captions
/// grid, or varies depending on letter width. Choose fixed grid to conform to
/// the spacing specified in the captions file more accurately. Choose
/// proportional to make the text easier to read for closed captions. Within
/// your job settings, all of your DVB-Sub settings must be identical.
enum DvbSubtitleTeletextSpacing {
  fixedGrid('FIXED_GRID'),
  proportional('PROPORTIONAL'),
  auto('AUTO'),
  ;

  final String value;

  const DvbSubtitleTeletextSpacing(this.value);

  static DvbSubtitleTeletextSpacing fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum DvbSubtitleTeletextSpacing'));
}

/// Specify whether your DVB subtitles are standard or for hearing impaired.
/// Choose hearing impaired if your subtitles include audio descriptions and
/// dialogue. Choose standard if your subtitles include only dialogue.
enum DvbSubtitlingType {
  hearingImpaired('HEARING_IMPAIRED'),
  standard('STANDARD'),
  ;

  final String value;

  const DvbSubtitlingType(this.value);

  static DvbSubtitlingType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum DvbSubtitlingType'));
}

/// Use these settings to insert a DVB Time and Date Table (TDT) in the
/// transport stream of this output.
class DvbTdtSettings {
  /// The number of milliseconds between instances of this table in the output
  /// transport stream.
  final int? tdtInterval;

  DvbTdtSettings({
    this.tdtInterval,
  });

  factory DvbTdtSettings.fromJson(Map<String, dynamic> json) {
    return DvbTdtSettings(
      tdtInterval: json['tdtInterval'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final tdtInterval = this.tdtInterval;
    return {
      if (tdtInterval != null) 'tdtInterval': tdtInterval,
    };
  }
}

/// Specify how MediaConvert handles the display definition segment (DDS). To
/// exclude the DDS from this set of captions: Keep the default, None. To
/// include the DDS: Choose Specified. When you do, also specify the offset
/// coordinates of the display window with DDS x-coordinate and DDS
/// y-coordinate. To include the DDS, but not include display window data:
/// Choose No display window. When you do, you can write position metadata to
/// the page composition segment (PCS) with DDS x-coordinate and DDS
/// y-coordinate. For video resolutions with a height of 576 pixels or less,
/// MediaConvert doesn't include the DDS, regardless of the value you choose for
/// DDS handling. All burn-in and DVB-Sub font settings must match.
enum DvbddsHandling {
  none('NONE'),
  specified('SPECIFIED'),
  noDisplayWindow('NO_DISPLAY_WINDOW'),
  ;

  final String value;

  const DvbddsHandling(this.value);

  static DvbddsHandling fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum DvbddsHandling'));
}

/// Specify the bitstream mode for the E-AC-3 stream that the encoder emits. For
/// more information about the EAC3 bitstream mode, see ATSC A/52-2012 (Annex
/// E).
enum Eac3AtmosBitstreamMode {
  completeMain('COMPLETE_MAIN'),
  ;

  final String value;

  const Eac3AtmosBitstreamMode(this.value);

  static Eac3AtmosBitstreamMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Eac3AtmosBitstreamMode'));
}

/// The coding mode for Dolby Digital Plus JOC (Atmos).
enum Eac3AtmosCodingMode {
  codingModeAuto('CODING_MODE_AUTO'),
  codingMode_5_1_4('CODING_MODE_5_1_4'),
  codingMode_7_1_4('CODING_MODE_7_1_4'),
  codingMode_9_1_6('CODING_MODE_9_1_6'),
  ;

  final String value;

  const Eac3AtmosCodingMode(this.value);

  static Eac3AtmosCodingMode fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Eac3AtmosCodingMode'));
}

/// Enable Dolby Dialogue Intelligence to adjust loudness based on dialogue
/// analysis.
enum Eac3AtmosDialogueIntelligence {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const Eac3AtmosDialogueIntelligence(this.value);

  static Eac3AtmosDialogueIntelligence fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Eac3AtmosDialogueIntelligence'));
}

/// Specify whether MediaConvert should use any downmix metadata from your input
/// file. Keep the default value, Custom to provide downmix values in your job
/// settings. Choose Follow source to use the metadata from your input. Related
/// settings--Use these settings to specify your downmix values: Left only/Right
/// only surround, Left total/Right total surround, Left total/Right total
/// center, Left only/Right only center, and Stereo downmix. When you keep
/// Custom for Downmix control and you don't specify values for the related
/// settings, MediaConvert uses default values for those settings.
enum Eac3AtmosDownmixControl {
  specified('SPECIFIED'),
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  ;

  final String value;

  const Eac3AtmosDownmixControl(this.value);

  static Eac3AtmosDownmixControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Eac3AtmosDownmixControl'));
}

/// Choose the Dolby dynamic range control (DRC) profile that MediaConvert uses
/// when encoding the metadata in the Dolby stream for the line operating mode.
/// Default value: Film light Related setting: To have MediaConvert use the
/// value you specify here, keep the default value, Custom for the setting
/// Dynamic range control. Otherwise, MediaConvert ignores Dynamic range
/// compression line. For information about the Dolby DRC operating modes and
/// profiles, see the Dynamic Range Control chapter of the Dolby Metadata Guide
/// at
/// https://developer.dolby.com/globalassets/professional/documents/dolby-metadata-guide.pdf.
enum Eac3AtmosDynamicRangeCompressionLine {
  none('NONE'),
  filmStandard('FILM_STANDARD'),
  filmLight('FILM_LIGHT'),
  musicStandard('MUSIC_STANDARD'),
  musicLight('MUSIC_LIGHT'),
  speech('SPEECH'),
  ;

  final String value;

  const Eac3AtmosDynamicRangeCompressionLine(this.value);

  static Eac3AtmosDynamicRangeCompressionLine fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Eac3AtmosDynamicRangeCompressionLine'));
}

/// Choose the Dolby dynamic range control (DRC) profile that MediaConvert uses
/// when encoding the metadata in the Dolby stream for the RF operating mode.
/// Default value: Film light Related setting: To have MediaConvert use the
/// value you specify here, keep the default value, Custom for the setting
/// Dynamic range control. Otherwise, MediaConvert ignores Dynamic range
/// compression RF. For information about the Dolby DRC operating modes and
/// profiles, see the Dynamic Range Control chapter of the Dolby Metadata Guide
/// at
/// https://developer.dolby.com/globalassets/professional/documents/dolby-metadata-guide.pdf.
enum Eac3AtmosDynamicRangeCompressionRf {
  none('NONE'),
  filmStandard('FILM_STANDARD'),
  filmLight('FILM_LIGHT'),
  musicStandard('MUSIC_STANDARD'),
  musicLight('MUSIC_LIGHT'),
  speech('SPEECH'),
  ;

  final String value;

  const Eac3AtmosDynamicRangeCompressionRf(this.value);

  static Eac3AtmosDynamicRangeCompressionRf fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Eac3AtmosDynamicRangeCompressionRf'));
}

/// Specify whether MediaConvert should use any dynamic range control metadata
/// from your input file. Keep the default value, Custom, to provide dynamic
/// range control values in your job settings. Choose Follow source to use the
/// metadata from your input. Related settings--Use these settings to specify
/// your dynamic range control values: Dynamic range compression line and
/// Dynamic range compression RF. When you keep the value Custom for Dynamic
/// range control and you don't specify values for the related settings,
/// MediaConvert uses default values for those settings.
enum Eac3AtmosDynamicRangeControl {
  specified('SPECIFIED'),
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  ;

  final String value;

  const Eac3AtmosDynamicRangeControl(this.value);

  static Eac3AtmosDynamicRangeControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Eac3AtmosDynamicRangeControl'));
}

/// Choose how the service meters the loudness of your audio.
enum Eac3AtmosMeteringMode {
  leqA('LEQ_A'),
  ituBs_1770_1('ITU_BS_1770_1'),
  ituBs_1770_2('ITU_BS_1770_2'),
  ituBs_1770_3('ITU_BS_1770_3'),
  ituBs_1770_4('ITU_BS_1770_4'),
  ;

  final String value;

  const Eac3AtmosMeteringMode(this.value);

  static Eac3AtmosMeteringMode fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Eac3AtmosMeteringMode'));
}

/// Required when you set Codec to the value EAC3_ATMOS.
class Eac3AtmosSettings {
  /// Specify the average bitrate for this output in bits per second. Valid
  /// values: 384k, 448k, 576k, 640k, 768k, 1024k Default value: 448k Note that
  /// MediaConvert supports 384k only with channel-based immersive (CBI) 7.1.4 and
  /// 5.1.4 inputs. For CBI 9.1.6 and other input types, MediaConvert
  /// automatically increases your output bitrate to 448k.
  final int? bitrate;

  /// Specify the bitstream mode for the E-AC-3 stream that the encoder emits. For
  /// more information about the EAC3 bitstream mode, see ATSC A/52-2012 (Annex
  /// E).
  final Eac3AtmosBitstreamMode? bitstreamMode;

  /// The coding mode for Dolby Digital Plus JOC (Atmos).
  final Eac3AtmosCodingMode? codingMode;

  /// Enable Dolby Dialogue Intelligence to adjust loudness based on dialogue
  /// analysis.
  final Eac3AtmosDialogueIntelligence? dialogueIntelligence;

  /// Specify whether MediaConvert should use any downmix metadata from your input
  /// file. Keep the default value, Custom to provide downmix values in your job
  /// settings. Choose Follow source to use the metadata from your input. Related
  /// settings--Use these settings to specify your downmix values: Left only/Right
  /// only surround, Left total/Right total surround, Left total/Right total
  /// center, Left only/Right only center, and Stereo downmix. When you keep
  /// Custom for Downmix control and you don't specify values for the related
  /// settings, MediaConvert uses default values for those settings.
  final Eac3AtmosDownmixControl? downmixControl;

  /// Choose the Dolby dynamic range control (DRC) profile that MediaConvert uses
  /// when encoding the metadata in the Dolby stream for the line operating mode.
  /// Default value: Film light Related setting: To have MediaConvert use the
  /// value you specify here, keep the default value, Custom for the setting
  /// Dynamic range control. Otherwise, MediaConvert ignores Dynamic range
  /// compression line. For information about the Dolby DRC operating modes and
  /// profiles, see the Dynamic Range Control chapter of the Dolby Metadata Guide
  /// at
  /// https://developer.dolby.com/globalassets/professional/documents/dolby-metadata-guide.pdf.
  final Eac3AtmosDynamicRangeCompressionLine? dynamicRangeCompressionLine;

  /// Choose the Dolby dynamic range control (DRC) profile that MediaConvert uses
  /// when encoding the metadata in the Dolby stream for the RF operating mode.
  /// Default value: Film light Related setting: To have MediaConvert use the
  /// value you specify here, keep the default value, Custom for the setting
  /// Dynamic range control. Otherwise, MediaConvert ignores Dynamic range
  /// compression RF. For information about the Dolby DRC operating modes and
  /// profiles, see the Dynamic Range Control chapter of the Dolby Metadata Guide
  /// at
  /// https://developer.dolby.com/globalassets/professional/documents/dolby-metadata-guide.pdf.
  final Eac3AtmosDynamicRangeCompressionRf? dynamicRangeCompressionRf;

  /// Specify whether MediaConvert should use any dynamic range control metadata
  /// from your input file. Keep the default value, Custom, to provide dynamic
  /// range control values in your job settings. Choose Follow source to use the
  /// metadata from your input. Related settings--Use these settings to specify
  /// your dynamic range control values: Dynamic range compression line and
  /// Dynamic range compression RF. When you keep the value Custom for Dynamic
  /// range control and you don't specify values for the related settings,
  /// MediaConvert uses default values for those settings.
  final Eac3AtmosDynamicRangeControl? dynamicRangeControl;

  /// Specify a value for the following Dolby Atmos setting: Left only/Right only
  /// center mix (Lo/Ro center). MediaConvert uses this value for downmixing.
  /// Default value: -3 dB. Valid values: 3.0, 1.5, 0.0, -1.5, -3.0, -4.5, and
  /// -6.0. Related setting: How the service uses this value depends on the value
  /// that you choose for Stereo downmix. Related setting: To have MediaConvert
  /// use this value, keep the default value, Custom for the setting Downmix
  /// control. Otherwise, MediaConvert ignores Left only/Right only center.
  final double? loRoCenterMixLevel;

  /// Specify a value for the following Dolby Atmos setting: Left only/Right only.
  /// MediaConvert uses this value for downmixing. Default value: -3 dB. Valid
  /// values: -1.5, -3.0, -4.5, -6.0, and -60. The value -60 mutes the channel.
  /// Related setting: How the service uses this value depends on the value that
  /// you choose for Stereo downmix. Related setting: To have MediaConvert use
  /// this value, keep the default value, Custom for the setting Downmix control.
  /// Otherwise, MediaConvert ignores Left only/Right only surround.
  final double? loRoSurroundMixLevel;

  /// Specify a value for the following Dolby Atmos setting: Left total/Right
  /// total center mix (Lt/Rt center). MediaConvert uses this value for
  /// downmixing. Default value: -3 dB Valid values: 3.0, 1.5, 0.0, -1.5, -3.0,
  /// -4.5, and -6.0. Related setting: How the service uses this value depends on
  /// the value that you choose for Stereo downmix. Related setting: To have
  /// MediaConvert use this value, keep the default value, Custom for the setting
  /// Downmix control. Otherwise, MediaConvert ignores Left total/Right total
  /// center.
  final double? ltRtCenterMixLevel;

  /// Specify a value for the following Dolby Atmos setting: Left total/Right
  /// total surround mix (Lt/Rt surround). MediaConvert uses this value for
  /// downmixing. Default value: -3 dB Valid values: -1.5, -3.0, -4.5, -6.0, and
  /// -60. The value -60 mutes the channel. Related setting: How the service uses
  /// this value depends on the value that you choose for Stereo downmix. Related
  /// setting: To have MediaConvert use this value, keep the default value, Custom
  /// for the setting Downmix control. Otherwise, the service ignores Left
  /// total/Right total surround.
  final double? ltRtSurroundMixLevel;

  /// Choose how the service meters the loudness of your audio.
  final Eac3AtmosMeteringMode? meteringMode;

  /// This value is always 48000. It represents the sample rate in Hz.
  final int? sampleRate;

  /// Specify the percentage of audio content, from 0% to 100%, that must be
  /// speech in order for the encoder to use the measured speech loudness as the
  /// overall program loudness. Default value: 15%
  final int? speechThreshold;

  /// Choose how the service does stereo downmixing. Default value: Not indicated
  /// Related setting: To have MediaConvert use this value, keep the default
  /// value, Custom for the setting Downmix control. Otherwise, MediaConvert
  /// ignores Stereo downmix.
  final Eac3AtmosStereoDownmix? stereoDownmix;

  /// Specify whether your input audio has an additional center rear surround
  /// channel matrix encoded into your left and right surround channels.
  final Eac3AtmosSurroundExMode? surroundExMode;

  Eac3AtmosSettings({
    this.bitrate,
    this.bitstreamMode,
    this.codingMode,
    this.dialogueIntelligence,
    this.downmixControl,
    this.dynamicRangeCompressionLine,
    this.dynamicRangeCompressionRf,
    this.dynamicRangeControl,
    this.loRoCenterMixLevel,
    this.loRoSurroundMixLevel,
    this.ltRtCenterMixLevel,
    this.ltRtSurroundMixLevel,
    this.meteringMode,
    this.sampleRate,
    this.speechThreshold,
    this.stereoDownmix,
    this.surroundExMode,
  });

  factory Eac3AtmosSettings.fromJson(Map<String, dynamic> json) {
    return Eac3AtmosSettings(
      bitrate: json['bitrate'] as int?,
      bitstreamMode: (json['bitstreamMode'] as String?)
          ?.let(Eac3AtmosBitstreamMode.fromString),
      codingMode:
          (json['codingMode'] as String?)?.let(Eac3AtmosCodingMode.fromString),
      dialogueIntelligence: (json['dialogueIntelligence'] as String?)
          ?.let(Eac3AtmosDialogueIntelligence.fromString),
      downmixControl: (json['downmixControl'] as String?)
          ?.let(Eac3AtmosDownmixControl.fromString),
      dynamicRangeCompressionLine:
          (json['dynamicRangeCompressionLine'] as String?)
              ?.let(Eac3AtmosDynamicRangeCompressionLine.fromString),
      dynamicRangeCompressionRf: (json['dynamicRangeCompressionRf'] as String?)
          ?.let(Eac3AtmosDynamicRangeCompressionRf.fromString),
      dynamicRangeControl: (json['dynamicRangeControl'] as String?)
          ?.let(Eac3AtmosDynamicRangeControl.fromString),
      loRoCenterMixLevel: json['loRoCenterMixLevel'] as double?,
      loRoSurroundMixLevel: json['loRoSurroundMixLevel'] as double?,
      ltRtCenterMixLevel: json['ltRtCenterMixLevel'] as double?,
      ltRtSurroundMixLevel: json['ltRtSurroundMixLevel'] as double?,
      meteringMode: (json['meteringMode'] as String?)
          ?.let(Eac3AtmosMeteringMode.fromString),
      sampleRate: json['sampleRate'] as int?,
      speechThreshold: json['speechThreshold'] as int?,
      stereoDownmix: (json['stereoDownmix'] as String?)
          ?.let(Eac3AtmosStereoDownmix.fromString),
      surroundExMode: (json['surroundExMode'] as String?)
          ?.let(Eac3AtmosSurroundExMode.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final bitrate = this.bitrate;
    final bitstreamMode = this.bitstreamMode;
    final codingMode = this.codingMode;
    final dialogueIntelligence = this.dialogueIntelligence;
    final downmixControl = this.downmixControl;
    final dynamicRangeCompressionLine = this.dynamicRangeCompressionLine;
    final dynamicRangeCompressionRf = this.dynamicRangeCompressionRf;
    final dynamicRangeControl = this.dynamicRangeControl;
    final loRoCenterMixLevel = this.loRoCenterMixLevel;
    final loRoSurroundMixLevel = this.loRoSurroundMixLevel;
    final ltRtCenterMixLevel = this.ltRtCenterMixLevel;
    final ltRtSurroundMixLevel = this.ltRtSurroundMixLevel;
    final meteringMode = this.meteringMode;
    final sampleRate = this.sampleRate;
    final speechThreshold = this.speechThreshold;
    final stereoDownmix = this.stereoDownmix;
    final surroundExMode = this.surroundExMode;
    return {
      if (bitrate != null) 'bitrate': bitrate,
      if (bitstreamMode != null) 'bitstreamMode': bitstreamMode.value,
      if (codingMode != null) 'codingMode': codingMode.value,
      if (dialogueIntelligence != null)
        'dialogueIntelligence': dialogueIntelligence.value,
      if (downmixControl != null) 'downmixControl': downmixControl.value,
      if (dynamicRangeCompressionLine != null)
        'dynamicRangeCompressionLine': dynamicRangeCompressionLine.value,
      if (dynamicRangeCompressionRf != null)
        'dynamicRangeCompressionRf': dynamicRangeCompressionRf.value,
      if (dynamicRangeControl != null)
        'dynamicRangeControl': dynamicRangeControl.value,
      if (loRoCenterMixLevel != null) 'loRoCenterMixLevel': loRoCenterMixLevel,
      if (loRoSurroundMixLevel != null)
        'loRoSurroundMixLevel': loRoSurroundMixLevel,
      if (ltRtCenterMixLevel != null) 'ltRtCenterMixLevel': ltRtCenterMixLevel,
      if (ltRtSurroundMixLevel != null)
        'ltRtSurroundMixLevel': ltRtSurroundMixLevel,
      if (meteringMode != null) 'meteringMode': meteringMode.value,
      if (sampleRate != null) 'sampleRate': sampleRate,
      if (speechThreshold != null) 'speechThreshold': speechThreshold,
      if (stereoDownmix != null) 'stereoDownmix': stereoDownmix.value,
      if (surroundExMode != null) 'surroundExMode': surroundExMode.value,
    };
  }
}

/// Choose how the service does stereo downmixing. Default value: Not indicated
/// Related setting: To have MediaConvert use this value, keep the default
/// value, Custom for the setting Downmix control. Otherwise, MediaConvert
/// ignores Stereo downmix.
enum Eac3AtmosStereoDownmix {
  notIndicated('NOT_INDICATED'),
  stereo('STEREO'),
  surround('SURROUND'),
  dpl2('DPL2'),
  ;

  final String value;

  const Eac3AtmosStereoDownmix(this.value);

  static Eac3AtmosStereoDownmix fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Eac3AtmosStereoDownmix'));
}

/// Specify whether your input audio has an additional center rear surround
/// channel matrix encoded into your left and right surround channels.
enum Eac3AtmosSurroundExMode {
  notIndicated('NOT_INDICATED'),
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const Eac3AtmosSurroundExMode(this.value);

  static Eac3AtmosSurroundExMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Eac3AtmosSurroundExMode'));
}

/// If set to ATTENUATE_3_DB, applies a 3 dB attenuation to the surround
/// channels. Only used for 3/2 coding mode.
enum Eac3AttenuationControl {
  attenuate_3Db('ATTENUATE_3_DB'),
  none('NONE'),
  ;

  final String value;

  const Eac3AttenuationControl(this.value);

  static Eac3AttenuationControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Eac3AttenuationControl'));
}

/// Specify the bitstream mode for the E-AC-3 stream that the encoder emits. For
/// more information about the EAC3 bitstream mode, see ATSC A/52-2012 (Annex
/// E).
enum Eac3BitstreamMode {
  completeMain('COMPLETE_MAIN'),
  commentary('COMMENTARY'),
  emergency('EMERGENCY'),
  hearingImpaired('HEARING_IMPAIRED'),
  visuallyImpaired('VISUALLY_IMPAIRED'),
  ;

  final String value;

  const Eac3BitstreamMode(this.value);

  static Eac3BitstreamMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Eac3BitstreamMode'));
}

/// Dolby Digital Plus coding mode. Determines number of channels.
enum Eac3CodingMode {
  codingMode_1_0('CODING_MODE_1_0'),
  codingMode_2_0('CODING_MODE_2_0'),
  codingMode_3_2('CODING_MODE_3_2'),
  ;

  final String value;

  const Eac3CodingMode(this.value);

  static Eac3CodingMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Eac3CodingMode'));
}

/// Activates a DC highpass filter for all input channels.
enum Eac3DcFilter {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const Eac3DcFilter(this.value);

  static Eac3DcFilter fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Eac3DcFilter'));
}

/// Choose the Dolby Digital dynamic range control (DRC) profile that
/// MediaConvert uses when encoding the metadata in the Dolby Digital stream for
/// the line operating mode. Related setting: When you use this setting,
/// MediaConvert ignores any value you provide for Dynamic range compression
/// profile. For information about the Dolby Digital DRC operating modes and
/// profiles, see the Dynamic Range Control chapter of the Dolby Metadata Guide
/// at
/// https://developer.dolby.com/globalassets/professional/documents/dolby-metadata-guide.pdf.
enum Eac3DynamicRangeCompressionLine {
  none('NONE'),
  filmStandard('FILM_STANDARD'),
  filmLight('FILM_LIGHT'),
  musicStandard('MUSIC_STANDARD'),
  musicLight('MUSIC_LIGHT'),
  speech('SPEECH'),
  ;

  final String value;

  const Eac3DynamicRangeCompressionLine(this.value);

  static Eac3DynamicRangeCompressionLine fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Eac3DynamicRangeCompressionLine'));
}

/// Choose the Dolby Digital dynamic range control (DRC) profile that
/// MediaConvert uses when encoding the metadata in the Dolby Digital stream for
/// the RF operating mode. Related setting: When you use this setting,
/// MediaConvert ignores any value you provide for Dynamic range compression
/// profile. For information about the Dolby Digital DRC operating modes and
/// profiles, see the Dynamic Range Control chapter of the Dolby Metadata Guide
/// at
/// https://developer.dolby.com/globalassets/professional/documents/dolby-metadata-guide.pdf.
enum Eac3DynamicRangeCompressionRf {
  none('NONE'),
  filmStandard('FILM_STANDARD'),
  filmLight('FILM_LIGHT'),
  musicStandard('MUSIC_STANDARD'),
  musicLight('MUSIC_LIGHT'),
  speech('SPEECH'),
  ;

  final String value;

  const Eac3DynamicRangeCompressionRf(this.value);

  static Eac3DynamicRangeCompressionRf fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Eac3DynamicRangeCompressionRf'));
}

/// When encoding 3/2 audio, controls whether the LFE channel is enabled
enum Eac3LfeControl {
  lfe('LFE'),
  noLfe('NO_LFE'),
  ;

  final String value;

  const Eac3LfeControl(this.value);

  static Eac3LfeControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Eac3LfeControl'));
}

/// Applies a 120Hz lowpass filter to the LFE channel prior to encoding. Only
/// valid with 3_2_LFE coding mode.
enum Eac3LfeFilter {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const Eac3LfeFilter(this.value);

  static Eac3LfeFilter fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Eac3LfeFilter'));
}

/// When set to FOLLOW_INPUT, encoder metadata will be sourced from the DD, DD+,
/// or DolbyE decoder that supplied this audio data. If audio was not supplied
/// from one of these streams, then the static metadata settings will be used.
enum Eac3MetadataControl {
  followInput('FOLLOW_INPUT'),
  useConfigured('USE_CONFIGURED'),
  ;

  final String value;

  const Eac3MetadataControl(this.value);

  static Eac3MetadataControl fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Eac3MetadataControl'));
}

/// When set to WHEN_POSSIBLE, input DD+ audio will be passed through if it is
/// present on the input. this detection is dynamic over the life of the
/// transcode. Inputs that alternate between DD+ and non-DD+ content will have a
/// consistent DD+ output as the system alternates between passthrough and
/// encoding.
enum Eac3PassthroughControl {
  whenPossible('WHEN_POSSIBLE'),
  noPassthrough('NO_PASSTHROUGH'),
  ;

  final String value;

  const Eac3PassthroughControl(this.value);

  static Eac3PassthroughControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Eac3PassthroughControl'));
}

/// Controls the amount of phase-shift applied to the surround channels. Only
/// used for 3/2 coding mode.
enum Eac3PhaseControl {
  shift_90Degrees('SHIFT_90_DEGREES'),
  noShift('NO_SHIFT'),
  ;

  final String value;

  const Eac3PhaseControl(this.value);

  static Eac3PhaseControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Eac3PhaseControl'));
}

/// Required when you set Codec to the value EAC3.
class Eac3Settings {
  /// If set to ATTENUATE_3_DB, applies a 3 dB attenuation to the surround
  /// channels. Only used for 3/2 coding mode.
  final Eac3AttenuationControl? attenuationControl;

  /// Specify the average bitrate in bits per second. The bitrate that you specify
  /// must be a multiple of 8000 within the allowed minimum and maximum values.
  /// Leave blank to use the default bitrate for the coding mode you select
  /// according ETSI TS 102 366. Valid bitrates for coding mode 1/0: Default:
  /// 96000. Minimum: 32000. Maximum: 3024000. Valid bitrates for coding mode 2/0:
  /// Default: 192000. Minimum: 96000. Maximum: 3024000. Valid bitrates for coding
  /// mode 3/2: Default: 384000. Minimum: 192000. Maximum: 3024000.
  final int? bitrate;

  /// Specify the bitstream mode for the E-AC-3 stream that the encoder emits. For
  /// more information about the EAC3 bitstream mode, see ATSC A/52-2012 (Annex
  /// E).
  final Eac3BitstreamMode? bitstreamMode;

  /// Dolby Digital Plus coding mode. Determines number of channels.
  final Eac3CodingMode? codingMode;

  /// Activates a DC highpass filter for all input channels.
  final Eac3DcFilter? dcFilter;

  /// Sets the dialnorm for the output. If blank and input audio is Dolby Digital
  /// Plus, dialnorm will be passed through.
  final int? dialnorm;

  /// Choose the Dolby Digital dynamic range control (DRC) profile that
  /// MediaConvert uses when encoding the metadata in the Dolby Digital stream for
  /// the line operating mode. Related setting: When you use this setting,
  /// MediaConvert ignores any value you provide for Dynamic range compression
  /// profile. For information about the Dolby Digital DRC operating modes and
  /// profiles, see the Dynamic Range Control chapter of the Dolby Metadata Guide
  /// at
  /// https://developer.dolby.com/globalassets/professional/documents/dolby-metadata-guide.pdf.
  final Eac3DynamicRangeCompressionLine? dynamicRangeCompressionLine;

  /// Choose the Dolby Digital dynamic range control (DRC) profile that
  /// MediaConvert uses when encoding the metadata in the Dolby Digital stream for
  /// the RF operating mode. Related setting: When you use this setting,
  /// MediaConvert ignores any value you provide for Dynamic range compression
  /// profile. For information about the Dolby Digital DRC operating modes and
  /// profiles, see the Dynamic Range Control chapter of the Dolby Metadata Guide
  /// at
  /// https://developer.dolby.com/globalassets/professional/documents/dolby-metadata-guide.pdf.
  final Eac3DynamicRangeCompressionRf? dynamicRangeCompressionRf;

  /// When encoding 3/2 audio, controls whether the LFE channel is enabled
  final Eac3LfeControl? lfeControl;

  /// Applies a 120Hz lowpass filter to the LFE channel prior to encoding. Only
  /// valid with 3_2_LFE coding mode.
  final Eac3LfeFilter? lfeFilter;

  /// Specify a value for the following Dolby Digital Plus setting: Left
  /// only/Right only center mix. MediaConvert uses this value for downmixing. How
  /// the service uses this value depends on the value that you choose for Stereo
  /// downmix. Valid values: 3.0, 1.5, 0.0, -1.5, -3.0, -4.5, -6.0, and -60. The
  /// value -60 mutes the channel. This setting applies only if you keep the
  /// default value of 3/2 - L, R, C, Ls, Rs for the setting Coding mode. If you
  /// choose a different value for Coding mode, the service ignores Left
  /// only/Right only center.
  final double? loRoCenterMixLevel;

  /// Specify a value for the following Dolby Digital Plus setting: Left
  /// only/Right only. MediaConvert uses this value for downmixing. How the
  /// service uses this value depends on the value that you choose for Stereo
  /// downmix. Valid values: -1.5, -3.0, -4.5, -6.0, and -60. The value -60 mutes
  /// the channel. This setting applies only if you keep the default value of 3/2
  /// - L, R, C, Ls, Rs for the setting Coding mode. If you choose a different
  /// value for Coding mode, the service ignores Left only/Right only surround.
  final double? loRoSurroundMixLevel;

  /// Specify a value for the following Dolby Digital Plus setting: Left
  /// total/Right total center mix. MediaConvert uses this value for downmixing.
  /// How the service uses this value depends on the value that you choose for
  /// Stereo downmix. Valid values: 3.0, 1.5, 0.0, -1.5, -3.0, -4.5, -6.0, and
  /// -60. The value -60 mutes the channel. This setting applies only if you keep
  /// the default value of 3/2 - L, R, C, Ls, Rs for the setting Coding mode. If
  /// you choose a different value for Coding mode, the service ignores Left
  /// total/Right total center.
  final double? ltRtCenterMixLevel;

  /// Specify a value for the following Dolby Digital Plus setting: Left
  /// total/Right total surround mix. MediaConvert uses this value for downmixing.
  /// How the service uses this value depends on the value that you choose for
  /// Stereo downmix. Valid values: -1.5, -3.0, -4.5, -6.0, and -60. The value -60
  /// mutes the channel. This setting applies only if you keep the default value
  /// of 3/2 - L, R, C, Ls, Rs for the setting Coding mode. If you choose a
  /// different value for Coding mode, the service ignores Left total/Right total
  /// surround.
  final double? ltRtSurroundMixLevel;

  /// When set to FOLLOW_INPUT, encoder metadata will be sourced from the DD, DD+,
  /// or DolbyE decoder that supplied this audio data. If audio was not supplied
  /// from one of these streams, then the static metadata settings will be used.
  final Eac3MetadataControl? metadataControl;

  /// When set to WHEN_POSSIBLE, input DD+ audio will be passed through if it is
  /// present on the input. this detection is dynamic over the life of the
  /// transcode. Inputs that alternate between DD+ and non-DD+ content will have a
  /// consistent DD+ output as the system alternates between passthrough and
  /// encoding.
  final Eac3PassthroughControl? passthroughControl;

  /// Controls the amount of phase-shift applied to the surround channels. Only
  /// used for 3/2 coding mode.
  final Eac3PhaseControl? phaseControl;

  /// This value is always 48000. It represents the sample rate in Hz.
  final int? sampleRate;

  /// Choose how the service does stereo downmixing. This setting only applies if
  /// you keep the default value of 3/2 - L, R, C, Ls, Rs for the setting Coding
  /// mode. If you choose a different value for Coding mode, the service ignores
  /// Stereo downmix.
  final Eac3StereoDownmix? stereoDownmix;

  /// When encoding 3/2 audio, sets whether an extra center back surround channel
  /// is matrix encoded into the left and right surround channels.
  final Eac3SurroundExMode? surroundExMode;

  /// When encoding 2/0 audio, sets whether Dolby Surround is matrix encoded into
  /// the two channels.
  final Eac3SurroundMode? surroundMode;

  Eac3Settings({
    this.attenuationControl,
    this.bitrate,
    this.bitstreamMode,
    this.codingMode,
    this.dcFilter,
    this.dialnorm,
    this.dynamicRangeCompressionLine,
    this.dynamicRangeCompressionRf,
    this.lfeControl,
    this.lfeFilter,
    this.loRoCenterMixLevel,
    this.loRoSurroundMixLevel,
    this.ltRtCenterMixLevel,
    this.ltRtSurroundMixLevel,
    this.metadataControl,
    this.passthroughControl,
    this.phaseControl,
    this.sampleRate,
    this.stereoDownmix,
    this.surroundExMode,
    this.surroundMode,
  });

  factory Eac3Settings.fromJson(Map<String, dynamic> json) {
    return Eac3Settings(
      attenuationControl: (json['attenuationControl'] as String?)
          ?.let(Eac3AttenuationControl.fromString),
      bitrate: json['bitrate'] as int?,
      bitstreamMode:
          (json['bitstreamMode'] as String?)?.let(Eac3BitstreamMode.fromString),
      codingMode:
          (json['codingMode'] as String?)?.let(Eac3CodingMode.fromString),
      dcFilter: (json['dcFilter'] as String?)?.let(Eac3DcFilter.fromString),
      dialnorm: json['dialnorm'] as int?,
      dynamicRangeCompressionLine:
          (json['dynamicRangeCompressionLine'] as String?)
              ?.let(Eac3DynamicRangeCompressionLine.fromString),
      dynamicRangeCompressionRf: (json['dynamicRangeCompressionRf'] as String?)
          ?.let(Eac3DynamicRangeCompressionRf.fromString),
      lfeControl:
          (json['lfeControl'] as String?)?.let(Eac3LfeControl.fromString),
      lfeFilter: (json['lfeFilter'] as String?)?.let(Eac3LfeFilter.fromString),
      loRoCenterMixLevel: json['loRoCenterMixLevel'] as double?,
      loRoSurroundMixLevel: json['loRoSurroundMixLevel'] as double?,
      ltRtCenterMixLevel: json['ltRtCenterMixLevel'] as double?,
      ltRtSurroundMixLevel: json['ltRtSurroundMixLevel'] as double?,
      metadataControl: (json['metadataControl'] as String?)
          ?.let(Eac3MetadataControl.fromString),
      passthroughControl: (json['passthroughControl'] as String?)
          ?.let(Eac3PassthroughControl.fromString),
      phaseControl:
          (json['phaseControl'] as String?)?.let(Eac3PhaseControl.fromString),
      sampleRate: json['sampleRate'] as int?,
      stereoDownmix:
          (json['stereoDownmix'] as String?)?.let(Eac3StereoDownmix.fromString),
      surroundExMode: (json['surroundExMode'] as String?)
          ?.let(Eac3SurroundExMode.fromString),
      surroundMode:
          (json['surroundMode'] as String?)?.let(Eac3SurroundMode.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final attenuationControl = this.attenuationControl;
    final bitrate = this.bitrate;
    final bitstreamMode = this.bitstreamMode;
    final codingMode = this.codingMode;
    final dcFilter = this.dcFilter;
    final dialnorm = this.dialnorm;
    final dynamicRangeCompressionLine = this.dynamicRangeCompressionLine;
    final dynamicRangeCompressionRf = this.dynamicRangeCompressionRf;
    final lfeControl = this.lfeControl;
    final lfeFilter = this.lfeFilter;
    final loRoCenterMixLevel = this.loRoCenterMixLevel;
    final loRoSurroundMixLevel = this.loRoSurroundMixLevel;
    final ltRtCenterMixLevel = this.ltRtCenterMixLevel;
    final ltRtSurroundMixLevel = this.ltRtSurroundMixLevel;
    final metadataControl = this.metadataControl;
    final passthroughControl = this.passthroughControl;
    final phaseControl = this.phaseControl;
    final sampleRate = this.sampleRate;
    final stereoDownmix = this.stereoDownmix;
    final surroundExMode = this.surroundExMode;
    final surroundMode = this.surroundMode;
    return {
      if (attenuationControl != null)
        'attenuationControl': attenuationControl.value,
      if (bitrate != null) 'bitrate': bitrate,
      if (bitstreamMode != null) 'bitstreamMode': bitstreamMode.value,
      if (codingMode != null) 'codingMode': codingMode.value,
      if (dcFilter != null) 'dcFilter': dcFilter.value,
      if (dialnorm != null) 'dialnorm': dialnorm,
      if (dynamicRangeCompressionLine != null)
        'dynamicRangeCompressionLine': dynamicRangeCompressionLine.value,
      if (dynamicRangeCompressionRf != null)
        'dynamicRangeCompressionRf': dynamicRangeCompressionRf.value,
      if (lfeControl != null) 'lfeControl': lfeControl.value,
      if (lfeFilter != null) 'lfeFilter': lfeFilter.value,
      if (loRoCenterMixLevel != null) 'loRoCenterMixLevel': loRoCenterMixLevel,
      if (loRoSurroundMixLevel != null)
        'loRoSurroundMixLevel': loRoSurroundMixLevel,
      if (ltRtCenterMixLevel != null) 'ltRtCenterMixLevel': ltRtCenterMixLevel,
      if (ltRtSurroundMixLevel != null)
        'ltRtSurroundMixLevel': ltRtSurroundMixLevel,
      if (metadataControl != null) 'metadataControl': metadataControl.value,
      if (passthroughControl != null)
        'passthroughControl': passthroughControl.value,
      if (phaseControl != null) 'phaseControl': phaseControl.value,
      if (sampleRate != null) 'sampleRate': sampleRate,
      if (stereoDownmix != null) 'stereoDownmix': stereoDownmix.value,
      if (surroundExMode != null) 'surroundExMode': surroundExMode.value,
      if (surroundMode != null) 'surroundMode': surroundMode.value,
    };
  }
}

/// Choose how the service does stereo downmixing. This setting only applies if
/// you keep the default value of 3/2 - L, R, C, Ls, Rs for the setting Coding
/// mode. If you choose a different value for Coding mode, the service ignores
/// Stereo downmix.
enum Eac3StereoDownmix {
  notIndicated('NOT_INDICATED'),
  loRo('LO_RO'),
  ltRt('LT_RT'),
  dpl2('DPL2'),
  ;

  final String value;

  const Eac3StereoDownmix(this.value);

  static Eac3StereoDownmix fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Eac3StereoDownmix'));
}

/// When encoding 3/2 audio, sets whether an extra center back surround channel
/// is matrix encoded into the left and right surround channels.
enum Eac3SurroundExMode {
  notIndicated('NOT_INDICATED'),
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const Eac3SurroundExMode(this.value);

  static Eac3SurroundExMode fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Eac3SurroundExMode'));
}

/// When encoding 2/0 audio, sets whether Dolby Surround is matrix encoded into
/// the two channels.
enum Eac3SurroundMode {
  notIndicated('NOT_INDICATED'),
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const Eac3SurroundMode(this.value);

  static Eac3SurroundMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Eac3SurroundMode'));
}

/// Specify whether this set of input captions appears in your outputs in both
/// 608 and 708 format. If you choose Upconvert, MediaConvert includes the
/// captions data in two ways: it passes the 608 data through using the 608
/// compatibility bytes fields of the 708 wrapper, and it also translates the
/// 608 data into 708.
enum EmbeddedConvert608To708 {
  upconvert('UPCONVERT'),
  disabled('DISABLED'),
  ;

  final String value;

  const EmbeddedConvert608To708(this.value);

  static EmbeddedConvert608To708 fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum EmbeddedConvert608To708'));
}

/// Settings related to CEA/EIA-608 and CEA/EIA-708 (also called embedded or
/// ancillary) captions. Set up embedded captions in the same output as your
/// video. For more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/embedded-output-captions.html.
class EmbeddedDestinationSettings {
  /// Ignore this setting unless your input captions are SCC format and your
  /// output captions are embedded in the video stream. Specify a CC number for
  /// each captions channel in this output. If you have two channels, choose CC
  /// numbers that aren't in the same field. For example, choose 1 and 3. For more
  /// information, see
  /// https://docs.aws.amazon.com/console/mediaconvert/dual-scc-to-embedded.
  final int? destination608ChannelNumber;

  /// Ignore this setting unless your input captions are SCC format and you want
  /// both 608 and 708 captions embedded in your output stream. Optionally,
  /// specify the 708 service number for each output captions channel. Choose a
  /// different number for each channel. To use this setting, also set Force 608
  /// to 708 upconvert to Upconvert in your input captions selector settings. If
  /// you choose to upconvert but don't specify a 708 service number, MediaConvert
  /// uses the number that you specify for CC channel number for the 708 service
  /// number. For more information, see
  /// https://docs.aws.amazon.com/console/mediaconvert/dual-scc-to-embedded.
  final int? destination708ServiceNumber;

  EmbeddedDestinationSettings({
    this.destination608ChannelNumber,
    this.destination708ServiceNumber,
  });

  factory EmbeddedDestinationSettings.fromJson(Map<String, dynamic> json) {
    return EmbeddedDestinationSettings(
      destination608ChannelNumber: json['destination608ChannelNumber'] as int?,
      destination708ServiceNumber: json['destination708ServiceNumber'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final destination608ChannelNumber = this.destination608ChannelNumber;
    final destination708ServiceNumber = this.destination708ServiceNumber;
    return {
      if (destination608ChannelNumber != null)
        'destination608ChannelNumber': destination608ChannelNumber,
      if (destination708ServiceNumber != null)
        'destination708ServiceNumber': destination708ServiceNumber,
    };
  }
}

/// Settings for embedded captions Source
class EmbeddedSourceSettings {
  /// Specify whether this set of input captions appears in your outputs in both
  /// 608 and 708 format. If you choose Upconvert, MediaConvert includes the
  /// captions data in two ways: it passes the 608 data through using the 608
  /// compatibility bytes fields of the 708 wrapper, and it also translates the
  /// 608 data into 708.
  final EmbeddedConvert608To708? convert608To708;

  /// Specifies the 608/708 channel number within the video track from which to
  /// extract captions. Unused for passthrough.
  final int? source608ChannelNumber;

  /// Specifies the video track index used for extracting captions. The system
  /// only supports one input video track, so this should always be set to '1'.
  final int? source608TrackNumber;

  /// By default, the service terminates any unterminated captions at the end of
  /// each input. If you want the caption to continue onto your next input,
  /// disable this setting.
  final EmbeddedTerminateCaptions? terminateCaptions;

  EmbeddedSourceSettings({
    this.convert608To708,
    this.source608ChannelNumber,
    this.source608TrackNumber,
    this.terminateCaptions,
  });

  factory EmbeddedSourceSettings.fromJson(Map<String, dynamic> json) {
    return EmbeddedSourceSettings(
      convert608To708: (json['convert608To708'] as String?)
          ?.let(EmbeddedConvert608To708.fromString),
      source608ChannelNumber: json['source608ChannelNumber'] as int?,
      source608TrackNumber: json['source608TrackNumber'] as int?,
      terminateCaptions: (json['terminateCaptions'] as String?)
          ?.let(EmbeddedTerminateCaptions.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final convert608To708 = this.convert608To708;
    final source608ChannelNumber = this.source608ChannelNumber;
    final source608TrackNumber = this.source608TrackNumber;
    final terminateCaptions = this.terminateCaptions;
    return {
      if (convert608To708 != null) 'convert608To708': convert608To708.value,
      if (source608ChannelNumber != null)
        'source608ChannelNumber': source608ChannelNumber,
      if (source608TrackNumber != null)
        'source608TrackNumber': source608TrackNumber,
      if (terminateCaptions != null)
        'terminateCaptions': terminateCaptions.value,
    };
  }
}

/// By default, the service terminates any unterminated captions at the end of
/// each input. If you want the caption to continue onto your next input,
/// disable this setting.
enum EmbeddedTerminateCaptions {
  endOfInput('END_OF_INPUT'),
  disabled('DISABLED'),
  ;

  final String value;

  const EmbeddedTerminateCaptions(this.value);

  static EmbeddedTerminateCaptions fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum EmbeddedTerminateCaptions'));
}

/// Set Embedded timecode override to Use MDPM when your AVCHD input contains
/// timecode tag data in the Modified Digital Video Pack Metadata. When you do,
/// we recommend you also set Timecode source to Embedded. Leave Embedded
/// timecode override blank, or set to None, when your input does not contain
/// MDPM timecode.
enum EmbeddedTimecodeOverride {
  none('NONE'),
  useMdpm('USE_MDPM'),
  ;

  final String value;

  const EmbeddedTimecodeOverride(this.value);

  static EmbeddedTimecodeOverride fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum EmbeddedTimecodeOverride'));
}

/// Describes an account-specific API endpoint.
class Endpoint {
  /// URL of endpoint
  final String? url;

  Endpoint({
    this.url,
  });

  factory Endpoint.fromJson(Map<String, dynamic> json) {
    return Endpoint(
      url: json['url'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final url = this.url;
    return {
      if (url != null) 'url': url,
    };
  }
}

/// ESAM ManifestConfirmConditionNotification defined by
/// OC-SP-ESAM-API-I03-131025.
class EsamManifestConfirmConditionNotification {
  /// Provide your ESAM ManifestConfirmConditionNotification XML document inside
  /// your JSON job settings. Form the XML document as per
  /// OC-SP-ESAM-API-I03-131025. The transcoder will use the Manifest Conditioning
  /// instructions in the message that you supply.
  final String? mccXml;

  EsamManifestConfirmConditionNotification({
    this.mccXml,
  });

  factory EsamManifestConfirmConditionNotification.fromJson(
      Map<String, dynamic> json) {
    return EsamManifestConfirmConditionNotification(
      mccXml: json['mccXml'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final mccXml = this.mccXml;
    return {
      if (mccXml != null) 'mccXml': mccXml,
    };
  }
}

/// Settings for Event Signaling And Messaging (ESAM). If you don't do ad
/// insertion, you can ignore these settings.
class EsamSettings {
  /// Specifies an ESAM ManifestConfirmConditionNotification XML as per
  /// OC-SP-ESAM-API-I03-131025. The transcoder uses the manifest conditioning
  /// instructions that you provide in the setting MCC XML.
  final EsamManifestConfirmConditionNotification?
      manifestConfirmConditionNotification;

  /// Specifies the stream distance, in milliseconds, between the SCTE 35 messages
  /// that the transcoder places and the splice points that they refer to. If the
  /// time between the start of the asset and the SCTE-35 message is less than
  /// this value, then the transcoder places the SCTE-35 marker at the beginning
  /// of the stream.
  final int? responseSignalPreroll;

  /// Specifies an ESAM SignalProcessingNotification XML as per
  /// OC-SP-ESAM-API-I03-131025. The transcoder uses the signal processing
  /// instructions that you provide in the setting SCC XML.
  final EsamSignalProcessingNotification? signalProcessingNotification;

  EsamSettings({
    this.manifestConfirmConditionNotification,
    this.responseSignalPreroll,
    this.signalProcessingNotification,
  });

  factory EsamSettings.fromJson(Map<String, dynamic> json) {
    return EsamSettings(
      manifestConfirmConditionNotification:
          json['manifestConfirmConditionNotification'] != null
              ? EsamManifestConfirmConditionNotification.fromJson(
                  json['manifestConfirmConditionNotification']
                      as Map<String, dynamic>)
              : null,
      responseSignalPreroll: json['responseSignalPreroll'] as int?,
      signalProcessingNotification: json['signalProcessingNotification'] != null
          ? EsamSignalProcessingNotification.fromJson(
              json['signalProcessingNotification'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final manifestConfirmConditionNotification =
        this.manifestConfirmConditionNotification;
    final responseSignalPreroll = this.responseSignalPreroll;
    final signalProcessingNotification = this.signalProcessingNotification;
    return {
      if (manifestConfirmConditionNotification != null)
        'manifestConfirmConditionNotification':
            manifestConfirmConditionNotification,
      if (responseSignalPreroll != null)
        'responseSignalPreroll': responseSignalPreroll,
      if (signalProcessingNotification != null)
        'signalProcessingNotification': signalProcessingNotification,
    };
  }
}

/// ESAM SignalProcessingNotification data defined by OC-SP-ESAM-API-I03-131025.
class EsamSignalProcessingNotification {
  /// Provide your ESAM SignalProcessingNotification XML document inside your JSON
  /// job settings. Form the XML document as per OC-SP-ESAM-API-I03-131025. The
  /// transcoder will use the signal processing instructions in the message that
  /// you supply. For your MPEG2-TS file outputs, if you want the service to place
  /// SCTE-35 markers at the insertion points you specify in the XML document, you
  /// must also enable SCTE-35 ESAM. Note that you can either specify an ESAM XML
  /// document or enable SCTE-35 passthrough. You can't do both.
  final String? sccXml;

  EsamSignalProcessingNotification({
    this.sccXml,
  });

  factory EsamSignalProcessingNotification.fromJson(Map<String, dynamic> json) {
    return EsamSignalProcessingNotification(
      sccXml: json['sccXml'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final sccXml = this.sccXml;
    return {
      if (sccXml != null) 'sccXml': sccXml,
    };
  }
}

/// If your source content has EIA-608 Line 21 Data Services, enable this
/// feature to specify what MediaConvert does with the Extended Data Services
/// (XDS) packets. You can choose to pass through XDS packets, or remove them
/// from the output. For more information about XDS, see EIA-608 Line Data
/// Services, section 9.5.1.5 05h Content Advisory.
class ExtendedDataServices {
  /// The action to take on copy and redistribution control XDS packets. If you
  /// select PASSTHROUGH, packets will not be changed. If you select STRIP, any
  /// packets will be removed in output captions.
  final CopyProtectionAction? copyProtectionAction;

  /// The action to take on content advisory XDS packets. If you select
  /// PASSTHROUGH, packets will not be changed. If you select STRIP, any packets
  /// will be removed in output captions.
  final VchipAction? vchipAction;

  ExtendedDataServices({
    this.copyProtectionAction,
    this.vchipAction,
  });

  factory ExtendedDataServices.fromJson(Map<String, dynamic> json) {
    return ExtendedDataServices(
      copyProtectionAction: (json['copyProtectionAction'] as String?)
          ?.let(CopyProtectionAction.fromString),
      vchipAction:
          (json['vchipAction'] as String?)?.let(VchipAction.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final copyProtectionAction = this.copyProtectionAction;
    final vchipAction = this.vchipAction;
    return {
      if (copyProtectionAction != null)
        'copyProtectionAction': copyProtectionAction.value,
      if (vchipAction != null) 'vchipAction': vchipAction.value,
    };
  }
}

/// To place the MOOV atom at the beginning of your output, which is useful for
/// progressive downloading: Leave blank or choose Progressive download. To
/// place the MOOV at the end of your output: Choose Normal.
enum F4vMoovPlacement {
  progressiveDownload('PROGRESSIVE_DOWNLOAD'),
  normal('NORMAL'),
  ;

  final String value;

  const F4vMoovPlacement(this.value);

  static F4vMoovPlacement fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum F4vMoovPlacement'));
}

/// Settings for F4v container
class F4vSettings {
  /// To place the MOOV atom at the beginning of your output, which is useful for
  /// progressive downloading: Leave blank or choose Progressive download. To
  /// place the MOOV at the end of your output: Choose Normal.
  final F4vMoovPlacement? moovPlacement;

  F4vSettings({
    this.moovPlacement,
  });

  factory F4vSettings.fromJson(Map<String, dynamic> json) {
    return F4vSettings(
      moovPlacement:
          (json['moovPlacement'] as String?)?.let(F4vMoovPlacement.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final moovPlacement = this.moovPlacement;
    return {
      if (moovPlacement != null) 'moovPlacement': moovPlacement.value,
    };
  }
}

/// Settings related to your File output group. MediaConvert uses this group of
/// settings to generate a single standalone file, rather than a streaming
/// package.
class FileGroupSettings {
  /// Use Destination to specify the S3 output location and the output filename
  /// base. Destination accepts format identifiers. If you do not specify the base
  /// filename in the URI, the service will use the filename of the input file. If
  /// your job has multiple inputs, the service uses the filename of the first
  /// input file.
  final String? destination;

  /// Settings associated with the destination. Will vary based on the type of
  /// destination
  final DestinationSettings? destinationSettings;

  FileGroupSettings({
    this.destination,
    this.destinationSettings,
  });

  factory FileGroupSettings.fromJson(Map<String, dynamic> json) {
    return FileGroupSettings(
      destination: json['destination'] as String?,
      destinationSettings: json['destinationSettings'] != null
          ? DestinationSettings.fromJson(
              json['destinationSettings'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final destination = this.destination;
    final destinationSettings = this.destinationSettings;
    return {
      if (destination != null) 'destination': destination,
      if (destinationSettings != null)
        'destinationSettings': destinationSettings,
    };
  }
}

/// Specify whether this set of input captions appears in your outputs in both
/// 608 and 708 format. If you choose Upconvert, MediaConvert includes the
/// captions data in two ways: it passes the 608 data through using the 608
/// compatibility bytes fields of the 708 wrapper, and it also translates the
/// 608 data into 708.
enum FileSourceConvert608To708 {
  upconvert('UPCONVERT'),
  disabled('DISABLED'),
  ;

  final String value;

  const FileSourceConvert608To708(this.value);

  static FileSourceConvert608To708 fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum FileSourceConvert608To708'));
}

/// If your input captions are SCC, SMI, SRT, STL, TTML, WebVTT, or IMSC 1.1 in
/// an xml file, specify the URI of the input caption source file. If your
/// caption source is IMSC in an IMF package, use TrackSourceSettings instead of
/// FileSoureSettings.
class FileSourceSettings {
  /// Specify whether this set of input captions appears in your outputs in both
  /// 608 and 708 format. If you choose Upconvert, MediaConvert includes the
  /// captions data in two ways: it passes the 608 data through using the 608
  /// compatibility bytes fields of the 708 wrapper, and it also translates the
  /// 608 data into 708.
  final FileSourceConvert608To708? convert608To708;

  /// Choose the presentation style of your input SCC captions. To use the same
  /// presentation style as your input: Keep the default value, Disabled. To
  /// convert paint-on captions to pop-on: Choose Enabled. We also recommend that
  /// you choose Enabled if you notice additional repeated lines in your output
  /// captions.
  final CaptionSourceConvertPaintOnToPopOn? convertPaintToPop;

  /// Ignore this setting unless your input captions format is SCC. To have the
  /// service compensate for differing frame rates between your input captions and
  /// input video, specify the frame rate of the captions file. Specify this value
  /// as a fraction. For example, you might specify 24 / 1 for 24 fps, 25 / 1 for
  /// 25 fps, 24000 / 1001 for 23.976 fps, or 30000 / 1001 for 29.97 fps.
  final CaptionSourceFramerate? framerate;

  /// External caption file used for loading captions. Accepted file extensions
  /// are 'scc', 'ttml', 'dfxp', 'stl', 'srt', 'xml', 'smi', 'webvtt', and 'vtt'.
  final String? sourceFile;

  /// Optional. Use this setting when you need to adjust the sync between your
  /// sidecar captions and your video. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/time-delta-use-cases.html.
  /// Enter a positive or negative number to modify the times in the captions
  /// file. For example, type 15 to add 15 seconds to all the times in the
  /// captions file. Type -5 to subtract 5 seconds from the times in the captions
  /// file. You can optionally specify your time delta in milliseconds instead of
  /// seconds. When you do so, set the related setting, Time delta units to
  /// Milliseconds. Note that, when you specify a time delta for timecode-based
  /// caption sources, such as SCC and STL, and your time delta isn't a multiple
  /// of the input frame rate, MediaConvert snaps the captions to the nearest
  /// frame. For example, when your input video frame rate is 25 fps and you
  /// specify 1010ms for time delta, MediaConvert delays your captions by 1000 ms.
  final int? timeDelta;

  /// When you use the setting Time delta to adjust the sync between your sidecar
  /// captions and your video, use this setting to specify the units for the delta
  /// that you specify. When you don't specify a value for Time delta units,
  /// MediaConvert uses seconds by default.
  final FileSourceTimeDeltaUnits? timeDeltaUnits;

  FileSourceSettings({
    this.convert608To708,
    this.convertPaintToPop,
    this.framerate,
    this.sourceFile,
    this.timeDelta,
    this.timeDeltaUnits,
  });

  factory FileSourceSettings.fromJson(Map<String, dynamic> json) {
    return FileSourceSettings(
      convert608To708: (json['convert608To708'] as String?)
          ?.let(FileSourceConvert608To708.fromString),
      convertPaintToPop: (json['convertPaintToPop'] as String?)
          ?.let(CaptionSourceConvertPaintOnToPopOn.fromString),
      framerate: json['framerate'] != null
          ? CaptionSourceFramerate.fromJson(
              json['framerate'] as Map<String, dynamic>)
          : null,
      sourceFile: json['sourceFile'] as String?,
      timeDelta: json['timeDelta'] as int?,
      timeDeltaUnits: (json['timeDeltaUnits'] as String?)
          ?.let(FileSourceTimeDeltaUnits.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final convert608To708 = this.convert608To708;
    final convertPaintToPop = this.convertPaintToPop;
    final framerate = this.framerate;
    final sourceFile = this.sourceFile;
    final timeDelta = this.timeDelta;
    final timeDeltaUnits = this.timeDeltaUnits;
    return {
      if (convert608To708 != null) 'convert608To708': convert608To708.value,
      if (convertPaintToPop != null)
        'convertPaintToPop': convertPaintToPop.value,
      if (framerate != null) 'framerate': framerate,
      if (sourceFile != null) 'sourceFile': sourceFile,
      if (timeDelta != null) 'timeDelta': timeDelta,
      if (timeDeltaUnits != null) 'timeDeltaUnits': timeDeltaUnits.value,
    };
  }
}

/// When you use the setting Time delta to adjust the sync between your sidecar
/// captions and your video, use this setting to specify the units for the delta
/// that you specify. When you don't specify a value for Time delta units,
/// MediaConvert uses seconds by default.
enum FileSourceTimeDeltaUnits {
  seconds('SECONDS'),
  milliseconds('MILLISECONDS'),
  ;

  final String value;

  const FileSourceTimeDeltaUnits(this.value);

  static FileSourceTimeDeltaUnits fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum FileSourceTimeDeltaUnits'));
}

/// Required when you set Codec, under AudioDescriptions>CodecSettings, to the
/// value FLAC.
class FlacSettings {
  /// Specify Bit depth (BitDepth), in bits per sample, to choose the encoding
  /// quality for this audio track.
  final int? bitDepth;

  /// Specify the number of channels in this output audio track. Choosing Mono on
  /// the console gives you 1 output channel; choosing Stereo gives you 2. In the
  /// API, valid values are between 1 and 8.
  final int? channels;

  /// Sample rate in Hz.
  final int? sampleRate;

  FlacSettings({
    this.bitDepth,
    this.channels,
    this.sampleRate,
  });

  factory FlacSettings.fromJson(Map<String, dynamic> json) {
    return FlacSettings(
      bitDepth: json['bitDepth'] as int?,
      channels: json['channels'] as int?,
      sampleRate: json['sampleRate'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final bitDepth = this.bitDepth;
    final channels = this.channels;
    final sampleRate = this.sampleRate;
    return {
      if (bitDepth != null) 'bitDepth': bitDepth,
      if (channels != null) 'channels': channels,
      if (sampleRate != null) 'sampleRate': sampleRate,
    };
  }
}

/// Provide the font script, using an ISO 15924 script code, if the LanguageCode
/// is not sufficient for determining the script type. Where LanguageCode or
/// CustomLanguageCode is sufficient, use "AUTOMATIC" or leave unset.
enum FontScript {
  automatic('AUTOMATIC'),
  hans('HANS'),
  hant('HANT'),
  ;

  final String value;

  const FontScript(this.value);

  static FontScript fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum FontScript'));
}

/// Use Force include renditions to specify one or more resolutions to include
/// your ABR stack. * (Recommended) To optimize automated ABR, specify as few
/// resolutions as possible. * (Required) The number of resolutions that you
/// specify must be equal to, or less than, the Max renditions setting. * If you
/// specify a Min top rendition size rule, specify at least one resolution that
/// is equal to, or greater than, Min top rendition size. * If you specify a Min
/// bottom rendition size rule, only specify resolutions that are equal to, or
/// greater than, Min bottom rendition size. * If you specify a Force include
/// renditions rule, do not specify a separate rule for Allowed renditions. *
/// Note: The ABR stack may include other resolutions that you do not specify
/// here, depending on the Max renditions setting.
class ForceIncludeRenditionSize {
  /// Use Height to define the video resolution height, in pixels, for this rule.
  final int? height;

  /// Use Width to define the video resolution width, in pixels, for this rule.
  final int? width;

  ForceIncludeRenditionSize({
    this.height,
    this.width,
  });

  factory ForceIncludeRenditionSize.fromJson(Map<String, dynamic> json) {
    return ForceIncludeRenditionSize(
      height: json['height'] as int?,
      width: json['width'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final height = this.height;
    final width = this.width;
    return {
      if (height != null) 'height': height,
      if (width != null) 'width': width,
    };
  }
}

/// Required when you set Codec to the value FRAME_CAPTURE.
class FrameCaptureSettings {
  /// Frame capture will encode the first frame of the output stream, then one
  /// frame every framerateDenominator/framerateNumerator seconds. For example,
  /// settings of framerateNumerator = 1 and framerateDenominator = 3 (a rate of
  /// 1/3 frame per second) will capture the first frame, then 1 frame every 3s.
  /// Files will be named as filename.n.jpg where n is the 0-based sequence number
  /// of each Capture.
  final int? framerateDenominator;

  /// Frame capture will encode the first frame of the output stream, then one
  /// frame every framerateDenominator/framerateNumerator seconds. For example,
  /// settings of framerateNumerator = 1 and framerateDenominator = 3 (a rate of
  /// 1/3 frame per second) will capture the first frame, then 1 frame every 3s.
  /// Files will be named as filename.NNNNNNN.jpg where N is the 0-based frame
  /// sequence number zero padded to 7 decimal places.
  final int? framerateNumerator;

  /// Maximum number of captures (encoded jpg output files).
  final int? maxCaptures;

  /// JPEG Quality - a higher value equals higher quality.
  final int? quality;

  FrameCaptureSettings({
    this.framerateDenominator,
    this.framerateNumerator,
    this.maxCaptures,
    this.quality,
  });

  factory FrameCaptureSettings.fromJson(Map<String, dynamic> json) {
    return FrameCaptureSettings(
      framerateDenominator: json['framerateDenominator'] as int?,
      framerateNumerator: json['framerateNumerator'] as int?,
      maxCaptures: json['maxCaptures'] as int?,
      quality: json['quality'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final framerateDenominator = this.framerateDenominator;
    final framerateNumerator = this.framerateNumerator;
    final maxCaptures = this.maxCaptures;
    final quality = this.quality;
    return {
      if (framerateDenominator != null)
        'framerateDenominator': framerateDenominator,
      if (framerateNumerator != null) 'framerateNumerator': framerateNumerator,
      if (maxCaptures != null) 'maxCaptures': maxCaptures,
      if (quality != null) 'quality': quality,
    };
  }
}

class GetJobResponse {
  /// Each job converts an input file into an output file or files. For more
  /// information, see the User Guide at
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/what-is.html
  final Job? job;

  GetJobResponse({
    this.job,
  });

  factory GetJobResponse.fromJson(Map<String, dynamic> json) {
    return GetJobResponse(
      job: json['job'] != null
          ? Job.fromJson(json['job'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final job = this.job;
    return {
      if (job != null) 'job': job,
    };
  }
}

class GetJobTemplateResponse {
  /// A job template is a pre-made set of encoding instructions that you can use
  /// to quickly create a job.
  final JobTemplate? jobTemplate;

  GetJobTemplateResponse({
    this.jobTemplate,
  });

  factory GetJobTemplateResponse.fromJson(Map<String, dynamic> json) {
    return GetJobTemplateResponse(
      jobTemplate: json['jobTemplate'] != null
          ? JobTemplate.fromJson(json['jobTemplate'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final jobTemplate = this.jobTemplate;
    return {
      if (jobTemplate != null) 'jobTemplate': jobTemplate,
    };
  }
}

class GetPolicyResponse {
  /// A policy configures behavior that you allow or disallow for your account.
  /// For information about MediaConvert policies, see the user guide at
  /// http://docs.aws.amazon.com/mediaconvert/latest/ug/what-is.html
  final Policy? policy;

  GetPolicyResponse({
    this.policy,
  });

  factory GetPolicyResponse.fromJson(Map<String, dynamic> json) {
    return GetPolicyResponse(
      policy: json['policy'] != null
          ? Policy.fromJson(json['policy'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final policy = this.policy;
    return {
      if (policy != null) 'policy': policy,
    };
  }
}

class GetPresetResponse {
  /// A preset is a collection of preconfigured media conversion settings that you
  /// want MediaConvert to apply to the output during the conversion process.
  final Preset? preset;

  GetPresetResponse({
    this.preset,
  });

  factory GetPresetResponse.fromJson(Map<String, dynamic> json) {
    return GetPresetResponse(
      preset: json['preset'] != null
          ? Preset.fromJson(json['preset'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final preset = this.preset;
    return {
      if (preset != null) 'preset': preset,
    };
  }
}

class GetQueueResponse {
  /// You can use queues to manage the resources that are available to your AWS
  /// account for running multiple transcoding jobs at the same time. If you don't
  /// specify a queue, the service sends all jobs through the default queue. For
  /// more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/working-with-queues.html.
  final Queue? queue;

  GetQueueResponse({
    this.queue,
  });

  factory GetQueueResponse.fromJson(Map<String, dynamic> json) {
    return GetQueueResponse(
      queue: json['queue'] != null
          ? Queue.fromJson(json['queue'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final queue = this.queue;
    return {
      if (queue != null) 'queue': queue,
    };
  }
}

/// Keep the default value, Auto, for this setting to have MediaConvert
/// automatically apply the best types of quantization for your video content.
/// When you want to apply your quantization settings manually, you must set
/// H264AdaptiveQuantization to a value other than Auto. Use this setting to
/// specify the strength of any adaptive quantization filters that you enable.
/// If you don't want MediaConvert to do any adaptive quantization in this
/// transcode, set Adaptive quantization to Off. Related settings: The value
/// that you choose here applies to the following settings:
/// H264FlickerAdaptiveQuantization, H264SpatialAdaptiveQuantization, and
/// H264TemporalAdaptiveQuantization.
enum H264AdaptiveQuantization {
  off('OFF'),
  auto('AUTO'),
  low('LOW'),
  medium('MEDIUM'),
  high('HIGH'),
  higher('HIGHER'),
  max('MAX'),
  ;

  final String value;

  const H264AdaptiveQuantization(this.value);

  static H264AdaptiveQuantization fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H264AdaptiveQuantization'));
}

/// Specify an H.264 level that is consistent with your output video settings.
/// If you aren't sure what level to specify, choose Auto.
enum H264CodecLevel {
  auto('AUTO'),
  level_1('LEVEL_1'),
  level_1_1('LEVEL_1_1'),
  level_1_2('LEVEL_1_2'),
  level_1_3('LEVEL_1_3'),
  level_2('LEVEL_2'),
  level_2_1('LEVEL_2_1'),
  level_2_2('LEVEL_2_2'),
  level_3('LEVEL_3'),
  level_3_1('LEVEL_3_1'),
  level_3_2('LEVEL_3_2'),
  level_4('LEVEL_4'),
  level_4_1('LEVEL_4_1'),
  level_4_2('LEVEL_4_2'),
  level_5('LEVEL_5'),
  level_5_1('LEVEL_5_1'),
  level_5_2('LEVEL_5_2'),
  ;

  final String value;

  const H264CodecLevel(this.value);

  static H264CodecLevel fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H264CodecLevel'));
}

/// H.264 Profile. High 4:2:2 and 10-bit profiles are only available with the
/// AVC-I License.
enum H264CodecProfile {
  baseline('BASELINE'),
  high('HIGH'),
  high_10bit('HIGH_10BIT'),
  high_422('HIGH_422'),
  high_422_10bit('HIGH_422_10BIT'),
  main('MAIN'),
  ;

  final String value;

  const H264CodecProfile(this.value);

  static H264CodecProfile fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H264CodecProfile'));
}

/// Choose Adaptive to improve subjective video quality for high-motion content.
/// This will cause the service to use fewer B-frames (which infer information
/// based on other frames) for high-motion portions of the video and more
/// B-frames for low-motion portions. The maximum number of B-frames is limited
/// by the value you provide for the setting B frames between reference frames.
enum H264DynamicSubGop {
  adaptive('ADAPTIVE'),
  static('STATIC'),
  ;

  final String value;

  const H264DynamicSubGop(this.value);

  static H264DynamicSubGop fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H264DynamicSubGop'));
}

/// Optionally include or suppress markers at the end of your output that signal
/// the end of the video stream. To include end of stream markers: Leave blank
/// or keep the default value, Include. To not include end of stream markers:
/// Choose Suppress. This is useful when your output will be inserted into
/// another stream.
enum H264EndOfStreamMarkers {
  include('INCLUDE'),
  suppress('SUPPRESS'),
  ;

  final String value;

  const H264EndOfStreamMarkers(this.value);

  static H264EndOfStreamMarkers fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H264EndOfStreamMarkers'));
}

/// Entropy encoding mode. Use CABAC (must be in Main or High profile) or CAVLC.
enum H264EntropyEncoding {
  cabac('CABAC'),
  cavlc('CAVLC'),
  ;

  final String value;

  const H264EntropyEncoding(this.value);

  static H264EntropyEncoding fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum H264EntropyEncoding'));
}

/// The video encoding method for your MPEG-4 AVC output. Keep the default
/// value, PAFF, to have MediaConvert use PAFF encoding for interlaced outputs.
/// Choose Force field to disable PAFF encoding and create separate interlaced
/// fields. Choose MBAFF to disable PAFF and have MediaConvert use MBAFF
/// encoding for interlaced outputs.
enum H264FieldEncoding {
  paff('PAFF'),
  forceField('FORCE_FIELD'),
  mbaff('MBAFF'),
  ;

  final String value;

  const H264FieldEncoding(this.value);

  static H264FieldEncoding fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H264FieldEncoding'));
}

/// Only use this setting when you change the default value, AUTO, for the
/// setting H264AdaptiveQuantization. When you keep all defaults, excluding
/// H264AdaptiveQuantization and all other adaptive quantization from your JSON
/// job specification, MediaConvert automatically applies the best types of
/// quantization for your video content. When you set H264AdaptiveQuantization
/// to a value other than AUTO, the default value for
/// H264FlickerAdaptiveQuantization is Disabled. Change this value to Enabled to
/// reduce I-frame pop. I-frame pop appears as a visual flicker that can arise
/// when the encoder saves bits by copying some macroblocks many times from
/// frame to frame, and then refreshes them at the I-frame. When you enable this
/// setting, the encoder updates these macroblocks slightly more often to smooth
/// out the flicker. To manually enable or disable
/// H264FlickerAdaptiveQuantization, you must set Adaptive quantization to a
/// value other than AUTO.
enum H264FlickerAdaptiveQuantization {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const H264FlickerAdaptiveQuantization(this.value);

  static H264FlickerAdaptiveQuantization fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H264FlickerAdaptiveQuantization'));
}

/// If you are using the console, use the Framerate setting to specify the frame
/// rate for this output. If you want to keep the same frame rate as the input
/// video, choose Follow source. If you want to do frame rate conversion, choose
/// a frame rate from the dropdown list or choose Custom. The framerates shown
/// in the dropdown list are decimal approximations of fractions. If you choose
/// Custom, specify your frame rate as a fraction.
enum H264FramerateControl {
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  specified('SPECIFIED'),
  ;

  final String value;

  const H264FramerateControl(this.value);

  static H264FramerateControl fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum H264FramerateControl'));
}

/// Choose the method that you want MediaConvert to use when increasing or
/// decreasing the frame rate. For numerically simple conversions, such as 60
/// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
/// For numerically complex conversions, to avoid stutter: Choose Interpolate.
/// This results in a smooth picture, but might introduce undesirable video
/// artifacts. For complex frame rate conversions, especially if your source
/// video has already been converted from its original cadence: Choose
/// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
/// best conversion method frame by frame. Note that using FrameFormer increases
/// the transcoding time and incurs a significant add-on cost. When you choose
/// FrameFormer, your input video resolution must be at least 128x96.
enum H264FramerateConversionAlgorithm {
  duplicateDrop('DUPLICATE_DROP'),
  interpolate('INTERPOLATE'),
  frameformer('FRAMEFORMER'),
  ;

  final String value;

  const H264FramerateConversionAlgorithm(this.value);

  static H264FramerateConversionAlgorithm fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H264FramerateConversionAlgorithm'));
}

/// Specify whether to allow B-frames to be referenced by other frame types. To
/// use reference B-frames when your GOP structure has 1 or more B-frames: Leave
/// blank or keep the default value Enabled. We recommend that you choose
/// Enabled to help improve the video quality of your output relative to its
/// bitrate. To not use reference B-frames: Choose Disabled.
enum H264GopBReference {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const H264GopBReference(this.value);

  static H264GopBReference fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H264GopBReference'));
}

/// Specify how the transcoder determines GOP size for this output. We recommend
/// that you have the transcoder automatically choose this value for you based
/// on characteristics of your input video. To enable this automatic behavior,
/// choose Auto and and leave GOP size blank. By default, if you don't specify
/// GOP mode control, MediaConvert will use automatic behavior. If your output
/// group specifies HLS, DASH, or CMAF, set GOP mode control to Auto and leave
/// GOP size blank in each output in your output group. To explicitly specify
/// the GOP length, choose Specified, frames or Specified, seconds and then
/// provide the GOP length in the related setting GOP size.
enum H264GopSizeUnits {
  frames('FRAMES'),
  seconds('SECONDS'),
  auto('AUTO'),
  ;

  final String value;

  const H264GopSizeUnits(this.value);

  static H264GopSizeUnits fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H264GopSizeUnits'));
}

/// Choose the scan line type for the output. Keep the default value,
/// Progressive to create a progressive output, regardless of the scan type of
/// your input. Use Top field first or Bottom field first to create an output
/// that's interlaced with the same field polarity throughout. Use Follow,
/// default top or Follow, default bottom to produce outputs with the same field
/// polarity as the source. For jobs that have multiple inputs, the output field
/// polarity might change over the course of the output. Follow behavior depends
/// on the input scan type. If the source is interlaced, the output will be
/// interlaced with the same polarity as the source. If the source is
/// progressive, the output will be interlaced with top field bottom field
/// first, depending on which of the Follow options you choose.
enum H264InterlaceMode {
  progressive('PROGRESSIVE'),
  topField('TOP_FIELD'),
  bottomField('BOTTOM_FIELD'),
  followTopField('FOLLOW_TOP_FIELD'),
  followBottomField('FOLLOW_BOTTOM_FIELD'),
  ;

  final String value;

  const H264InterlaceMode(this.value);

  static H264InterlaceMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H264InterlaceMode'));
}

/// Optional. Specify how the service determines the pixel aspect ratio (PAR)
/// for this output. The default behavior, Follow source, uses the PAR from your
/// input video for your output. To specify a different PAR in the console,
/// choose any value other than Follow source. When you choose SPECIFIED for
/// this setting, you must also specify values for the parNumerator and
/// parDenominator settings.
enum H264ParControl {
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  specified('SPECIFIED'),
  ;

  final String value;

  const H264ParControl(this.value);

  static H264ParControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H264ParControl'));
}

/// The Quality tuning level you choose represents a trade-off between the
/// encoding speed of your job and the output video quality. For the fastest
/// encoding speed at the cost of video quality: Choose Single pass. For a good
/// balance between encoding speed and video quality: Leave blank or keep the
/// default value Single pass HQ. For the best video quality, at the cost of
/// encoding speed: Choose Multi pass HQ. MediaConvert performs an analysis pass
/// on your input followed by an encoding pass. Outputs that use this feature
/// incur pro-tier pricing.
enum H264QualityTuningLevel {
  singlePass('SINGLE_PASS'),
  singlePassHq('SINGLE_PASS_HQ'),
  multiPassHq('MULTI_PASS_HQ'),
  ;

  final String value;

  const H264QualityTuningLevel(this.value);

  static H264QualityTuningLevel fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H264QualityTuningLevel'));
}

/// Settings for quality-defined variable bitrate encoding with the H.264 codec.
/// Use these settings only when you set QVBR for Rate control mode.
class H264QvbrSettings {
  /// Use this setting only when Rate control mode is QVBR and Quality tuning
  /// level is Multi-pass HQ. For Max average bitrate values suited to the
  /// complexity of your input video, the service limits the average bitrate of
  /// the video part of this output to the value that you choose. That is, the
  /// total size of the video element is less than or equal to the value you set
  /// multiplied by the number of seconds of encoded output.
  final int? maxAverageBitrate;

  /// Use this setting only when you set Rate control mode to QVBR. Specify the
  /// target quality level for this output. MediaConvert determines the right
  /// number of bits to use for each part of the video to maintain the video
  /// quality that you specify. When you keep the default value, AUTO,
  /// MediaConvert picks a quality level for you, based on characteristics of your
  /// input video. If you prefer to specify a quality level, specify a number from
  /// 1 through 10. Use higher numbers for greater quality. Level 10 results in
  /// nearly lossless compression. The quality level for most broadcast-quality
  /// transcodes is between 6 and 9. Optionally, to specify a value between whole
  /// numbers, also provide a value for the setting qvbrQualityLevelFineTune. For
  /// example, if you want your QVBR quality level to be 7.33, set
  /// qvbrQualityLevel to 7 and set qvbrQualityLevelFineTune to .33.
  final int? qvbrQualityLevel;

  /// Optional. Specify a value here to set the QVBR quality to a level that is
  /// between whole numbers. For example, if you want your QVBR quality level to
  /// be 7.33, set qvbrQualityLevel to 7 and set qvbrQualityLevelFineTune to .33.
  /// MediaConvert rounds your QVBR quality level to the nearest third of a whole
  /// number. For example, if you set qvbrQualityLevel to 7 and you set
  /// qvbrQualityLevelFineTune to .25, your actual QVBR quality level is 7.33.
  final double? qvbrQualityLevelFineTune;

  H264QvbrSettings({
    this.maxAverageBitrate,
    this.qvbrQualityLevel,
    this.qvbrQualityLevelFineTune,
  });

  factory H264QvbrSettings.fromJson(Map<String, dynamic> json) {
    return H264QvbrSettings(
      maxAverageBitrate: json['maxAverageBitrate'] as int?,
      qvbrQualityLevel: json['qvbrQualityLevel'] as int?,
      qvbrQualityLevelFineTune: json['qvbrQualityLevelFineTune'] as double?,
    );
  }

  Map<String, dynamic> toJson() {
    final maxAverageBitrate = this.maxAverageBitrate;
    final qvbrQualityLevel = this.qvbrQualityLevel;
    final qvbrQualityLevelFineTune = this.qvbrQualityLevelFineTune;
    return {
      if (maxAverageBitrate != null) 'maxAverageBitrate': maxAverageBitrate,
      if (qvbrQualityLevel != null) 'qvbrQualityLevel': qvbrQualityLevel,
      if (qvbrQualityLevelFineTune != null)
        'qvbrQualityLevelFineTune': qvbrQualityLevelFineTune,
    };
  }
}

/// Use this setting to specify whether this output has a variable bitrate
/// (VBR), constant bitrate (CBR) or quality-defined variable bitrate (QVBR).
enum H264RateControlMode {
  vbr('VBR'),
  cbr('CBR'),
  qvbr('QVBR'),
  ;

  final String value;

  const H264RateControlMode(this.value);

  static H264RateControlMode fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum H264RateControlMode'));
}

/// Places a PPS header on each encoded picture, even if repeated.
enum H264RepeatPps {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const H264RepeatPps(this.value);

  static H264RepeatPps fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H264RepeatPps'));
}

/// Use this setting for interlaced outputs, when your output frame rate is half
/// of your input frame rate. In this situation, choose Optimized interlacing to
/// create a better quality interlaced output. In this case, each progressive
/// frame from the input corresponds to an interlaced field in the output. Keep
/// the default value, Basic interlacing, for all other output frame rates. With
/// basic interlacing, MediaConvert performs any frame rate conversion first and
/// then interlaces the frames. When you choose Optimized interlacing and you
/// set your output frame rate to a value that isn't suitable for optimized
/// interlacing, MediaConvert automatically falls back to basic interlacing.
/// Required settings: To use optimized interlacing, you must set Telecine to
/// None or Soft. You can't use optimized interlacing for hard telecine outputs.
/// You must also set Interlace mode to a value other than Progressive.
enum H264ScanTypeConversionMode {
  interlaced('INTERLACED'),
  interlacedOptimize('INTERLACED_OPTIMIZE'),
  ;

  final String value;

  const H264ScanTypeConversionMode(this.value);

  static H264ScanTypeConversionMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H264ScanTypeConversionMode'));
}

/// Enable this setting to insert I-frames at scene changes that the service
/// automatically detects. This improves video quality and is enabled by
/// default. If this output uses QVBR, choose Transition detection for further
/// video quality improvement. For more information about QVBR, see
/// https://docs.aws.amazon.com/console/mediaconvert/cbr-vbr-qvbr.
enum H264SceneChangeDetect {
  disabled('DISABLED'),
  enabled('ENABLED'),
  transitionDetection('TRANSITION_DETECTION'),
  ;

  final String value;

  const H264SceneChangeDetect(this.value);

  static H264SceneChangeDetect fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum H264SceneChangeDetect'));
}

/// Required when you set Codec to the value H_264.
class H264Settings {
  /// Keep the default value, Auto, for this setting to have MediaConvert
  /// automatically apply the best types of quantization for your video content.
  /// When you want to apply your quantization settings manually, you must set
  /// H264AdaptiveQuantization to a value other than Auto. Use this setting to
  /// specify the strength of any adaptive quantization filters that you enable.
  /// If you don't want MediaConvert to do any adaptive quantization in this
  /// transcode, set Adaptive quantization to Off. Related settings: The value
  /// that you choose here applies to the following settings:
  /// H264FlickerAdaptiveQuantization, H264SpatialAdaptiveQuantization, and
  /// H264TemporalAdaptiveQuantization.
  final H264AdaptiveQuantization? adaptiveQuantization;

  /// The Bandwidth reduction filter increases the video quality of your output
  /// relative to its bitrate. Use to lower the bitrate of your constant quality
  /// QVBR output, with little or no perceptual decrease in quality. Or, use to
  /// increase the video quality of outputs with other rate control modes relative
  /// to the bitrate that you specify. Bandwidth reduction increases further when
  /// your input is low quality or noisy. Outputs that use this feature incur
  /// pro-tier pricing. When you include Bandwidth reduction filter, you cannot
  /// include the Noise reducer preprocessor.
  final BandwidthReductionFilter? bandwidthReductionFilter;

  /// Specify the average bitrate in bits per second. Required for VBR and CBR.
  /// For MS Smooth outputs, bitrates must be unique when rounded down to the
  /// nearest multiple of 1000.
  final int? bitrate;

  /// Specify an H.264 level that is consistent with your output video settings.
  /// If you aren't sure what level to specify, choose Auto.
  final H264CodecLevel? codecLevel;

  /// H.264 Profile. High 4:2:2 and 10-bit profiles are only available with the
  /// AVC-I License.
  final H264CodecProfile? codecProfile;

  /// Specify whether to allow the number of B-frames in your output GOP structure
  /// to vary or not depending on your input video content. To improve the
  /// subjective video quality of your output that has high-motion content: Leave
  /// blank or keep the default value Adaptive. MediaConvert will use fewer
  /// B-frames for high-motion video content than low-motion content. The maximum
  /// number of B- frames is limited by the value that you choose for B-frames
  /// between reference frames. To use the same number B-frames for all types of
  /// content: Choose Static.
  final H264DynamicSubGop? dynamicSubGop;

  /// Optionally include or suppress markers at the end of your output that signal
  /// the end of the video stream. To include end of stream markers: Leave blank
  /// or keep the default value, Include. To not include end of stream markers:
  /// Choose Suppress. This is useful when your output will be inserted into
  /// another stream.
  final H264EndOfStreamMarkers? endOfStreamMarkers;

  /// Entropy encoding mode. Use CABAC (must be in Main or High profile) or CAVLC.
  final H264EntropyEncoding? entropyEncoding;

  /// The video encoding method for your MPEG-4 AVC output. Keep the default
  /// value, PAFF, to have MediaConvert use PAFF encoding for interlaced outputs.
  /// Choose Force field to disable PAFF encoding and create separate interlaced
  /// fields. Choose MBAFF to disable PAFF and have MediaConvert use MBAFF
  /// encoding for interlaced outputs.
  final H264FieldEncoding? fieldEncoding;

  /// Only use this setting when you change the default value, AUTO, for the
  /// setting H264AdaptiveQuantization. When you keep all defaults, excluding
  /// H264AdaptiveQuantization and all other adaptive quantization from your JSON
  /// job specification, MediaConvert automatically applies the best types of
  /// quantization for your video content. When you set H264AdaptiveQuantization
  /// to a value other than AUTO, the default value for
  /// H264FlickerAdaptiveQuantization is Disabled. Change this value to Enabled to
  /// reduce I-frame pop. I-frame pop appears as a visual flicker that can arise
  /// when the encoder saves bits by copying some macroblocks many times from
  /// frame to frame, and then refreshes them at the I-frame. When you enable this
  /// setting, the encoder updates these macroblocks slightly more often to smooth
  /// out the flicker. To manually enable or disable
  /// H264FlickerAdaptiveQuantization, you must set Adaptive quantization to a
  /// value other than AUTO.
  final H264FlickerAdaptiveQuantization? flickerAdaptiveQuantization;

  /// If you are using the console, use the Framerate setting to specify the frame
  /// rate for this output. If you want to keep the same frame rate as the input
  /// video, choose Follow source. If you want to do frame rate conversion, choose
  /// a frame rate from the dropdown list or choose Custom. The framerates shown
  /// in the dropdown list are decimal approximations of fractions. If you choose
  /// Custom, specify your frame rate as a fraction.
  final H264FramerateControl? framerateControl;

  /// Choose the method that you want MediaConvert to use when increasing or
  /// decreasing the frame rate. For numerically simple conversions, such as 60
  /// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
  /// For numerically complex conversions, to avoid stutter: Choose Interpolate.
  /// This results in a smooth picture, but might introduce undesirable video
  /// artifacts. For complex frame rate conversions, especially if your source
  /// video has already been converted from its original cadence: Choose
  /// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
  /// best conversion method frame by frame. Note that using FrameFormer increases
  /// the transcoding time and incurs a significant add-on cost. When you choose
  /// FrameFormer, your input video resolution must be at least 128x96.
  final H264FramerateConversionAlgorithm? framerateConversionAlgorithm;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateDenominator to specify the denominator of this fraction.
  /// In this example, use 1001 for the value of FramerateDenominator. When you
  /// use the console for transcode jobs that use frame rate conversion, provide
  /// the value as a decimal number for Framerate. In this example, specify
  /// 23.976.
  final int? framerateDenominator;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateNumerator to specify the numerator of this fraction. In
  /// this example, use 24000 for the value of FramerateNumerator. When you use
  /// the console for transcode jobs that use frame rate conversion, provide the
  /// value as a decimal number for Framerate. In this example, specify 23.976.
  final int? framerateNumerator;

  /// Specify whether to allow B-frames to be referenced by other frame types. To
  /// use reference B-frames when your GOP structure has 1 or more B-frames: Leave
  /// blank or keep the default value Enabled. We recommend that you choose
  /// Enabled to help improve the video quality of your output relative to its
  /// bitrate. To not use reference B-frames: Choose Disabled.
  final H264GopBReference? gopBReference;

  /// Specify the relative frequency of open to closed GOPs in this output. For
  /// example, if you want to allow four open GOPs and then require a closed GOP,
  /// set this value to 5. We recommend that you have the transcoder automatically
  /// choose this value for you based on characteristics of your input video. In
  /// the console, do this by keeping the default empty value. If you do
  /// explicitly specify a value, for segmented outputs, don't set this value to
  /// 0.
  final int? gopClosedCadence;

  /// Use this setting only when you set GOP mode control to Specified, frames or
  /// Specified, seconds. Specify the GOP length using a whole number of frames or
  /// a decimal value of seconds. MediaConvert will interpret this value as frames
  /// or seconds depending on the value you choose for GOP mode control. If you
  /// want to allow MediaConvert to automatically determine GOP size, leave GOP
  /// size blank and set GOP mode control to Auto. If your output group specifies
  /// HLS, DASH, or CMAF, leave GOP size blank and set GOP mode control to Auto in
  /// each output in your output group.
  final double? gopSize;

  /// Specify how the transcoder determines GOP size for this output. We recommend
  /// that you have the transcoder automatically choose this value for you based
  /// on characteristics of your input video. To enable this automatic behavior,
  /// choose Auto and and leave GOP size blank. By default, if you don't specify
  /// GOP mode control, MediaConvert will use automatic behavior. If your output
  /// group specifies HLS, DASH, or CMAF, set GOP mode control to Auto and leave
  /// GOP size blank in each output in your output group. To explicitly specify
  /// the GOP length, choose Specified, frames or Specified, seconds and then
  /// provide the GOP length in the related setting GOP size.
  final H264GopSizeUnits? gopSizeUnits;

  /// If your downstream systems have strict buffer requirements: Specify the
  /// minimum percentage of the HRD buffer that's available at the end of each
  /// encoded video segment. For the best video quality: Set to 0 or leave blank
  /// to automatically determine the final buffer fill percentage.
  final int? hrdBufferFinalFillPercentage;

  /// Percentage of the buffer that should initially be filled (HRD buffer model).
  final int? hrdBufferInitialFillPercentage;

  /// Size of buffer (HRD buffer model) in bits. For example, enter five megabits
  /// as 5000000.
  final int? hrdBufferSize;

  /// Choose the scan line type for the output. Keep the default value,
  /// Progressive to create a progressive output, regardless of the scan type of
  /// your input. Use Top field first or Bottom field first to create an output
  /// that's interlaced with the same field polarity throughout. Use Follow,
  /// default top or Follow, default bottom to produce outputs with the same field
  /// polarity as the source. For jobs that have multiple inputs, the output field
  /// polarity might change over the course of the output. Follow behavior depends
  /// on the input scan type. If the source is interlaced, the output will be
  /// interlaced with the same polarity as the source. If the source is
  /// progressive, the output will be interlaced with top field bottom field
  /// first, depending on which of the Follow options you choose.
  final H264InterlaceMode? interlaceMode;

  /// Maximum bitrate in bits/second. For example, enter five megabits per second
  /// as 5000000. Required when Rate control mode is QVBR.
  final int? maxBitrate;

  /// Use this setting only when you also enable Scene change detection. This
  /// setting determines how the encoder manages the spacing between I-frames that
  /// it inserts as part of the I-frame cadence and the I-frames that it inserts
  /// for Scene change detection. We recommend that you have the transcoder
  /// automatically choose this value for you based on characteristics of your
  /// input video. To enable this automatic behavior, do this by keeping the
  /// default empty value. When you explicitly specify a value for this setting,
  /// the encoder determines whether to skip a cadence-driven I-frame by the value
  /// you set. For example, if you set Min I interval to 5 and a cadence-driven
  /// I-frame would fall within 5 frames of a scene-change I-frame, then the
  /// encoder skips the cadence-driven I-frame. In this way, one GOP is shrunk
  /// slightly and one GOP is stretched slightly. When the cadence-driven I-frames
  /// are farther from the scene-change I-frame than the value you set, then the
  /// encoder leaves all I-frames in place and the GOPs surrounding the scene
  /// change are smaller than the usual cadence GOPs.
  final int? minIInterval;

  /// Specify the number of B-frames between reference frames in this output. For
  /// the best video quality: Leave blank. MediaConvert automatically determines
  /// the number of B-frames to use based on the characteristics of your input
  /// video. To manually specify the number of B-frames between reference frames:
  /// Enter an integer from 0 to 7.
  final int? numberBFramesBetweenReferenceFrames;

  /// Number of reference frames to use. The encoder may use more than requested
  /// if using B-frames and/or interlaced encoding.
  final int? numberReferenceFrames;

  /// Optional. Specify how the service determines the pixel aspect ratio (PAR)
  /// for this output. The default behavior, Follow source, uses the PAR from your
  /// input video for your output. To specify a different PAR in the console,
  /// choose any value other than Follow source. When you choose SPECIFIED for
  /// this setting, you must also specify values for the parNumerator and
  /// parDenominator settings.
  final H264ParControl? parControl;

  /// Required when you set Pixel aspect ratio to SPECIFIED. On the console, this
  /// corresponds to any value other than Follow source. When you specify an
  /// output pixel aspect ratio (PAR) that is different from your input video PAR,
  /// provide your output PAR as a ratio. For example, for D1/DV NTSC widescreen,
  /// you would specify the ratio 40:33. In this example, the value for
  /// parDenominator is 33.
  final int? parDenominator;

  /// Required when you set Pixel aspect ratio to SPECIFIED. On the console, this
  /// corresponds to any value other than Follow source. When you specify an
  /// output pixel aspect ratio (PAR) that is different from your input video PAR,
  /// provide your output PAR as a ratio. For example, for D1/DV NTSC widescreen,
  /// you would specify the ratio 40:33. In this example, the value for
  /// parNumerator is 40.
  final int? parNumerator;

  /// The Quality tuning level you choose represents a trade-off between the
  /// encoding speed of your job and the output video quality. For the fastest
  /// encoding speed at the cost of video quality: Choose Single pass. For a good
  /// balance between encoding speed and video quality: Leave blank or keep the
  /// default value Single pass HQ. For the best video quality, at the cost of
  /// encoding speed: Choose Multi pass HQ. MediaConvert performs an analysis pass
  /// on your input followed by an encoding pass. Outputs that use this feature
  /// incur pro-tier pricing.
  final H264QualityTuningLevel? qualityTuningLevel;

  /// Settings for quality-defined variable bitrate encoding with the H.265 codec.
  /// Use these settings only when you set QVBR for Rate control mode.
  final H264QvbrSettings? qvbrSettings;

  /// Use this setting to specify whether this output has a variable bitrate
  /// (VBR), constant bitrate (CBR) or quality-defined variable bitrate (QVBR).
  final H264RateControlMode? rateControlMode;

  /// Places a PPS header on each encoded picture, even if repeated.
  final H264RepeatPps? repeatPps;

  /// Use this setting for interlaced outputs, when your output frame rate is half
  /// of your input frame rate. In this situation, choose Optimized interlacing to
  /// create a better quality interlaced output. In this case, each progressive
  /// frame from the input corresponds to an interlaced field in the output. Keep
  /// the default value, Basic interlacing, for all other output frame rates. With
  /// basic interlacing, MediaConvert performs any frame rate conversion first and
  /// then interlaces the frames. When you choose Optimized interlacing and you
  /// set your output frame rate to a value that isn't suitable for optimized
  /// interlacing, MediaConvert automatically falls back to basic interlacing.
  /// Required settings: To use optimized interlacing, you must set Telecine to
  /// None or Soft. You can't use optimized interlacing for hard telecine outputs.
  /// You must also set Interlace mode to a value other than Progressive.
  final H264ScanTypeConversionMode? scanTypeConversionMode;

  /// Enable this setting to insert I-frames at scene changes that the service
  /// automatically detects. This improves video quality and is enabled by
  /// default. If this output uses QVBR, choose Transition detection for further
  /// video quality improvement. For more information about QVBR, see
  /// https://docs.aws.amazon.com/console/mediaconvert/cbr-vbr-qvbr.
  final H264SceneChangeDetect? sceneChangeDetect;

  /// Number of slices per picture. Must be less than or equal to the number of
  /// macroblock rows for progressive pictures, and less than or equal to half the
  /// number of macroblock rows for interlaced pictures.
  final int? slices;

  /// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
  /// second (fps). Enable slow PAL to create a 25 fps output. When you enable
  /// slow PAL, MediaConvert relabels the video frames to 25 fps and resamples
  /// your audio to keep it synchronized with the video. Note that enabling this
  /// setting will slightly reduce the duration of your video. Required settings:
  /// You must also set Framerate to 25.
  final H264SlowPal? slowPal;

  /// Ignore this setting unless you need to comply with a specification that
  /// requires a specific value. If you don't have a specification requirement, we
  /// recommend that you adjust the softness of your output by using a lower value
  /// for the setting Sharpness or by enabling a noise reducer filter. The
  /// Softness setting specifies the quantization matrices that the encoder uses.
  /// Keep the default value, 0, for flat quantization. Choose the value 1 or 16
  /// to use the default JVT softening quantization matricies from the H.264
  /// specification. Choose a value from 17 to 128 to use planar interpolation.
  /// Increasing values from 17 to 128 result in increasing reduction of
  /// high-frequency data. The value 128 results in the softest video.
  final int? softness;

  /// Only use this setting when you change the default value, Auto, for the
  /// setting H264AdaptiveQuantization. When you keep all defaults, excluding
  /// H264AdaptiveQuantization and all other adaptive quantization from your JSON
  /// job specification, MediaConvert automatically applies the best types of
  /// quantization for your video content. When you set H264AdaptiveQuantization
  /// to a value other than AUTO, the default value for
  /// H264SpatialAdaptiveQuantization is Enabled. Keep this default value to
  /// adjust quantization within each frame based on spatial variation of content
  /// complexity. When you enable this feature, the encoder uses fewer bits on
  /// areas that can sustain more distortion with no noticeable visual degradation
  /// and uses more bits on areas where any small distortion will be noticeable.
  /// For example, complex textured blocks are encoded with fewer bits and smooth
  /// textured blocks are encoded with more bits. Enabling this feature will
  /// almost always improve your video quality. Note, though, that this feature
  /// doesn't take into account where the viewer's attention is likely to be. If
  /// viewers are likely to be focusing their attention on a part of the screen
  /// with a lot of complex texture, you might choose to set
  /// H264SpatialAdaptiveQuantization to Disabled. Related setting: When you
  /// enable spatial adaptive quantization, set the value for Adaptive
  /// quantization depending on your content. For homogeneous content, such as
  /// cartoons and video games, set it to Low. For content with a wider variety of
  /// textures, set it to High or Higher. To manually enable or disable
  /// H264SpatialAdaptiveQuantization, you must set Adaptive quantization to a
  /// value other than AUTO.
  final H264SpatialAdaptiveQuantization? spatialAdaptiveQuantization;

  /// Produces a bitstream compliant with SMPTE RP-2027.
  final H264Syntax? syntax;

  /// When you do frame rate conversion from 23.976 frames per second (fps) to
  /// 29.97 fps, and your output scan type is interlaced, you can optionally
  /// enable hard or soft telecine to create a smoother picture. Hard telecine
  /// produces a 29.97i output. Soft telecine produces an output with a 23.976
  /// output that signals to the video player device to do the conversion during
  /// play back. When you keep the default value, None, MediaConvert does a
  /// standard frame rate conversion to 29.97 without doing anything with the
  /// field polarity to create a smoother picture.
  final H264Telecine? telecine;

  /// Only use this setting when you change the default value, AUTO, for the
  /// setting H264AdaptiveQuantization. When you keep all defaults, excluding
  /// H264AdaptiveQuantization and all other adaptive quantization from your JSON
  /// job specification, MediaConvert automatically applies the best types of
  /// quantization for your video content. When you set H264AdaptiveQuantization
  /// to a value other than AUTO, the default value for
  /// H264TemporalAdaptiveQuantization is Enabled. Keep this default value to
  /// adjust quantization within each frame based on temporal variation of content
  /// complexity. When you enable this feature, the encoder uses fewer bits on
  /// areas of the frame that aren't moving and uses more bits on complex objects
  /// with sharp edges that move a lot. For example, this feature improves the
  /// readability of text tickers on newscasts and scoreboards on sports matches.
  /// Enabling this feature will almost always improve your video quality. Note,
  /// though, that this feature doesn't take into account where the viewer's
  /// attention is likely to be. If viewers are likely to be focusing their
  /// attention on a part of the screen that doesn't have moving objects with
  /// sharp edges, such as sports athletes' faces, you might choose to set
  /// H264TemporalAdaptiveQuantization to Disabled. Related setting: When you
  /// enable temporal quantization, adjust the strength of the filter with the
  /// setting Adaptive quantization. To manually enable or disable
  /// H264TemporalAdaptiveQuantization, you must set Adaptive quantization to a
  /// value other than AUTO.
  final H264TemporalAdaptiveQuantization? temporalAdaptiveQuantization;

  /// Inserts timecode for each frame as 4 bytes of an unregistered SEI message.
  final H264UnregisteredSeiTimecode? unregisteredSeiTimecode;

  H264Settings({
    this.adaptiveQuantization,
    this.bandwidthReductionFilter,
    this.bitrate,
    this.codecLevel,
    this.codecProfile,
    this.dynamicSubGop,
    this.endOfStreamMarkers,
    this.entropyEncoding,
    this.fieldEncoding,
    this.flickerAdaptiveQuantization,
    this.framerateControl,
    this.framerateConversionAlgorithm,
    this.framerateDenominator,
    this.framerateNumerator,
    this.gopBReference,
    this.gopClosedCadence,
    this.gopSize,
    this.gopSizeUnits,
    this.hrdBufferFinalFillPercentage,
    this.hrdBufferInitialFillPercentage,
    this.hrdBufferSize,
    this.interlaceMode,
    this.maxBitrate,
    this.minIInterval,
    this.numberBFramesBetweenReferenceFrames,
    this.numberReferenceFrames,
    this.parControl,
    this.parDenominator,
    this.parNumerator,
    this.qualityTuningLevel,
    this.qvbrSettings,
    this.rateControlMode,
    this.repeatPps,
    this.scanTypeConversionMode,
    this.sceneChangeDetect,
    this.slices,
    this.slowPal,
    this.softness,
    this.spatialAdaptiveQuantization,
    this.syntax,
    this.telecine,
    this.temporalAdaptiveQuantization,
    this.unregisteredSeiTimecode,
  });

  factory H264Settings.fromJson(Map<String, dynamic> json) {
    return H264Settings(
      adaptiveQuantization: (json['adaptiveQuantization'] as String?)
          ?.let(H264AdaptiveQuantization.fromString),
      bandwidthReductionFilter: json['bandwidthReductionFilter'] != null
          ? BandwidthReductionFilter.fromJson(
              json['bandwidthReductionFilter'] as Map<String, dynamic>)
          : null,
      bitrate: json['bitrate'] as int?,
      codecLevel:
          (json['codecLevel'] as String?)?.let(H264CodecLevel.fromString),
      codecProfile:
          (json['codecProfile'] as String?)?.let(H264CodecProfile.fromString),
      dynamicSubGop:
          (json['dynamicSubGop'] as String?)?.let(H264DynamicSubGop.fromString),
      endOfStreamMarkers: (json['endOfStreamMarkers'] as String?)
          ?.let(H264EndOfStreamMarkers.fromString),
      entropyEncoding: (json['entropyEncoding'] as String?)
          ?.let(H264EntropyEncoding.fromString),
      fieldEncoding:
          (json['fieldEncoding'] as String?)?.let(H264FieldEncoding.fromString),
      flickerAdaptiveQuantization:
          (json['flickerAdaptiveQuantization'] as String?)
              ?.let(H264FlickerAdaptiveQuantization.fromString),
      framerateControl: (json['framerateControl'] as String?)
          ?.let(H264FramerateControl.fromString),
      framerateConversionAlgorithm:
          (json['framerateConversionAlgorithm'] as String?)
              ?.let(H264FramerateConversionAlgorithm.fromString),
      framerateDenominator: json['framerateDenominator'] as int?,
      framerateNumerator: json['framerateNumerator'] as int?,
      gopBReference:
          (json['gopBReference'] as String?)?.let(H264GopBReference.fromString),
      gopClosedCadence: json['gopClosedCadence'] as int?,
      gopSize: json['gopSize'] as double?,
      gopSizeUnits:
          (json['gopSizeUnits'] as String?)?.let(H264GopSizeUnits.fromString),
      hrdBufferFinalFillPercentage:
          json['hrdBufferFinalFillPercentage'] as int?,
      hrdBufferInitialFillPercentage:
          json['hrdBufferInitialFillPercentage'] as int?,
      hrdBufferSize: json['hrdBufferSize'] as int?,
      interlaceMode:
          (json['interlaceMode'] as String?)?.let(H264InterlaceMode.fromString),
      maxBitrate: json['maxBitrate'] as int?,
      minIInterval: json['minIInterval'] as int?,
      numberBFramesBetweenReferenceFrames:
          json['numberBFramesBetweenReferenceFrames'] as int?,
      numberReferenceFrames: json['numberReferenceFrames'] as int?,
      parControl:
          (json['parControl'] as String?)?.let(H264ParControl.fromString),
      parDenominator: json['parDenominator'] as int?,
      parNumerator: json['parNumerator'] as int?,
      qualityTuningLevel: (json['qualityTuningLevel'] as String?)
          ?.let(H264QualityTuningLevel.fromString),
      qvbrSettings: json['qvbrSettings'] != null
          ? H264QvbrSettings.fromJson(
              json['qvbrSettings'] as Map<String, dynamic>)
          : null,
      rateControlMode: (json['rateControlMode'] as String?)
          ?.let(H264RateControlMode.fromString),
      repeatPps: (json['repeatPps'] as String?)?.let(H264RepeatPps.fromString),
      scanTypeConversionMode: (json['scanTypeConversionMode'] as String?)
          ?.let(H264ScanTypeConversionMode.fromString),
      sceneChangeDetect: (json['sceneChangeDetect'] as String?)
          ?.let(H264SceneChangeDetect.fromString),
      slices: json['slices'] as int?,
      slowPal: (json['slowPal'] as String?)?.let(H264SlowPal.fromString),
      softness: json['softness'] as int?,
      spatialAdaptiveQuantization:
          (json['spatialAdaptiveQuantization'] as String?)
              ?.let(H264SpatialAdaptiveQuantization.fromString),
      syntax: (json['syntax'] as String?)?.let(H264Syntax.fromString),
      telecine: (json['telecine'] as String?)?.let(H264Telecine.fromString),
      temporalAdaptiveQuantization:
          (json['temporalAdaptiveQuantization'] as String?)
              ?.let(H264TemporalAdaptiveQuantization.fromString),
      unregisteredSeiTimecode: (json['unregisteredSeiTimecode'] as String?)
          ?.let(H264UnregisteredSeiTimecode.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final adaptiveQuantization = this.adaptiveQuantization;
    final bandwidthReductionFilter = this.bandwidthReductionFilter;
    final bitrate = this.bitrate;
    final codecLevel = this.codecLevel;
    final codecProfile = this.codecProfile;
    final dynamicSubGop = this.dynamicSubGop;
    final endOfStreamMarkers = this.endOfStreamMarkers;
    final entropyEncoding = this.entropyEncoding;
    final fieldEncoding = this.fieldEncoding;
    final flickerAdaptiveQuantization = this.flickerAdaptiveQuantization;
    final framerateControl = this.framerateControl;
    final framerateConversionAlgorithm = this.framerateConversionAlgorithm;
    final framerateDenominator = this.framerateDenominator;
    final framerateNumerator = this.framerateNumerator;
    final gopBReference = this.gopBReference;
    final gopClosedCadence = this.gopClosedCadence;
    final gopSize = this.gopSize;
    final gopSizeUnits = this.gopSizeUnits;
    final hrdBufferFinalFillPercentage = this.hrdBufferFinalFillPercentage;
    final hrdBufferInitialFillPercentage = this.hrdBufferInitialFillPercentage;
    final hrdBufferSize = this.hrdBufferSize;
    final interlaceMode = this.interlaceMode;
    final maxBitrate = this.maxBitrate;
    final minIInterval = this.minIInterval;
    final numberBFramesBetweenReferenceFrames =
        this.numberBFramesBetweenReferenceFrames;
    final numberReferenceFrames = this.numberReferenceFrames;
    final parControl = this.parControl;
    final parDenominator = this.parDenominator;
    final parNumerator = this.parNumerator;
    final qualityTuningLevel = this.qualityTuningLevel;
    final qvbrSettings = this.qvbrSettings;
    final rateControlMode = this.rateControlMode;
    final repeatPps = this.repeatPps;
    final scanTypeConversionMode = this.scanTypeConversionMode;
    final sceneChangeDetect = this.sceneChangeDetect;
    final slices = this.slices;
    final slowPal = this.slowPal;
    final softness = this.softness;
    final spatialAdaptiveQuantization = this.spatialAdaptiveQuantization;
    final syntax = this.syntax;
    final telecine = this.telecine;
    final temporalAdaptiveQuantization = this.temporalAdaptiveQuantization;
    final unregisteredSeiTimecode = this.unregisteredSeiTimecode;
    return {
      if (adaptiveQuantization != null)
        'adaptiveQuantization': adaptiveQuantization.value,
      if (bandwidthReductionFilter != null)
        'bandwidthReductionFilter': bandwidthReductionFilter,
      if (bitrate != null) 'bitrate': bitrate,
      if (codecLevel != null) 'codecLevel': codecLevel.value,
      if (codecProfile != null) 'codecProfile': codecProfile.value,
      if (dynamicSubGop != null) 'dynamicSubGop': dynamicSubGop.value,
      if (endOfStreamMarkers != null)
        'endOfStreamMarkers': endOfStreamMarkers.value,
      if (entropyEncoding != null) 'entropyEncoding': entropyEncoding.value,
      if (fieldEncoding != null) 'fieldEncoding': fieldEncoding.value,
      if (flickerAdaptiveQuantization != null)
        'flickerAdaptiveQuantization': flickerAdaptiveQuantization.value,
      if (framerateControl != null) 'framerateControl': framerateControl.value,
      if (framerateConversionAlgorithm != null)
        'framerateConversionAlgorithm': framerateConversionAlgorithm.value,
      if (framerateDenominator != null)
        'framerateDenominator': framerateDenominator,
      if (framerateNumerator != null) 'framerateNumerator': framerateNumerator,
      if (gopBReference != null) 'gopBReference': gopBReference.value,
      if (gopClosedCadence != null) 'gopClosedCadence': gopClosedCadence,
      if (gopSize != null) 'gopSize': gopSize,
      if (gopSizeUnits != null) 'gopSizeUnits': gopSizeUnits.value,
      if (hrdBufferFinalFillPercentage != null)
        'hrdBufferFinalFillPercentage': hrdBufferFinalFillPercentage,
      if (hrdBufferInitialFillPercentage != null)
        'hrdBufferInitialFillPercentage': hrdBufferInitialFillPercentage,
      if (hrdBufferSize != null) 'hrdBufferSize': hrdBufferSize,
      if (interlaceMode != null) 'interlaceMode': interlaceMode.value,
      if (maxBitrate != null) 'maxBitrate': maxBitrate,
      if (minIInterval != null) 'minIInterval': minIInterval,
      if (numberBFramesBetweenReferenceFrames != null)
        'numberBFramesBetweenReferenceFrames':
            numberBFramesBetweenReferenceFrames,
      if (numberReferenceFrames != null)
        'numberReferenceFrames': numberReferenceFrames,
      if (parControl != null) 'parControl': parControl.value,
      if (parDenominator != null) 'parDenominator': parDenominator,
      if (parNumerator != null) 'parNumerator': parNumerator,
      if (qualityTuningLevel != null)
        'qualityTuningLevel': qualityTuningLevel.value,
      if (qvbrSettings != null) 'qvbrSettings': qvbrSettings,
      if (rateControlMode != null) 'rateControlMode': rateControlMode.value,
      if (repeatPps != null) 'repeatPps': repeatPps.value,
      if (scanTypeConversionMode != null)
        'scanTypeConversionMode': scanTypeConversionMode.value,
      if (sceneChangeDetect != null)
        'sceneChangeDetect': sceneChangeDetect.value,
      if (slices != null) 'slices': slices,
      if (slowPal != null) 'slowPal': slowPal.value,
      if (softness != null) 'softness': softness,
      if (spatialAdaptiveQuantization != null)
        'spatialAdaptiveQuantization': spatialAdaptiveQuantization.value,
      if (syntax != null) 'syntax': syntax.value,
      if (telecine != null) 'telecine': telecine.value,
      if (temporalAdaptiveQuantization != null)
        'temporalAdaptiveQuantization': temporalAdaptiveQuantization.value,
      if (unregisteredSeiTimecode != null)
        'unregisteredSeiTimecode': unregisteredSeiTimecode.value,
    };
  }
}

/// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
/// second (fps). Enable slow PAL to create a 25 fps output. When you enable
/// slow PAL, MediaConvert relabels the video frames to 25 fps and resamples
/// your audio to keep it synchronized with the video. Note that enabling this
/// setting will slightly reduce the duration of your video. Required settings:
/// You must also set Framerate to 25.
enum H264SlowPal {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const H264SlowPal(this.value);

  static H264SlowPal fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum H264SlowPal'));
}

/// Only use this setting when you change the default value, Auto, for the
/// setting H264AdaptiveQuantization. When you keep all defaults, excluding
/// H264AdaptiveQuantization and all other adaptive quantization from your JSON
/// job specification, MediaConvert automatically applies the best types of
/// quantization for your video content. When you set H264AdaptiveQuantization
/// to a value other than AUTO, the default value for
/// H264SpatialAdaptiveQuantization is Enabled. Keep this default value to
/// adjust quantization within each frame based on spatial variation of content
/// complexity. When you enable this feature, the encoder uses fewer bits on
/// areas that can sustain more distortion with no noticeable visual degradation
/// and uses more bits on areas where any small distortion will be noticeable.
/// For example, complex textured blocks are encoded with fewer bits and smooth
/// textured blocks are encoded with more bits. Enabling this feature will
/// almost always improve your video quality. Note, though, that this feature
/// doesn't take into account where the viewer's attention is likely to be. If
/// viewers are likely to be focusing their attention on a part of the screen
/// with a lot of complex texture, you might choose to set
/// H264SpatialAdaptiveQuantization to Disabled. Related setting: When you
/// enable spatial adaptive quantization, set the value for Adaptive
/// quantization depending on your content. For homogeneous content, such as
/// cartoons and video games, set it to Low. For content with a wider variety of
/// textures, set it to High or Higher. To manually enable or disable
/// H264SpatialAdaptiveQuantization, you must set Adaptive quantization to a
/// value other than AUTO.
enum H264SpatialAdaptiveQuantization {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const H264SpatialAdaptiveQuantization(this.value);

  static H264SpatialAdaptiveQuantization fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H264SpatialAdaptiveQuantization'));
}

/// Produces a bitstream compliant with SMPTE RP-2027.
enum H264Syntax {
  $default('DEFAULT'),
  rp2027('RP2027'),
  ;

  final String value;

  const H264Syntax(this.value);

  static H264Syntax fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum H264Syntax'));
}

/// When you do frame rate conversion from 23.976 frames per second (fps) to
/// 29.97 fps, and your output scan type is interlaced, you can optionally
/// enable hard or soft telecine to create a smoother picture. Hard telecine
/// produces a 29.97i output. Soft telecine produces an output with a 23.976
/// output that signals to the video player device to do the conversion during
/// play back. When you keep the default value, None, MediaConvert does a
/// standard frame rate conversion to 29.97 without doing anything with the
/// field polarity to create a smoother picture.
enum H264Telecine {
  none('NONE'),
  soft('SOFT'),
  hard('HARD'),
  ;

  final String value;

  const H264Telecine(this.value);

  static H264Telecine fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H264Telecine'));
}

/// Only use this setting when you change the default value, AUTO, for the
/// setting H264AdaptiveQuantization. When you keep all defaults, excluding
/// H264AdaptiveQuantization and all other adaptive quantization from your JSON
/// job specification, MediaConvert automatically applies the best types of
/// quantization for your video content. When you set H264AdaptiveQuantization
/// to a value other than AUTO, the default value for
/// H264TemporalAdaptiveQuantization is Enabled. Keep this default value to
/// adjust quantization within each frame based on temporal variation of content
/// complexity. When you enable this feature, the encoder uses fewer bits on
/// areas of the frame that aren't moving and uses more bits on complex objects
/// with sharp edges that move a lot. For example, this feature improves the
/// readability of text tickers on newscasts and scoreboards on sports matches.
/// Enabling this feature will almost always improve your video quality. Note,
/// though, that this feature doesn't take into account where the viewer's
/// attention is likely to be. If viewers are likely to be focusing their
/// attention on a part of the screen that doesn't have moving objects with
/// sharp edges, such as sports athletes' faces, you might choose to set
/// H264TemporalAdaptiveQuantization to Disabled. Related setting: When you
/// enable temporal quantization, adjust the strength of the filter with the
/// setting Adaptive quantization. To manually enable or disable
/// H264TemporalAdaptiveQuantization, you must set Adaptive quantization to a
/// value other than AUTO.
enum H264TemporalAdaptiveQuantization {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const H264TemporalAdaptiveQuantization(this.value);

  static H264TemporalAdaptiveQuantization fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H264TemporalAdaptiveQuantization'));
}

/// Inserts timecode for each frame as 4 bytes of an unregistered SEI message.
enum H264UnregisteredSeiTimecode {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const H264UnregisteredSeiTimecode(this.value);

  static H264UnregisteredSeiTimecode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H264UnregisteredSeiTimecode'));
}

/// When you set Adaptive Quantization to Auto, or leave blank, MediaConvert
/// automatically applies quantization to improve the video quality of your
/// output. Set Adaptive Quantization to Low, Medium, High, Higher, or Max to
/// manually control the strength of the quantization filter. When you do, you
/// can specify a value for Spatial Adaptive Quantization, Temporal Adaptive
/// Quantization, and Flicker Adaptive Quantization, to further control the
/// quantization filter. Set Adaptive Quantization to Off to apply no
/// quantization to your output.
enum H265AdaptiveQuantization {
  off('OFF'),
  low('LOW'),
  medium('MEDIUM'),
  high('HIGH'),
  higher('HIGHER'),
  max('MAX'),
  auto('AUTO'),
  ;

  final String value;

  const H265AdaptiveQuantization(this.value);

  static H265AdaptiveQuantization fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H265AdaptiveQuantization'));
}

/// Enables Alternate Transfer Function SEI message for outputs using Hybrid Log
/// Gamma (HLG) Electro-Optical Transfer Function (EOTF).
enum H265AlternateTransferFunctionSei {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const H265AlternateTransferFunctionSei(this.value);

  static H265AlternateTransferFunctionSei fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H265AlternateTransferFunctionSei'));
}

/// H.265 Level.
enum H265CodecLevel {
  auto('AUTO'),
  level_1('LEVEL_1'),
  level_2('LEVEL_2'),
  level_2_1('LEVEL_2_1'),
  level_3('LEVEL_3'),
  level_3_1('LEVEL_3_1'),
  level_4('LEVEL_4'),
  level_4_1('LEVEL_4_1'),
  level_5('LEVEL_5'),
  level_5_1('LEVEL_5_1'),
  level_5_2('LEVEL_5_2'),
  level_6('LEVEL_6'),
  level_6_1('LEVEL_6_1'),
  level_6_2('LEVEL_6_2'),
  ;

  final String value;

  const H265CodecLevel(this.value);

  static H265CodecLevel fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H265CodecLevel'));
}

/// Represents the Profile and Tier, per the HEVC (H.265) specification.
/// Selections are grouped as [Profile] / [Tier], so "Main/High" represents Main
/// Profile with High Tier. 4:2:2 profiles are only available with the HEVC
/// 4:2:2 License.
enum H265CodecProfile {
  mainMain('MAIN_MAIN'),
  mainHigh('MAIN_HIGH'),
  main10Main('MAIN10_MAIN'),
  main10High('MAIN10_HIGH'),
  main_422_8bitMain('MAIN_422_8BIT_MAIN'),
  main_422_8bitHigh('MAIN_422_8BIT_HIGH'),
  main_422_10bitMain('MAIN_422_10BIT_MAIN'),
  main_422_10bitHigh('MAIN_422_10BIT_HIGH'),
  ;

  final String value;

  const H265CodecProfile(this.value);

  static H265CodecProfile fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H265CodecProfile'));
}

/// Choose Adaptive to improve subjective video quality for high-motion content.
/// This will cause the service to use fewer B-frames (which infer information
/// based on other frames) for high-motion portions of the video and more
/// B-frames for low-motion portions. The maximum number of B-frames is limited
/// by the value you provide for the setting B frames between reference frames.
enum H265DynamicSubGop {
  adaptive('ADAPTIVE'),
  static('STATIC'),
  ;

  final String value;

  const H265DynamicSubGop(this.value);

  static H265DynamicSubGop fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H265DynamicSubGop'));
}

/// Optionally include or suppress markers at the end of your output that signal
/// the end of the video stream. To include end of stream markers: Leave blank
/// or keep the default value, Include. To not include end of stream markers:
/// Choose Suppress. This is useful when your output will be inserted into
/// another stream.
enum H265EndOfStreamMarkers {
  include('INCLUDE'),
  suppress('SUPPRESS'),
  ;

  final String value;

  const H265EndOfStreamMarkers(this.value);

  static H265EndOfStreamMarkers fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H265EndOfStreamMarkers'));
}

/// Enable this setting to have the encoder reduce I-frame pop. I-frame pop
/// appears as a visual flicker that can arise when the encoder saves bits by
/// copying some macroblocks many times from frame to frame, and then refreshes
/// them at the I-frame. When you enable this setting, the encoder updates these
/// macroblocks slightly more often to smooth out the flicker. This setting is
/// disabled by default. Related setting: In addition to enabling this setting,
/// you must also set adaptiveQuantization to a value other than Off.
enum H265FlickerAdaptiveQuantization {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const H265FlickerAdaptiveQuantization(this.value);

  static H265FlickerAdaptiveQuantization fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H265FlickerAdaptiveQuantization'));
}

/// Use the Framerate setting to specify the frame rate for this output. If you
/// want to keep the same frame rate as the input video, choose Follow source.
/// If you want to do frame rate conversion, choose a frame rate from the
/// dropdown list or choose Custom. The framerates shown in the dropdown list
/// are decimal approximations of fractions. If you choose Custom, specify your
/// frame rate as a fraction.
enum H265FramerateControl {
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  specified('SPECIFIED'),
  ;

  final String value;

  const H265FramerateControl(this.value);

  static H265FramerateControl fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum H265FramerateControl'));
}

/// Choose the method that you want MediaConvert to use when increasing or
/// decreasing the frame rate. For numerically simple conversions, such as 60
/// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
/// For numerically complex conversions, to avoid stutter: Choose Interpolate.
/// This results in a smooth picture, but might introduce undesirable video
/// artifacts. For complex frame rate conversions, especially if your source
/// video has already been converted from its original cadence: Choose
/// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
/// best conversion method frame by frame. Note that using FrameFormer increases
/// the transcoding time and incurs a significant add-on cost. When you choose
/// FrameFormer, your input video resolution must be at least 128x96.
enum H265FramerateConversionAlgorithm {
  duplicateDrop('DUPLICATE_DROP'),
  interpolate('INTERPOLATE'),
  frameformer('FRAMEFORMER'),
  ;

  final String value;

  const H265FramerateConversionAlgorithm(this.value);

  static H265FramerateConversionAlgorithm fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H265FramerateConversionAlgorithm'));
}

/// Specify whether to allow B-frames to be referenced by other frame types. To
/// use reference B-frames when your GOP structure has 1 or more B-frames: Leave
/// blank or keep the default value Enabled. We recommend that you choose
/// Enabled to help improve the video quality of your output relative to its
/// bitrate. To not use reference B-frames: Choose Disabled.
enum H265GopBReference {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const H265GopBReference(this.value);

  static H265GopBReference fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H265GopBReference'));
}

/// Specify how the transcoder determines GOP size for this output. We recommend
/// that you have the transcoder automatically choose this value for you based
/// on characteristics of your input video. To enable this automatic behavior,
/// choose Auto and and leave GOP size blank. By default, if you don't specify
/// GOP mode control, MediaConvert will use automatic behavior. If your output
/// group specifies HLS, DASH, or CMAF, set GOP mode control to Auto and leave
/// GOP size blank in each output in your output group. To explicitly specify
/// the GOP length, choose Specified, frames or Specified, seconds and then
/// provide the GOP length in the related setting GOP size.
enum H265GopSizeUnits {
  frames('FRAMES'),
  seconds('SECONDS'),
  auto('AUTO'),
  ;

  final String value;

  const H265GopSizeUnits(this.value);

  static H265GopSizeUnits fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H265GopSizeUnits'));
}

/// Choose the scan line type for the output. Keep the default value,
/// Progressive to create a progressive output, regardless of the scan type of
/// your input. Use Top field first or Bottom field first to create an output
/// that's interlaced with the same field polarity throughout. Use Follow,
/// default top or Follow, default bottom to produce outputs with the same field
/// polarity as the source. For jobs that have multiple inputs, the output field
/// polarity might change over the course of the output. Follow behavior depends
/// on the input scan type. If the source is interlaced, the output will be
/// interlaced with the same polarity as the source. If the source is
/// progressive, the output will be interlaced with top field bottom field
/// first, depending on which of the Follow options you choose.
enum H265InterlaceMode {
  progressive('PROGRESSIVE'),
  topField('TOP_FIELD'),
  bottomField('BOTTOM_FIELD'),
  followTopField('FOLLOW_TOP_FIELD'),
  followBottomField('FOLLOW_BOTTOM_FIELD'),
  ;

  final String value;

  const H265InterlaceMode(this.value);

  static H265InterlaceMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H265InterlaceMode'));
}

/// Optional. Specify how the service determines the pixel aspect ratio (PAR)
/// for this output. The default behavior, Follow source, uses the PAR from your
/// input video for your output. To specify a different PAR, choose any value
/// other than Follow source. When you choose SPECIFIED for this setting, you
/// must also specify values for the parNumerator and parDenominator settings.
enum H265ParControl {
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  specified('SPECIFIED'),
  ;

  final String value;

  const H265ParControl(this.value);

  static H265ParControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H265ParControl'));
}

/// Optional. Use Quality tuning level to choose how you want to trade off
/// encoding speed for output video quality. The default behavior is faster,
/// lower quality, single-pass encoding.
enum H265QualityTuningLevel {
  singlePass('SINGLE_PASS'),
  singlePassHq('SINGLE_PASS_HQ'),
  multiPassHq('MULTI_PASS_HQ'),
  ;

  final String value;

  const H265QualityTuningLevel(this.value);

  static H265QualityTuningLevel fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H265QualityTuningLevel'));
}

/// Settings for quality-defined variable bitrate encoding with the H.265 codec.
/// Use these settings only when you set QVBR for Rate control mode.
class H265QvbrSettings {
  /// Use this setting only when Rate control mode is QVBR and Quality tuning
  /// level is Multi-pass HQ. For Max average bitrate values suited to the
  /// complexity of your input video, the service limits the average bitrate of
  /// the video part of this output to the value that you choose. That is, the
  /// total size of the video element is less than or equal to the value you set
  /// multiplied by the number of seconds of encoded output.
  final int? maxAverageBitrate;

  /// Use this setting only when you set Rate control mode to QVBR. Specify the
  /// target quality level for this output. MediaConvert determines the right
  /// number of bits to use for each part of the video to maintain the video
  /// quality that you specify. When you keep the default value, AUTO,
  /// MediaConvert picks a quality level for you, based on characteristics of your
  /// input video. If you prefer to specify a quality level, specify a number from
  /// 1 through 10. Use higher numbers for greater quality. Level 10 results in
  /// nearly lossless compression. The quality level for most broadcast-quality
  /// transcodes is between 6 and 9. Optionally, to specify a value between whole
  /// numbers, also provide a value for the setting qvbrQualityLevelFineTune. For
  /// example, if you want your QVBR quality level to be 7.33, set
  /// qvbrQualityLevel to 7 and set qvbrQualityLevelFineTune to .33.
  final int? qvbrQualityLevel;

  /// Optional. Specify a value here to set the QVBR quality to a level that is
  /// between whole numbers. For example, if you want your QVBR quality level to
  /// be 7.33, set qvbrQualityLevel to 7 and set qvbrQualityLevelFineTune to .33.
  /// MediaConvert rounds your QVBR quality level to the nearest third of a whole
  /// number. For example, if you set qvbrQualityLevel to 7 and you set
  /// qvbrQualityLevelFineTune to .25, your actual QVBR quality level is 7.33.
  final double? qvbrQualityLevelFineTune;

  H265QvbrSettings({
    this.maxAverageBitrate,
    this.qvbrQualityLevel,
    this.qvbrQualityLevelFineTune,
  });

  factory H265QvbrSettings.fromJson(Map<String, dynamic> json) {
    return H265QvbrSettings(
      maxAverageBitrate: json['maxAverageBitrate'] as int?,
      qvbrQualityLevel: json['qvbrQualityLevel'] as int?,
      qvbrQualityLevelFineTune: json['qvbrQualityLevelFineTune'] as double?,
    );
  }

  Map<String, dynamic> toJson() {
    final maxAverageBitrate = this.maxAverageBitrate;
    final qvbrQualityLevel = this.qvbrQualityLevel;
    final qvbrQualityLevelFineTune = this.qvbrQualityLevelFineTune;
    return {
      if (maxAverageBitrate != null) 'maxAverageBitrate': maxAverageBitrate,
      if (qvbrQualityLevel != null) 'qvbrQualityLevel': qvbrQualityLevel,
      if (qvbrQualityLevelFineTune != null)
        'qvbrQualityLevelFineTune': qvbrQualityLevelFineTune,
    };
  }
}

/// Use this setting to specify whether this output has a variable bitrate
/// (VBR), constant bitrate (CBR) or quality-defined variable bitrate (QVBR).
enum H265RateControlMode {
  vbr('VBR'),
  cbr('CBR'),
  qvbr('QVBR'),
  ;

  final String value;

  const H265RateControlMode(this.value);

  static H265RateControlMode fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum H265RateControlMode'));
}

/// Specify Sample Adaptive Offset (SAO) filter strength. Adaptive mode
/// dynamically selects best strength based on content
enum H265SampleAdaptiveOffsetFilterMode {
  $default('DEFAULT'),
  adaptive('ADAPTIVE'),
  off('OFF'),
  ;

  final String value;

  const H265SampleAdaptiveOffsetFilterMode(this.value);

  static H265SampleAdaptiveOffsetFilterMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H265SampleAdaptiveOffsetFilterMode'));
}

/// Use this setting for interlaced outputs, when your output frame rate is half
/// of your input frame rate. In this situation, choose Optimized interlacing to
/// create a better quality interlaced output. In this case, each progressive
/// frame from the input corresponds to an interlaced field in the output. Keep
/// the default value, Basic interlacing, for all other output frame rates. With
/// basic interlacing, MediaConvert performs any frame rate conversion first and
/// then interlaces the frames. When you choose Optimized interlacing and you
/// set your output frame rate to a value that isn't suitable for optimized
/// interlacing, MediaConvert automatically falls back to basic interlacing.
/// Required settings: To use optimized interlacing, you must set Telecine to
/// None or Soft. You can't use optimized interlacing for hard telecine outputs.
/// You must also set Interlace mode to a value other than Progressive.
enum H265ScanTypeConversionMode {
  interlaced('INTERLACED'),
  interlacedOptimize('INTERLACED_OPTIMIZE'),
  ;

  final String value;

  const H265ScanTypeConversionMode(this.value);

  static H265ScanTypeConversionMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H265ScanTypeConversionMode'));
}

/// Enable this setting to insert I-frames at scene changes that the service
/// automatically detects. This improves video quality and is enabled by
/// default. If this output uses QVBR, choose Transition detection for further
/// video quality improvement. For more information about QVBR, see
/// https://docs.aws.amazon.com/console/mediaconvert/cbr-vbr-qvbr.
enum H265SceneChangeDetect {
  disabled('DISABLED'),
  enabled('ENABLED'),
  transitionDetection('TRANSITION_DETECTION'),
  ;

  final String value;

  const H265SceneChangeDetect(this.value);

  static H265SceneChangeDetect fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum H265SceneChangeDetect'));
}

/// Settings for H265 codec
class H265Settings {
  /// When you set Adaptive Quantization to Auto, or leave blank, MediaConvert
  /// automatically applies quantization to improve the video quality of your
  /// output. Set Adaptive Quantization to Low, Medium, High, Higher, or Max to
  /// manually control the strength of the quantization filter. When you do, you
  /// can specify a value for Spatial Adaptive Quantization, Temporal Adaptive
  /// Quantization, and Flicker Adaptive Quantization, to further control the
  /// quantization filter. Set Adaptive Quantization to Off to apply no
  /// quantization to your output.
  final H265AdaptiveQuantization? adaptiveQuantization;

  /// Enables Alternate Transfer Function SEI message for outputs using Hybrid Log
  /// Gamma (HLG) Electro-Optical Transfer Function (EOTF).
  final H265AlternateTransferFunctionSei? alternateTransferFunctionSei;

  /// The Bandwidth reduction filter increases the video quality of your output
  /// relative to its bitrate. Use to lower the bitrate of your constant quality
  /// QVBR output, with little or no perceptual decrease in quality. Or, use to
  /// increase the video quality of outputs with other rate control modes relative
  /// to the bitrate that you specify. Bandwidth reduction increases further when
  /// your input is low quality or noisy. Outputs that use this feature incur
  /// pro-tier pricing. When you include Bandwidth reduction filter, you cannot
  /// include the Noise reducer preprocessor.
  final BandwidthReductionFilter? bandwidthReductionFilter;

  /// Specify the average bitrate in bits per second. Required for VBR and CBR.
  /// For MS Smooth outputs, bitrates must be unique when rounded down to the
  /// nearest multiple of 1000.
  final int? bitrate;

  /// H.265 Level.
  final H265CodecLevel? codecLevel;

  /// Represents the Profile and Tier, per the HEVC (H.265) specification.
  /// Selections are grouped as [Profile] / [Tier], so "Main/High" represents Main
  /// Profile with High Tier. 4:2:2 profiles are only available with the HEVC
  /// 4:2:2 License.
  final H265CodecProfile? codecProfile;

  /// Specify whether to allow the number of B-frames in your output GOP structure
  /// to vary or not depending on your input video content. To improve the
  /// subjective video quality of your output that has high-motion content: Leave
  /// blank or keep the default value Adaptive. MediaConvert will use fewer
  /// B-frames for high-motion video content than low-motion content. The maximum
  /// number of B- frames is limited by the value that you choose for B-frames
  /// between reference frames. To use the same number B-frames for all types of
  /// content: Choose Static.
  final H265DynamicSubGop? dynamicSubGop;

  /// Optionally include or suppress markers at the end of your output that signal
  /// the end of the video stream. To include end of stream markers: Leave blank
  /// or keep the default value, Include. To not include end of stream markers:
  /// Choose Suppress. This is useful when your output will be inserted into
  /// another stream.
  final H265EndOfStreamMarkers? endOfStreamMarkers;

  /// Enable this setting to have the encoder reduce I-frame pop. I-frame pop
  /// appears as a visual flicker that can arise when the encoder saves bits by
  /// copying some macroblocks many times from frame to frame, and then refreshes
  /// them at the I-frame. When you enable this setting, the encoder updates these
  /// macroblocks slightly more often to smooth out the flicker. This setting is
  /// disabled by default. Related setting: In addition to enabling this setting,
  /// you must also set adaptiveQuantization to a value other than Off.
  final H265FlickerAdaptiveQuantization? flickerAdaptiveQuantization;

  /// Use the Framerate setting to specify the frame rate for this output. If you
  /// want to keep the same frame rate as the input video, choose Follow source.
  /// If you want to do frame rate conversion, choose a frame rate from the
  /// dropdown list or choose Custom. The framerates shown in the dropdown list
  /// are decimal approximations of fractions. If you choose Custom, specify your
  /// frame rate as a fraction.
  final H265FramerateControl? framerateControl;

  /// Choose the method that you want MediaConvert to use when increasing or
  /// decreasing the frame rate. For numerically simple conversions, such as 60
  /// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
  /// For numerically complex conversions, to avoid stutter: Choose Interpolate.
  /// This results in a smooth picture, but might introduce undesirable video
  /// artifacts. For complex frame rate conversions, especially if your source
  /// video has already been converted from its original cadence: Choose
  /// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
  /// best conversion method frame by frame. Note that using FrameFormer increases
  /// the transcoding time and incurs a significant add-on cost. When you choose
  /// FrameFormer, your input video resolution must be at least 128x96.
  final H265FramerateConversionAlgorithm? framerateConversionAlgorithm;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateDenominator to specify the denominator of this fraction.
  /// In this example, use 1001 for the value of FramerateDenominator. When you
  /// use the console for transcode jobs that use frame rate conversion, provide
  /// the value as a decimal number for Framerate. In this example, specify
  /// 23.976.
  final int? framerateDenominator;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateNumerator to specify the numerator of this fraction. In
  /// this example, use 24000 for the value of FramerateNumerator. When you use
  /// the console for transcode jobs that use frame rate conversion, provide the
  /// value as a decimal number for Framerate. In this example, specify 23.976.
  final int? framerateNumerator;

  /// Specify whether to allow B-frames to be referenced by other frame types. To
  /// use reference B-frames when your GOP structure has 1 or more B-frames: Leave
  /// blank or keep the default value Enabled. We recommend that you choose
  /// Enabled to help improve the video quality of your output relative to its
  /// bitrate. To not use reference B-frames: Choose Disabled.
  final H265GopBReference? gopBReference;

  /// Specify the relative frequency of open to closed GOPs in this output. For
  /// example, if you want to allow four open GOPs and then require a closed GOP,
  /// set this value to 5. We recommend that you have the transcoder automatically
  /// choose this value for you based on characteristics of your input video. To
  /// enable this automatic behavior, do this by keeping the default empty value.
  /// If you do explicitly specify a value, for segmented outputs, don't set this
  /// value to 0.
  final int? gopClosedCadence;

  /// Use this setting only when you set GOP mode control to Specified, frames or
  /// Specified, seconds. Specify the GOP length using a whole number of frames or
  /// a decimal value of seconds. MediaConvert will interpret this value as frames
  /// or seconds depending on the value you choose for GOP mode control. If you
  /// want to allow MediaConvert to automatically determine GOP size, leave GOP
  /// size blank and set GOP mode control to Auto. If your output group specifies
  /// HLS, DASH, or CMAF, leave GOP size blank and set GOP mode control to Auto in
  /// each output in your output group.
  final double? gopSize;

  /// Specify how the transcoder determines GOP size for this output. We recommend
  /// that you have the transcoder automatically choose this value for you based
  /// on characteristics of your input video. To enable this automatic behavior,
  /// choose Auto and and leave GOP size blank. By default, if you don't specify
  /// GOP mode control, MediaConvert will use automatic behavior. If your output
  /// group specifies HLS, DASH, or CMAF, set GOP mode control to Auto and leave
  /// GOP size blank in each output in your output group. To explicitly specify
  /// the GOP length, choose Specified, frames or Specified, seconds and then
  /// provide the GOP length in the related setting GOP size.
  final H265GopSizeUnits? gopSizeUnits;

  /// If your downstream systems have strict buffer requirements: Specify the
  /// minimum percentage of the HRD buffer that's available at the end of each
  /// encoded video segment. For the best video quality: Set to 0 or leave blank
  /// to automatically determine the final buffer fill percentage.
  final int? hrdBufferFinalFillPercentage;

  /// Percentage of the buffer that should initially be filled (HRD buffer model).
  final int? hrdBufferInitialFillPercentage;

  /// Size of buffer (HRD buffer model) in bits. For example, enter five megabits
  /// as 5000000.
  final int? hrdBufferSize;

  /// Choose the scan line type for the output. Keep the default value,
  /// Progressive to create a progressive output, regardless of the scan type of
  /// your input. Use Top field first or Bottom field first to create an output
  /// that's interlaced with the same field polarity throughout. Use Follow,
  /// default top or Follow, default bottom to produce outputs with the same field
  /// polarity as the source. For jobs that have multiple inputs, the output field
  /// polarity might change over the course of the output. Follow behavior depends
  /// on the input scan type. If the source is interlaced, the output will be
  /// interlaced with the same polarity as the source. If the source is
  /// progressive, the output will be interlaced with top field bottom field
  /// first, depending on which of the Follow options you choose.
  final H265InterlaceMode? interlaceMode;

  /// Maximum bitrate in bits/second. For example, enter five megabits per second
  /// as 5000000. Required when Rate control mode is QVBR.
  final int? maxBitrate;

  /// Use this setting only when you also enable Scene change detection. This
  /// setting determines how the encoder manages the spacing between I-frames that
  /// it inserts as part of the I-frame cadence and the I-frames that it inserts
  /// for Scene change detection. We recommend that you have the transcoder
  /// automatically choose this value for you based on characteristics of your
  /// input video. To enable this automatic behavior, do this by keeping the
  /// default empty value. When you explicitly specify a value for this setting,
  /// the encoder determines whether to skip a cadence-driven I-frame by the value
  /// you set. For example, if you set Min I interval to 5 and a cadence-driven
  /// I-frame would fall within 5 frames of a scene-change I-frame, then the
  /// encoder skips the cadence-driven I-frame. In this way, one GOP is shrunk
  /// slightly and one GOP is stretched slightly. When the cadence-driven I-frames
  /// are farther from the scene-change I-frame than the value you set, then the
  /// encoder leaves all I-frames in place and the GOPs surrounding the scene
  /// change are smaller than the usual cadence GOPs.
  final int? minIInterval;

  /// Specify the number of B-frames between reference frames in this output. For
  /// the best video quality: Leave blank. MediaConvert automatically determines
  /// the number of B-frames to use based on the characteristics of your input
  /// video. To manually specify the number of B-frames between reference frames:
  /// Enter an integer from 0 to 7.
  final int? numberBFramesBetweenReferenceFrames;

  /// Number of reference frames to use. The encoder may use more than requested
  /// if using B-frames and/or interlaced encoding.
  final int? numberReferenceFrames;

  /// Optional. Specify how the service determines the pixel aspect ratio (PAR)
  /// for this output. The default behavior, Follow source, uses the PAR from your
  /// input video for your output. To specify a different PAR, choose any value
  /// other than Follow source. When you choose SPECIFIED for this setting, you
  /// must also specify values for the parNumerator and parDenominator settings.
  final H265ParControl? parControl;

  /// Required when you set Pixel aspect ratio to SPECIFIED. On the console, this
  /// corresponds to any value other than Follow source. When you specify an
  /// output pixel aspect ratio (PAR) that is different from your input video PAR,
  /// provide your output PAR as a ratio. For example, for D1/DV NTSC widescreen,
  /// you would specify the ratio 40:33. In this example, the value for
  /// parDenominator is 33.
  final int? parDenominator;

  /// Required when you set Pixel aspect ratio to SPECIFIED. On the console, this
  /// corresponds to any value other than Follow source. When you specify an
  /// output pixel aspect ratio (PAR) that is different from your input video PAR,
  /// provide your output PAR as a ratio. For example, for D1/DV NTSC widescreen,
  /// you would specify the ratio 40:33. In this example, the value for
  /// parNumerator is 40.
  final int? parNumerator;

  /// Optional. Use Quality tuning level to choose how you want to trade off
  /// encoding speed for output video quality. The default behavior is faster,
  /// lower quality, single-pass encoding.
  final H265QualityTuningLevel? qualityTuningLevel;

  /// Settings for quality-defined variable bitrate encoding with the H.265 codec.
  /// Use these settings only when you set QVBR for Rate control mode.
  final H265QvbrSettings? qvbrSettings;

  /// Use this setting to specify whether this output has a variable bitrate
  /// (VBR), constant bitrate (CBR) or quality-defined variable bitrate (QVBR).
  final H265RateControlMode? rateControlMode;

  /// Specify Sample Adaptive Offset (SAO) filter strength. Adaptive mode
  /// dynamically selects best strength based on content
  final H265SampleAdaptiveOffsetFilterMode? sampleAdaptiveOffsetFilterMode;

  /// Use this setting for interlaced outputs, when your output frame rate is half
  /// of your input frame rate. In this situation, choose Optimized interlacing to
  /// create a better quality interlaced output. In this case, each progressive
  /// frame from the input corresponds to an interlaced field in the output. Keep
  /// the default value, Basic interlacing, for all other output frame rates. With
  /// basic interlacing, MediaConvert performs any frame rate conversion first and
  /// then interlaces the frames. When you choose Optimized interlacing and you
  /// set your output frame rate to a value that isn't suitable for optimized
  /// interlacing, MediaConvert automatically falls back to basic interlacing.
  /// Required settings: To use optimized interlacing, you must set Telecine to
  /// None or Soft. You can't use optimized interlacing for hard telecine outputs.
  /// You must also set Interlace mode to a value other than Progressive.
  final H265ScanTypeConversionMode? scanTypeConversionMode;

  /// Enable this setting to insert I-frames at scene changes that the service
  /// automatically detects. This improves video quality and is enabled by
  /// default. If this output uses QVBR, choose Transition detection for further
  /// video quality improvement. For more information about QVBR, see
  /// https://docs.aws.amazon.com/console/mediaconvert/cbr-vbr-qvbr.
  final H265SceneChangeDetect? sceneChangeDetect;

  /// Number of slices per picture. Must be less than or equal to the number of
  /// macroblock rows for progressive pictures, and less than or equal to half the
  /// number of macroblock rows for interlaced pictures.
  final int? slices;

  /// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
  /// second (fps). Enable slow PAL to create a 25 fps output. When you enable
  /// slow PAL, MediaConvert relabels the video frames to 25 fps and resamples
  /// your audio to keep it synchronized with the video. Note that enabling this
  /// setting will slightly reduce the duration of your video. Required settings:
  /// You must also set Framerate to 25.
  final H265SlowPal? slowPal;

  /// Keep the default value, Enabled, to adjust quantization within each frame
  /// based on spatial variation of content complexity. When you enable this
  /// feature, the encoder uses fewer bits on areas that can sustain more
  /// distortion with no noticeable visual degradation and uses more bits on areas
  /// where any small distortion will be noticeable. For example, complex textured
  /// blocks are encoded with fewer bits and smooth textured blocks are encoded
  /// with more bits. Enabling this feature will almost always improve your video
  /// quality. Note, though, that this feature doesn't take into account where the
  /// viewer's attention is likely to be. If viewers are likely to be focusing
  /// their attention on a part of the screen with a lot of complex texture, you
  /// might choose to disable this feature. Related setting: When you enable
  /// spatial adaptive quantization, set the value for Adaptive quantization
  /// depending on your content. For homogeneous content, such as cartoons and
  /// video games, set it to Low. For content with a wider variety of textures,
  /// set it to High or Higher.
  final H265SpatialAdaptiveQuantization? spatialAdaptiveQuantization;

  /// This field applies only if the Streams > Advanced > Framerate field is set
  /// to 29.970. This field works with the Streams > Advanced > Preprocessors >
  /// Deinterlacer field and the Streams > Advanced > Interlaced Mode field to
  /// identify the scan type for the output: Progressive, Interlaced, Hard
  /// Telecine or Soft Telecine. - Hard: produces 29.97i output from 23.976 input.
  /// - Soft: produces 23.976; the player converts this output to 29.97i.
  final H265Telecine? telecine;

  /// Keep the default value, Enabled, to adjust quantization within each frame
  /// based on temporal variation of content complexity. When you enable this
  /// feature, the encoder uses fewer bits on areas of the frame that aren't
  /// moving and uses more bits on complex objects with sharp edges that move a
  /// lot. For example, this feature improves the readability of text tickers on
  /// newscasts and scoreboards on sports matches. Enabling this feature will
  /// almost always improve your video quality. Note, though, that this feature
  /// doesn't take into account where the viewer's attention is likely to be. If
  /// viewers are likely to be focusing their attention on a part of the screen
  /// that doesn't have moving objects with sharp edges, such as sports athletes'
  /// faces, you might choose to disable this feature. Related setting: When you
  /// enable temporal quantization, adjust the strength of the filter with the
  /// setting Adaptive quantization.
  final H265TemporalAdaptiveQuantization? temporalAdaptiveQuantization;

  /// Enables temporal layer identifiers in the encoded bitstream. Up to 3 layers
  /// are supported depending on GOP structure: I- and P-frames form one layer,
  /// reference B-frames can form a second layer and non-reference b-frames can
  /// form a third layer. Decoders can optionally decode only the lower temporal
  /// layers to generate a lower frame rate output. For example, given a bitstream
  /// with temporal IDs and with b-frames = 1 (i.e. IbPbPb display order), a
  /// decoder could decode all the frames for full frame rate output or only the I
  /// and P frames (lowest temporal layer) for a half frame rate output.
  final H265TemporalIds? temporalIds;

  /// Enable use of tiles, allowing horizontal as well as vertical subdivision of
  /// the encoded pictures.
  final H265Tiles? tiles;

  /// Inserts timecode for each frame as 4 bytes of an unregistered SEI message.
  final H265UnregisteredSeiTimecode? unregisteredSeiTimecode;

  /// If the location of parameter set NAL units doesn't matter in your workflow,
  /// ignore this setting. Use this setting only with CMAF or DASH outputs, or
  /// with standalone file outputs in an MPEG-4 container (MP4 outputs). Choose
  /// HVC1 to mark your output as HVC1. This makes your output compliant with the
  /// following specification: ISO IECJTC1 SC29 N13798 Text ISO/IEC FDIS 14496-15
  /// 3rd Edition. For these outputs, the service stores parameter set NAL units
  /// in the sample headers but not in the samples directly. For MP4 outputs, when
  /// you choose HVC1, your output video might not work properly with some
  /// downstream systems and video players. The service defaults to marking your
  /// output as HEV1. For these outputs, the service writes parameter set NAL
  /// units directly into the samples.
  final H265WriteMp4PackagingType? writeMp4PackagingType;

  H265Settings({
    this.adaptiveQuantization,
    this.alternateTransferFunctionSei,
    this.bandwidthReductionFilter,
    this.bitrate,
    this.codecLevel,
    this.codecProfile,
    this.dynamicSubGop,
    this.endOfStreamMarkers,
    this.flickerAdaptiveQuantization,
    this.framerateControl,
    this.framerateConversionAlgorithm,
    this.framerateDenominator,
    this.framerateNumerator,
    this.gopBReference,
    this.gopClosedCadence,
    this.gopSize,
    this.gopSizeUnits,
    this.hrdBufferFinalFillPercentage,
    this.hrdBufferInitialFillPercentage,
    this.hrdBufferSize,
    this.interlaceMode,
    this.maxBitrate,
    this.minIInterval,
    this.numberBFramesBetweenReferenceFrames,
    this.numberReferenceFrames,
    this.parControl,
    this.parDenominator,
    this.parNumerator,
    this.qualityTuningLevel,
    this.qvbrSettings,
    this.rateControlMode,
    this.sampleAdaptiveOffsetFilterMode,
    this.scanTypeConversionMode,
    this.sceneChangeDetect,
    this.slices,
    this.slowPal,
    this.spatialAdaptiveQuantization,
    this.telecine,
    this.temporalAdaptiveQuantization,
    this.temporalIds,
    this.tiles,
    this.unregisteredSeiTimecode,
    this.writeMp4PackagingType,
  });

  factory H265Settings.fromJson(Map<String, dynamic> json) {
    return H265Settings(
      adaptiveQuantization: (json['adaptiveQuantization'] as String?)
          ?.let(H265AdaptiveQuantization.fromString),
      alternateTransferFunctionSei:
          (json['alternateTransferFunctionSei'] as String?)
              ?.let(H265AlternateTransferFunctionSei.fromString),
      bandwidthReductionFilter: json['bandwidthReductionFilter'] != null
          ? BandwidthReductionFilter.fromJson(
              json['bandwidthReductionFilter'] as Map<String, dynamic>)
          : null,
      bitrate: json['bitrate'] as int?,
      codecLevel:
          (json['codecLevel'] as String?)?.let(H265CodecLevel.fromString),
      codecProfile:
          (json['codecProfile'] as String?)?.let(H265CodecProfile.fromString),
      dynamicSubGop:
          (json['dynamicSubGop'] as String?)?.let(H265DynamicSubGop.fromString),
      endOfStreamMarkers: (json['endOfStreamMarkers'] as String?)
          ?.let(H265EndOfStreamMarkers.fromString),
      flickerAdaptiveQuantization:
          (json['flickerAdaptiveQuantization'] as String?)
              ?.let(H265FlickerAdaptiveQuantization.fromString),
      framerateControl: (json['framerateControl'] as String?)
          ?.let(H265FramerateControl.fromString),
      framerateConversionAlgorithm:
          (json['framerateConversionAlgorithm'] as String?)
              ?.let(H265FramerateConversionAlgorithm.fromString),
      framerateDenominator: json['framerateDenominator'] as int?,
      framerateNumerator: json['framerateNumerator'] as int?,
      gopBReference:
          (json['gopBReference'] as String?)?.let(H265GopBReference.fromString),
      gopClosedCadence: json['gopClosedCadence'] as int?,
      gopSize: json['gopSize'] as double?,
      gopSizeUnits:
          (json['gopSizeUnits'] as String?)?.let(H265GopSizeUnits.fromString),
      hrdBufferFinalFillPercentage:
          json['hrdBufferFinalFillPercentage'] as int?,
      hrdBufferInitialFillPercentage:
          json['hrdBufferInitialFillPercentage'] as int?,
      hrdBufferSize: json['hrdBufferSize'] as int?,
      interlaceMode:
          (json['interlaceMode'] as String?)?.let(H265InterlaceMode.fromString),
      maxBitrate: json['maxBitrate'] as int?,
      minIInterval: json['minIInterval'] as int?,
      numberBFramesBetweenReferenceFrames:
          json['numberBFramesBetweenReferenceFrames'] as int?,
      numberReferenceFrames: json['numberReferenceFrames'] as int?,
      parControl:
          (json['parControl'] as String?)?.let(H265ParControl.fromString),
      parDenominator: json['parDenominator'] as int?,
      parNumerator: json['parNumerator'] as int?,
      qualityTuningLevel: (json['qualityTuningLevel'] as String?)
          ?.let(H265QualityTuningLevel.fromString),
      qvbrSettings: json['qvbrSettings'] != null
          ? H265QvbrSettings.fromJson(
              json['qvbrSettings'] as Map<String, dynamic>)
          : null,
      rateControlMode: (json['rateControlMode'] as String?)
          ?.let(H265RateControlMode.fromString),
      sampleAdaptiveOffsetFilterMode:
          (json['sampleAdaptiveOffsetFilterMode'] as String?)
              ?.let(H265SampleAdaptiveOffsetFilterMode.fromString),
      scanTypeConversionMode: (json['scanTypeConversionMode'] as String?)
          ?.let(H265ScanTypeConversionMode.fromString),
      sceneChangeDetect: (json['sceneChangeDetect'] as String?)
          ?.let(H265SceneChangeDetect.fromString),
      slices: json['slices'] as int?,
      slowPal: (json['slowPal'] as String?)?.let(H265SlowPal.fromString),
      spatialAdaptiveQuantization:
          (json['spatialAdaptiveQuantization'] as String?)
              ?.let(H265SpatialAdaptiveQuantization.fromString),
      telecine: (json['telecine'] as String?)?.let(H265Telecine.fromString),
      temporalAdaptiveQuantization:
          (json['temporalAdaptiveQuantization'] as String?)
              ?.let(H265TemporalAdaptiveQuantization.fromString),
      temporalIds:
          (json['temporalIds'] as String?)?.let(H265TemporalIds.fromString),
      tiles: (json['tiles'] as String?)?.let(H265Tiles.fromString),
      unregisteredSeiTimecode: (json['unregisteredSeiTimecode'] as String?)
          ?.let(H265UnregisteredSeiTimecode.fromString),
      writeMp4PackagingType: (json['writeMp4PackagingType'] as String?)
          ?.let(H265WriteMp4PackagingType.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final adaptiveQuantization = this.adaptiveQuantization;
    final alternateTransferFunctionSei = this.alternateTransferFunctionSei;
    final bandwidthReductionFilter = this.bandwidthReductionFilter;
    final bitrate = this.bitrate;
    final codecLevel = this.codecLevel;
    final codecProfile = this.codecProfile;
    final dynamicSubGop = this.dynamicSubGop;
    final endOfStreamMarkers = this.endOfStreamMarkers;
    final flickerAdaptiveQuantization = this.flickerAdaptiveQuantization;
    final framerateControl = this.framerateControl;
    final framerateConversionAlgorithm = this.framerateConversionAlgorithm;
    final framerateDenominator = this.framerateDenominator;
    final framerateNumerator = this.framerateNumerator;
    final gopBReference = this.gopBReference;
    final gopClosedCadence = this.gopClosedCadence;
    final gopSize = this.gopSize;
    final gopSizeUnits = this.gopSizeUnits;
    final hrdBufferFinalFillPercentage = this.hrdBufferFinalFillPercentage;
    final hrdBufferInitialFillPercentage = this.hrdBufferInitialFillPercentage;
    final hrdBufferSize = this.hrdBufferSize;
    final interlaceMode = this.interlaceMode;
    final maxBitrate = this.maxBitrate;
    final minIInterval = this.minIInterval;
    final numberBFramesBetweenReferenceFrames =
        this.numberBFramesBetweenReferenceFrames;
    final numberReferenceFrames = this.numberReferenceFrames;
    final parControl = this.parControl;
    final parDenominator = this.parDenominator;
    final parNumerator = this.parNumerator;
    final qualityTuningLevel = this.qualityTuningLevel;
    final qvbrSettings = this.qvbrSettings;
    final rateControlMode = this.rateControlMode;
    final sampleAdaptiveOffsetFilterMode = this.sampleAdaptiveOffsetFilterMode;
    final scanTypeConversionMode = this.scanTypeConversionMode;
    final sceneChangeDetect = this.sceneChangeDetect;
    final slices = this.slices;
    final slowPal = this.slowPal;
    final spatialAdaptiveQuantization = this.spatialAdaptiveQuantization;
    final telecine = this.telecine;
    final temporalAdaptiveQuantization = this.temporalAdaptiveQuantization;
    final temporalIds = this.temporalIds;
    final tiles = this.tiles;
    final unregisteredSeiTimecode = this.unregisteredSeiTimecode;
    final writeMp4PackagingType = this.writeMp4PackagingType;
    return {
      if (adaptiveQuantization != null)
        'adaptiveQuantization': adaptiveQuantization.value,
      if (alternateTransferFunctionSei != null)
        'alternateTransferFunctionSei': alternateTransferFunctionSei.value,
      if (bandwidthReductionFilter != null)
        'bandwidthReductionFilter': bandwidthReductionFilter,
      if (bitrate != null) 'bitrate': bitrate,
      if (codecLevel != null) 'codecLevel': codecLevel.value,
      if (codecProfile != null) 'codecProfile': codecProfile.value,
      if (dynamicSubGop != null) 'dynamicSubGop': dynamicSubGop.value,
      if (endOfStreamMarkers != null)
        'endOfStreamMarkers': endOfStreamMarkers.value,
      if (flickerAdaptiveQuantization != null)
        'flickerAdaptiveQuantization': flickerAdaptiveQuantization.value,
      if (framerateControl != null) 'framerateControl': framerateControl.value,
      if (framerateConversionAlgorithm != null)
        'framerateConversionAlgorithm': framerateConversionAlgorithm.value,
      if (framerateDenominator != null)
        'framerateDenominator': framerateDenominator,
      if (framerateNumerator != null) 'framerateNumerator': framerateNumerator,
      if (gopBReference != null) 'gopBReference': gopBReference.value,
      if (gopClosedCadence != null) 'gopClosedCadence': gopClosedCadence,
      if (gopSize != null) 'gopSize': gopSize,
      if (gopSizeUnits != null) 'gopSizeUnits': gopSizeUnits.value,
      if (hrdBufferFinalFillPercentage != null)
        'hrdBufferFinalFillPercentage': hrdBufferFinalFillPercentage,
      if (hrdBufferInitialFillPercentage != null)
        'hrdBufferInitialFillPercentage': hrdBufferInitialFillPercentage,
      if (hrdBufferSize != null) 'hrdBufferSize': hrdBufferSize,
      if (interlaceMode != null) 'interlaceMode': interlaceMode.value,
      if (maxBitrate != null) 'maxBitrate': maxBitrate,
      if (minIInterval != null) 'minIInterval': minIInterval,
      if (numberBFramesBetweenReferenceFrames != null)
        'numberBFramesBetweenReferenceFrames':
            numberBFramesBetweenReferenceFrames,
      if (numberReferenceFrames != null)
        'numberReferenceFrames': numberReferenceFrames,
      if (parControl != null) 'parControl': parControl.value,
      if (parDenominator != null) 'parDenominator': parDenominator,
      if (parNumerator != null) 'parNumerator': parNumerator,
      if (qualityTuningLevel != null)
        'qualityTuningLevel': qualityTuningLevel.value,
      if (qvbrSettings != null) 'qvbrSettings': qvbrSettings,
      if (rateControlMode != null) 'rateControlMode': rateControlMode.value,
      if (sampleAdaptiveOffsetFilterMode != null)
        'sampleAdaptiveOffsetFilterMode': sampleAdaptiveOffsetFilterMode.value,
      if (scanTypeConversionMode != null)
        'scanTypeConversionMode': scanTypeConversionMode.value,
      if (sceneChangeDetect != null)
        'sceneChangeDetect': sceneChangeDetect.value,
      if (slices != null) 'slices': slices,
      if (slowPal != null) 'slowPal': slowPal.value,
      if (spatialAdaptiveQuantization != null)
        'spatialAdaptiveQuantization': spatialAdaptiveQuantization.value,
      if (telecine != null) 'telecine': telecine.value,
      if (temporalAdaptiveQuantization != null)
        'temporalAdaptiveQuantization': temporalAdaptiveQuantization.value,
      if (temporalIds != null) 'temporalIds': temporalIds.value,
      if (tiles != null) 'tiles': tiles.value,
      if (unregisteredSeiTimecode != null)
        'unregisteredSeiTimecode': unregisteredSeiTimecode.value,
      if (writeMp4PackagingType != null)
        'writeMp4PackagingType': writeMp4PackagingType.value,
    };
  }
}

/// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
/// second (fps). Enable slow PAL to create a 25 fps output. When you enable
/// slow PAL, MediaConvert relabels the video frames to 25 fps and resamples
/// your audio to keep it synchronized with the video. Note that enabling this
/// setting will slightly reduce the duration of your video. Required settings:
/// You must also set Framerate to 25.
enum H265SlowPal {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const H265SlowPal(this.value);

  static H265SlowPal fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum H265SlowPal'));
}

/// Keep the default value, Enabled, to adjust quantization within each frame
/// based on spatial variation of content complexity. When you enable this
/// feature, the encoder uses fewer bits on areas that can sustain more
/// distortion with no noticeable visual degradation and uses more bits on areas
/// where any small distortion will be noticeable. For example, complex textured
/// blocks are encoded with fewer bits and smooth textured blocks are encoded
/// with more bits. Enabling this feature will almost always improve your video
/// quality. Note, though, that this feature doesn't take into account where the
/// viewer's attention is likely to be. If viewers are likely to be focusing
/// their attention on a part of the screen with a lot of complex texture, you
/// might choose to disable this feature. Related setting: When you enable
/// spatial adaptive quantization, set the value for Adaptive quantization
/// depending on your content. For homogeneous content, such as cartoons and
/// video games, set it to Low. For content with a wider variety of textures,
/// set it to High or Higher.
enum H265SpatialAdaptiveQuantization {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const H265SpatialAdaptiveQuantization(this.value);

  static H265SpatialAdaptiveQuantization fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H265SpatialAdaptiveQuantization'));
}

/// This field applies only if the Streams > Advanced > Framerate field is set
/// to 29.970. This field works with the Streams > Advanced > Preprocessors >
/// Deinterlacer field and the Streams > Advanced > Interlaced Mode field to
/// identify the scan type for the output: Progressive, Interlaced, Hard
/// Telecine or Soft Telecine. - Hard: produces 29.97i output from 23.976 input.
/// - Soft: produces 23.976; the player converts this output to 29.97i.
enum H265Telecine {
  none('NONE'),
  soft('SOFT'),
  hard('HARD'),
  ;

  final String value;

  const H265Telecine(this.value);

  static H265Telecine fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H265Telecine'));
}

/// Keep the default value, Enabled, to adjust quantization within each frame
/// based on temporal variation of content complexity. When you enable this
/// feature, the encoder uses fewer bits on areas of the frame that aren't
/// moving and uses more bits on complex objects with sharp edges that move a
/// lot. For example, this feature improves the readability of text tickers on
/// newscasts and scoreboards on sports matches. Enabling this feature will
/// almost always improve your video quality. Note, though, that this feature
/// doesn't take into account where the viewer's attention is likely to be. If
/// viewers are likely to be focusing their attention on a part of the screen
/// that doesn't have moving objects with sharp edges, such as sports athletes'
/// faces, you might choose to disable this feature. Related setting: When you
/// enable temporal quantization, adjust the strength of the filter with the
/// setting Adaptive quantization.
enum H265TemporalAdaptiveQuantization {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const H265TemporalAdaptiveQuantization(this.value);

  static H265TemporalAdaptiveQuantization fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H265TemporalAdaptiveQuantization'));
}

/// Enables temporal layer identifiers in the encoded bitstream. Up to 3 layers
/// are supported depending on GOP structure: I- and P-frames form one layer,
/// reference B-frames can form a second layer and non-reference b-frames can
/// form a third layer. Decoders can optionally decode only the lower temporal
/// layers to generate a lower frame rate output. For example, given a bitstream
/// with temporal IDs and with b-frames = 1 (i.e. IbPbPb display order), a
/// decoder could decode all the frames for full frame rate output or only the I
/// and P frames (lowest temporal layer) for a half frame rate output.
enum H265TemporalIds {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const H265TemporalIds(this.value);

  static H265TemporalIds fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum H265TemporalIds'));
}

/// Enable use of tiles, allowing horizontal as well as vertical subdivision of
/// the encoded pictures.
enum H265Tiles {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const H265Tiles(this.value);

  static H265Tiles fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum H265Tiles'));
}

/// Inserts timecode for each frame as 4 bytes of an unregistered SEI message.
enum H265UnregisteredSeiTimecode {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const H265UnregisteredSeiTimecode(this.value);

  static H265UnregisteredSeiTimecode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H265UnregisteredSeiTimecode'));
}

/// If the location of parameter set NAL units doesn't matter in your workflow,
/// ignore this setting. Use this setting only with CMAF or DASH outputs, or
/// with standalone file outputs in an MPEG-4 container (MP4 outputs). Choose
/// HVC1 to mark your output as HVC1. This makes your output compliant with the
/// following specification: ISO IECJTC1 SC29 N13798 Text ISO/IEC FDIS 14496-15
/// 3rd Edition. For these outputs, the service stores parameter set NAL units
/// in the sample headers but not in the samples directly. For MP4 outputs, when
/// you choose HVC1, your output video might not work properly with some
/// downstream systems and video players. The service defaults to marking your
/// output as HEV1. For these outputs, the service writes parameter set NAL
/// units directly into the samples.
enum H265WriteMp4PackagingType {
  hvc1('HVC1'),
  hev1('HEV1'),
  ;

  final String value;

  const H265WriteMp4PackagingType(this.value);

  static H265WriteMp4PackagingType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum H265WriteMp4PackagingType'));
}

/// Specify how MediaConvert maps brightness and colors from your HDR input to
/// your SDR output. The mode that you select represents a creative choice, with
/// different tradeoffs in the details and tones of your output. To maintain
/// details in bright or saturated areas of your output: Choose Preserve
/// details. For some sources, your SDR output may look less bright and less
/// saturated when compared to your HDR source. MediaConvert automatically
/// applies this mode for HLG sources, regardless of your choice. For a bright
/// and saturated output: Choose Vibrant. We recommend that you choose this mode
/// when any of your source content is HDR10, and for the best results when it
/// is mastered for 1000 nits. You may notice loss of details in bright or
/// saturated areas of your output. HDR to SDR tone mapping has no effect when
/// your input is SDR.
enum HDRToSDRToneMapper {
  preserveDetails('PRESERVE_DETAILS'),
  vibrant('VIBRANT'),
  ;

  final String value;

  const HDRToSDRToneMapper(this.value);

  static HDRToSDRToneMapper fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum HDRToSDRToneMapper'));
}

/// Use these settings to specify static color calibration metadata, as defined
/// by SMPTE ST 2086. These values don't affect the pixel values that are
/// encoded in the video stream. They are intended to help the downstream video
/// player display content in a way that reflects the intentions of the the
/// content creator.
class Hdr10Metadata {
  /// HDR Master Display Information must be provided by a color grader, using
  /// color grading tools. Range is 0 to 50,000, each increment represents 0.00002
  /// in CIE1931 color coordinate. Note that this setting is not for color
  /// correction.
  final int? bluePrimaryX;

  /// HDR Master Display Information must be provided by a color grader, using
  /// color grading tools. Range is 0 to 50,000, each increment represents 0.00002
  /// in CIE1931 color coordinate. Note that this setting is not for color
  /// correction.
  final int? bluePrimaryY;

  /// HDR Master Display Information must be provided by a color grader, using
  /// color grading tools. Range is 0 to 50,000, each increment represents 0.00002
  /// in CIE1931 color coordinate. Note that this setting is not for color
  /// correction.
  final int? greenPrimaryX;

  /// HDR Master Display Information must be provided by a color grader, using
  /// color grading tools. Range is 0 to 50,000, each increment represents 0.00002
  /// in CIE1931 color coordinate. Note that this setting is not for color
  /// correction.
  final int? greenPrimaryY;

  /// Maximum light level among all samples in the coded video sequence, in units
  /// of candelas per square meter. This setting doesn't have a default value; you
  /// must specify a value that is suitable for the content.
  final int? maxContentLightLevel;

  /// Maximum average light level of any frame in the coded video sequence, in
  /// units of candelas per square meter. This setting doesn't have a default
  /// value; you must specify a value that is suitable for the content.
  final int? maxFrameAverageLightLevel;

  /// Nominal maximum mastering display luminance in units of of 0.0001 candelas
  /// per square meter.
  final int? maxLuminance;

  /// Nominal minimum mastering display luminance in units of of 0.0001 candelas
  /// per square meter
  final int? minLuminance;

  /// HDR Master Display Information must be provided by a color grader, using
  /// color grading tools. Range is 0 to 50,000, each increment represents 0.00002
  /// in CIE1931 color coordinate. Note that this setting is not for color
  /// correction.
  final int? redPrimaryX;

  /// HDR Master Display Information must be provided by a color grader, using
  /// color grading tools. Range is 0 to 50,000, each increment represents 0.00002
  /// in CIE1931 color coordinate. Note that this setting is not for color
  /// correction.
  final int? redPrimaryY;

  /// HDR Master Display Information must be provided by a color grader, using
  /// color grading tools. Range is 0 to 50,000, each increment represents 0.00002
  /// in CIE1931 color coordinate. Note that this setting is not for color
  /// correction.
  final int? whitePointX;

  /// HDR Master Display Information must be provided by a color grader, using
  /// color grading tools. Range is 0 to 50,000, each increment represents 0.00002
  /// in CIE1931 color coordinate. Note that this setting is not for color
  /// correction.
  final int? whitePointY;

  Hdr10Metadata({
    this.bluePrimaryX,
    this.bluePrimaryY,
    this.greenPrimaryX,
    this.greenPrimaryY,
    this.maxContentLightLevel,
    this.maxFrameAverageLightLevel,
    this.maxLuminance,
    this.minLuminance,
    this.redPrimaryX,
    this.redPrimaryY,
    this.whitePointX,
    this.whitePointY,
  });

  factory Hdr10Metadata.fromJson(Map<String, dynamic> json) {
    return Hdr10Metadata(
      bluePrimaryX: json['bluePrimaryX'] as int?,
      bluePrimaryY: json['bluePrimaryY'] as int?,
      greenPrimaryX: json['greenPrimaryX'] as int?,
      greenPrimaryY: json['greenPrimaryY'] as int?,
      maxContentLightLevel: json['maxContentLightLevel'] as int?,
      maxFrameAverageLightLevel: json['maxFrameAverageLightLevel'] as int?,
      maxLuminance: json['maxLuminance'] as int?,
      minLuminance: json['minLuminance'] as int?,
      redPrimaryX: json['redPrimaryX'] as int?,
      redPrimaryY: json['redPrimaryY'] as int?,
      whitePointX: json['whitePointX'] as int?,
      whitePointY: json['whitePointY'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final bluePrimaryX = this.bluePrimaryX;
    final bluePrimaryY = this.bluePrimaryY;
    final greenPrimaryX = this.greenPrimaryX;
    final greenPrimaryY = this.greenPrimaryY;
    final maxContentLightLevel = this.maxContentLightLevel;
    final maxFrameAverageLightLevel = this.maxFrameAverageLightLevel;
    final maxLuminance = this.maxLuminance;
    final minLuminance = this.minLuminance;
    final redPrimaryX = this.redPrimaryX;
    final redPrimaryY = this.redPrimaryY;
    final whitePointX = this.whitePointX;
    final whitePointY = this.whitePointY;
    return {
      if (bluePrimaryX != null) 'bluePrimaryX': bluePrimaryX,
      if (bluePrimaryY != null) 'bluePrimaryY': bluePrimaryY,
      if (greenPrimaryX != null) 'greenPrimaryX': greenPrimaryX,
      if (greenPrimaryY != null) 'greenPrimaryY': greenPrimaryY,
      if (maxContentLightLevel != null)
        'maxContentLightLevel': maxContentLightLevel,
      if (maxFrameAverageLightLevel != null)
        'maxFrameAverageLightLevel': maxFrameAverageLightLevel,
      if (maxLuminance != null) 'maxLuminance': maxLuminance,
      if (minLuminance != null) 'minLuminance': minLuminance,
      if (redPrimaryX != null) 'redPrimaryX': redPrimaryX,
      if (redPrimaryY != null) 'redPrimaryY': redPrimaryY,
      if (whitePointX != null) 'whitePointX': whitePointX,
      if (whitePointY != null) 'whitePointY': whitePointY,
    };
  }
}

/// Setting for HDR10+ metadata insertion
class Hdr10Plus {
  /// Specify the HDR10+ mastering display normalized peak luminance, in nits.
  /// This is the normalized actual peak luminance of the mastering display, as
  /// defined by ST 2094-40.
  final int? masteringMonitorNits;

  /// Specify the HDR10+ target display nominal peak luminance, in nits. This is
  /// the nominal maximum luminance of the target display as defined by ST
  /// 2094-40.
  final int? targetMonitorNits;

  Hdr10Plus({
    this.masteringMonitorNits,
    this.targetMonitorNits,
  });

  factory Hdr10Plus.fromJson(Map<String, dynamic> json) {
    return Hdr10Plus(
      masteringMonitorNits: json['masteringMonitorNits'] as int?,
      targetMonitorNits: json['targetMonitorNits'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final masteringMonitorNits = this.masteringMonitorNits;
    final targetMonitorNits = this.targetMonitorNits;
    return {
      if (masteringMonitorNits != null)
        'masteringMonitorNits': masteringMonitorNits,
      if (targetMonitorNits != null) 'targetMonitorNits': targetMonitorNits,
    };
  }
}

/// Ad marker for Apple HLS manifest.
enum HlsAdMarkers {
  elemental('ELEMENTAL'),
  elementalScte35('ELEMENTAL_SCTE35'),
  ;

  final String value;

  const HlsAdMarkers(this.value);

  static HlsAdMarkers fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum HlsAdMarkers'));
}

/// Specify the details for each additional HLS manifest that you want the
/// service to generate for this output group. Each manifest can reference a
/// different subset of outputs in the group.
class HlsAdditionalManifest {
  /// Specify a name modifier that the service adds to the name of this manifest
  /// to make it different from the file names of the other main manifests in the
  /// output group. For example, say that the default main manifest for your HLS
  /// group is film-name.m3u8. If you enter "-no-premium" for this setting, then
  /// the file name the service generates for this top-level manifest is
  /// film-name-no-premium.m3u8. For HLS output groups, specify a
  /// manifestNameModifier that is different from the nameModifier of the output.
  /// The service uses the output name modifier to create unique names for the
  /// individual variant manifests.
  final String? manifestNameModifier;

  /// Specify the outputs that you want this additional top-level manifest to
  /// reference.
  final List<String>? selectedOutputs;

  HlsAdditionalManifest({
    this.manifestNameModifier,
    this.selectedOutputs,
  });

  factory HlsAdditionalManifest.fromJson(Map<String, dynamic> json) {
    return HlsAdditionalManifest(
      manifestNameModifier: json['manifestNameModifier'] as String?,
      selectedOutputs: (json['selectedOutputs'] as List?)
          ?.nonNulls
          .map((e) => e as String)
          .toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final manifestNameModifier = this.manifestNameModifier;
    final selectedOutputs = this.selectedOutputs;
    return {
      if (manifestNameModifier != null)
        'manifestNameModifier': manifestNameModifier,
      if (selectedOutputs != null) 'selectedOutputs': selectedOutputs,
    };
  }
}

/// Use this setting only in audio-only outputs. Choose MPEG-2 Transport Stream
/// (M2TS) to create a file in an MPEG2-TS container. Keep the default value
/// Automatic to create a raw audio-only file with no container. Regardless of
/// the value that you specify here, if this output has video, the service will
/// place outputs into an MPEG2-TS container.
enum HlsAudioOnlyContainer {
  automatic('AUTOMATIC'),
  m2ts('M2TS'),
  ;

  final String value;

  const HlsAudioOnlyContainer(this.value);

  static HlsAudioOnlyContainer fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum HlsAudioOnlyContainer'));
}

/// Ignore this setting unless you are using FairPlay DRM with Verimatrix and
/// you encounter playback issues. Keep the default value, Include, to output
/// audio-only headers. Choose Exclude to remove the audio-only headers from
/// your audio segments.
enum HlsAudioOnlyHeader {
  include('INCLUDE'),
  exclude('EXCLUDE'),
  ;

  final String value;

  const HlsAudioOnlyHeader(this.value);

  static HlsAudioOnlyHeader fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum HlsAudioOnlyHeader'));
}

/// Four types of audio-only tracks are supported: Audio-Only Variant Stream The
/// client can play back this audio-only stream instead of video in
/// low-bandwidth scenarios. Represented as an EXT-X-STREAM-INF in the HLS
/// manifest. Alternate Audio, Auto Select, Default Alternate rendition that the
/// client should try to play back by default. Represented as an EXT-X-MEDIA in
/// the HLS manifest with DEFAULT=YES, AUTOSELECT=YES Alternate Audio, Auto
/// Select, Not Default Alternate rendition that the client may try to play back
/// by default. Represented as an EXT-X-MEDIA in the HLS manifest with
/// DEFAULT=NO, AUTOSELECT=YES Alternate Audio, not Auto Select Alternate
/// rendition that the client will not try to play back by default. Represented
/// as an EXT-X-MEDIA in the HLS manifest with DEFAULT=NO, AUTOSELECT=NO
enum HlsAudioTrackType {
  alternateAudioAutoSelectDefault('ALTERNATE_AUDIO_AUTO_SELECT_DEFAULT'),
  alternateAudioAutoSelect('ALTERNATE_AUDIO_AUTO_SELECT'),
  alternateAudioNotAutoSelect('ALTERNATE_AUDIO_NOT_AUTO_SELECT'),
  audioOnlyVariantStream('AUDIO_ONLY_VARIANT_STREAM'),
  ;

  final String value;

  const HlsAudioTrackType(this.value);

  static HlsAudioTrackType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum HlsAudioTrackType'));
}

/// Caption Language Mapping
class HlsCaptionLanguageMapping {
  /// Caption channel.
  final int? captionChannel;

  /// Specify the language for this captions channel, using the ISO 639-2 or ISO
  /// 639-3 three-letter language code
  final String? customLanguageCode;

  /// Specify the language, using the ISO 639-2 three-letter code listed at
  /// https://www.loc.gov/standards/iso639-2/php/code_list.php.
  final LanguageCode? languageCode;

  /// Caption language description.
  final String? languageDescription;

  HlsCaptionLanguageMapping({
    this.captionChannel,
    this.customLanguageCode,
    this.languageCode,
    this.languageDescription,
  });

  factory HlsCaptionLanguageMapping.fromJson(Map<String, dynamic> json) {
    return HlsCaptionLanguageMapping(
      captionChannel: json['captionChannel'] as int?,
      customLanguageCode: json['customLanguageCode'] as String?,
      languageCode:
          (json['languageCode'] as String?)?.let(LanguageCode.fromString),
      languageDescription: json['languageDescription'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final captionChannel = this.captionChannel;
    final customLanguageCode = this.customLanguageCode;
    final languageCode = this.languageCode;
    final languageDescription = this.languageDescription;
    return {
      if (captionChannel != null) 'captionChannel': captionChannel,
      if (customLanguageCode != null) 'customLanguageCode': customLanguageCode,
      if (languageCode != null) 'languageCode': languageCode.value,
      if (languageDescription != null)
        'languageDescription': languageDescription,
    };
  }
}

/// Applies only to 608 Embedded output captions. Insert: Include
/// CLOSED-CAPTIONS lines in the manifest. Specify at least one language in the
/// CC1 Language Code field. One CLOSED-CAPTION line is added for each Language
/// Code you specify. Make sure to specify the languages in the order in which
/// they appear in the original source (if the source is embedded format) or the
/// order of the caption selectors (if the source is other than embedded).
/// Otherwise, languages in the manifest will not match up properly with the
/// output captions. None: Include CLOSED-CAPTIONS=NONE line in the manifest.
/// Omit: Omit any CLOSED-CAPTIONS line from the manifest.
enum HlsCaptionLanguageSetting {
  insert('INSERT'),
  omit('OMIT'),
  none('NONE'),
  ;

  final String value;

  const HlsCaptionLanguageSetting(this.value);

  static HlsCaptionLanguageSetting fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum HlsCaptionLanguageSetting'));
}

/// Set Caption segment length control to Match video to create caption segments
/// that align with the video segments from the first video output in this
/// output group. For example, if the video segments are 2 seconds long, your
/// WebVTT segments will also be 2 seconds long. Keep the default setting, Large
/// segments to create caption segments that are 300 seconds long.
enum HlsCaptionSegmentLengthControl {
  largeSegments('LARGE_SEGMENTS'),
  matchVideo('MATCH_VIDEO'),
  ;

  final String value;

  const HlsCaptionSegmentLengthControl(this.value);

  static HlsCaptionSegmentLengthControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum HlsCaptionSegmentLengthControl'));
}

/// Disable this setting only when your workflow requires the
/// #EXT-X-ALLOW-CACHE:no tag. Otherwise, keep the default value Enabled and
/// control caching in your video distribution set up. For example, use the
/// Cache-Control http header.
enum HlsClientCache {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const HlsClientCache(this.value);

  static HlsClientCache fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum HlsClientCache'));
}

/// Specification to use (RFC-6381 or the default RFC-4281) during m3u8 playlist
/// generation.
enum HlsCodecSpecification {
  rfc_6381('RFC_6381'),
  rfc_4281('RFC_4281'),
  ;

  final String value;

  const HlsCodecSpecification(this.value);

  static HlsCodecSpecification fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum HlsCodecSpecification'));
}

/// Specify whether to flag this audio track as descriptive video service (DVS)
/// in your HLS parent manifest. When you choose Flag, MediaConvert includes the
/// parameter CHARACTERISTICS="public.accessibility.describes-video" in the
/// EXT-X-MEDIA entry for this track. When you keep the default choice, Don't
/// flag, MediaConvert leaves this parameter out. The DVS flag can help with
/// accessibility on Apple devices. For more information, see the Apple
/// documentation.
enum HlsDescriptiveVideoServiceFlag {
  dontFlag('DONT_FLAG'),
  flag('FLAG'),
  ;

  final String value;

  const HlsDescriptiveVideoServiceFlag(this.value);

  static HlsDescriptiveVideoServiceFlag fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum HlsDescriptiveVideoServiceFlag'));
}

/// Indicates whether segments should be placed in subdirectories.
enum HlsDirectoryStructure {
  singleDirectory('SINGLE_DIRECTORY'),
  subdirectoryPerStream('SUBDIRECTORY_PER_STREAM'),
  ;

  final String value;

  const HlsDirectoryStructure(this.value);

  static HlsDirectoryStructure fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum HlsDirectoryStructure'));
}

/// Settings for HLS encryption
class HlsEncryptionSettings {
  /// This is a 128-bit, 16-byte hex value represented by a 32-character text
  /// string. If this parameter is not set then the Initialization Vector will
  /// follow the segment number by default.
  final String? constantInitializationVector;

  /// Encrypts the segments with the given encryption scheme. Leave blank to
  /// disable. Selecting 'Disabled' in the web interface also disables encryption.
  final HlsEncryptionType? encryptionMethod;

  /// The Initialization Vector is a 128-bit number used in conjunction with the
  /// key for encrypting blocks. If set to INCLUDE, Initialization Vector is
  /// listed in the manifest. Otherwise Initialization Vector is not in the
  /// manifest.
  final HlsInitializationVectorInManifest? initializationVectorInManifest;

  /// Enable this setting to insert the EXT-X-SESSION-KEY element into the master
  /// playlist. This allows for offline Apple HLS FairPlay content protection.
  final HlsOfflineEncrypted? offlineEncrypted;

  /// If your output group type is HLS, DASH, or Microsoft Smooth, use these
  /// settings when doing DRM encryption with a SPEKE-compliant key provider. If
  /// your output group type is CMAF, use the SpekeKeyProviderCmaf settings
  /// instead.
  final SpekeKeyProvider? spekeKeyProvider;

  /// Use these settings to set up encryption with a static key provider.
  final StaticKeyProvider? staticKeyProvider;

  /// Specify whether your DRM encryption key is static or from a key provider
  /// that follows the SPEKE standard. For more information about SPEKE, see
  /// https://docs.aws.amazon.com/speke/latest/documentation/what-is-speke.html.
  final HlsKeyProviderType? type;

  HlsEncryptionSettings({
    this.constantInitializationVector,
    this.encryptionMethod,
    this.initializationVectorInManifest,
    this.offlineEncrypted,
    this.spekeKeyProvider,
    this.staticKeyProvider,
    this.type,
  });

  factory HlsEncryptionSettings.fromJson(Map<String, dynamic> json) {
    return HlsEncryptionSettings(
      constantInitializationVector:
          json['constantInitializationVector'] as String?,
      encryptionMethod: (json['encryptionMethod'] as String?)
          ?.let(HlsEncryptionType.fromString),
      initializationVectorInManifest:
          (json['initializationVectorInManifest'] as String?)
              ?.let(HlsInitializationVectorInManifest.fromString),
      offlineEncrypted: (json['offlineEncrypted'] as String?)
          ?.let(HlsOfflineEncrypted.fromString),
      spekeKeyProvider: json['spekeKeyProvider'] != null
          ? SpekeKeyProvider.fromJson(
              json['spekeKeyProvider'] as Map<String, dynamic>)
          : null,
      staticKeyProvider: json['staticKeyProvider'] != null
          ? StaticKeyProvider.fromJson(
              json['staticKeyProvider'] as Map<String, dynamic>)
          : null,
      type: (json['type'] as String?)?.let(HlsKeyProviderType.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final constantInitializationVector = this.constantInitializationVector;
    final encryptionMethod = this.encryptionMethod;
    final initializationVectorInManifest = this.initializationVectorInManifest;
    final offlineEncrypted = this.offlineEncrypted;
    final spekeKeyProvider = this.spekeKeyProvider;
    final staticKeyProvider = this.staticKeyProvider;
    final type = this.type;
    return {
      if (constantInitializationVector != null)
        'constantInitializationVector': constantInitializationVector,
      if (encryptionMethod != null) 'encryptionMethod': encryptionMethod.value,
      if (initializationVectorInManifest != null)
        'initializationVectorInManifest': initializationVectorInManifest.value,
      if (offlineEncrypted != null) 'offlineEncrypted': offlineEncrypted.value,
      if (spekeKeyProvider != null) 'spekeKeyProvider': spekeKeyProvider,
      if (staticKeyProvider != null) 'staticKeyProvider': staticKeyProvider,
      if (type != null) 'type': type.value,
    };
  }
}

/// Encrypts the segments with the given encryption scheme. Leave blank to
/// disable. Selecting 'Disabled' in the web interface also disables encryption.
enum HlsEncryptionType {
  aes128('AES128'),
  sampleAes('SAMPLE_AES'),
  ;

  final String value;

  const HlsEncryptionType(this.value);

  static HlsEncryptionType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum HlsEncryptionType'));
}

/// Settings related to your HLS output package. For more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/outputs-file-ABR.html.
class HlsGroupSettings {
  /// Choose one or more ad marker types to decorate your Apple HLS manifest. This
  /// setting does not determine whether SCTE-35 markers appear in the outputs
  /// themselves.
  final List<HlsAdMarkers>? adMarkers;

  /// By default, the service creates one top-level .m3u8 HLS manifest for each
  /// HLS output group in your job. This default manifest references every output
  /// in the output group. To create additional top-level manifests that reference
  /// a subset of the outputs in the output group, specify a list of them here.
  final List<HlsAdditionalManifest>? additionalManifests;

  /// Ignore this setting unless you are using FairPlay DRM with Verimatrix and
  /// you encounter playback issues. Keep the default value, Include, to output
  /// audio-only headers. Choose Exclude to remove the audio-only headers from
  /// your audio segments.
  final HlsAudioOnlyHeader? audioOnlyHeader;

  /// A partial URI prefix that will be prepended to each output in the media
  /// .m3u8 file. Can be used if base manifest is delivered from a different URL
  /// than the main .m3u8 file.
  final String? baseUrl;

  /// Language to be used on Caption outputs
  final List<HlsCaptionLanguageMapping>? captionLanguageMappings;

  /// Applies only to 608 Embedded output captions. Insert: Include
  /// CLOSED-CAPTIONS lines in the manifest. Specify at least one language in the
  /// CC1 Language Code field. One CLOSED-CAPTION line is added for each Language
  /// Code you specify. Make sure to specify the languages in the order in which
  /// they appear in the original source (if the source is embedded format) or the
  /// order of the caption selectors (if the source is other than embedded).
  /// Otherwise, languages in the manifest will not match up properly with the
  /// output captions. None: Include CLOSED-CAPTIONS=NONE line in the manifest.
  /// Omit: Omit any CLOSED-CAPTIONS line from the manifest.
  final HlsCaptionLanguageSetting? captionLanguageSetting;

  /// Set Caption segment length control to Match video to create caption segments
  /// that align with the video segments from the first video output in this
  /// output group. For example, if the video segments are 2 seconds long, your
  /// WebVTT segments will also be 2 seconds long. Keep the default setting, Large
  /// segments to create caption segments that are 300 seconds long.
  final HlsCaptionSegmentLengthControl? captionSegmentLengthControl;

  /// Disable this setting only when your workflow requires the
  /// #EXT-X-ALLOW-CACHE:no tag. Otherwise, keep the default value Enabled and
  /// control caching in your video distribution set up. For example, use the
  /// Cache-Control http header.
  final HlsClientCache? clientCache;

  /// Specification to use (RFC-6381 or the default RFC-4281) during m3u8 playlist
  /// generation.
  final HlsCodecSpecification? codecSpecification;

  /// Use Destination to specify the S3 output location and the output filename
  /// base. Destination accepts format identifiers. If you do not specify the base
  /// filename in the URI, the service will use the filename of the input file. If
  /// your job has multiple inputs, the service uses the filename of the first
  /// input file.
  final String? destination;

  /// Settings associated with the destination. Will vary based on the type of
  /// destination
  final DestinationSettings? destinationSettings;

  /// Indicates whether segments should be placed in subdirectories.
  final HlsDirectoryStructure? directoryStructure;

  /// DRM settings.
  final HlsEncryptionSettings? encryption;

  /// Specify whether MediaConvert generates images for trick play. Keep the
  /// default value, None, to not generate any images. Choose Thumbnail to
  /// generate tiled thumbnails. Choose Thumbnail and full frame to generate tiled
  /// thumbnails and full-resolution images of single frames. MediaConvert creates
  /// a child manifest for each set of images that you generate and adds
  /// corresponding entries to the parent manifest. A common application for these
  /// images is Roku trick mode. The thumbnails and full-frame images that
  /// MediaConvert creates with this feature are compatible with this Roku
  /// specification:
  /// https://developer.roku.com/docs/developer-program/media-playback/trick-mode/hls-and-dash.md
  final HlsImageBasedTrickPlay? imageBasedTrickPlay;

  /// Tile and thumbnail settings applicable when imageBasedTrickPlay is ADVANCED
  final HlsImageBasedTrickPlaySettings? imageBasedTrickPlaySettings;

  /// When set to GZIP, compresses HLS playlist.
  final HlsManifestCompression? manifestCompression;

  /// Indicates whether the output manifest should use floating point values for
  /// segment duration.
  final HlsManifestDurationFormat? manifestDurationFormat;

  /// Keep this setting at the default value of 0, unless you are troubleshooting
  /// a problem with how devices play back the end of your video asset. If you
  /// know that player devices are hanging on the final segment of your video
  /// because the length of your final segment is too short, use this setting to
  /// specify a minimum final segment length, in seconds. Choose a value that is
  /// greater than or equal to 1 and less than your segment length. When you
  /// specify a value for this setting, the encoder will combine any final segment
  /// that is shorter than the length that you specify with the previous segment.
  /// For example, your segment length is 3 seconds and your final segment is .5
  /// seconds without a minimum final segment length; when you set the minimum
  /// final segment length to 1, your final segment is 3.5 seconds.
  final double? minFinalSegmentLength;

  /// When set, Minimum Segment Size is enforced by looking ahead and back within
  /// the specified range for a nearby avail and extending the segment size if
  /// needed.
  final int? minSegmentLength;

  /// Indicates whether the .m3u8 manifest file should be generated for this HLS
  /// output group.
  final HlsOutputSelection? outputSelection;

  /// Includes or excludes EXT-X-PROGRAM-DATE-TIME tag in .m3u8 manifest files.
  /// The value is calculated as follows: either the program date and time are
  /// initialized using the input timecode source, or the time is initialized
  /// using the input timecode source and the date is initialized using the
  /// timestamp_offset.
  final HlsProgramDateTime? programDateTime;

  /// Period of insertion of EXT-X-PROGRAM-DATE-TIME entry, in seconds.
  final int? programDateTimePeriod;

  /// Specify whether MediaConvert generates HLS manifests while your job is
  /// running or when your job is complete. To generate HLS manifests while your
  /// job is running: Choose Enabled. Use if you want to play back your content as
  /// soon as it's available. MediaConvert writes the parent and child manifests
  /// after the first three media segments are written to your destination S3
  /// bucket. It then writes new updated manifests after each additional segment
  /// is written. The parent manifest includes the latest BANDWIDTH and
  /// AVERAGE-BANDWIDTH attributes, and child manifests include the latest
  /// available media segment. When your job completes, the final child playlists
  /// include an EXT-X-ENDLIST tag. To generate HLS manifests only when your job
  /// completes: Choose Disabled.
  final HlsProgressiveWriteHlsManifest? progressiveWriteHlsManifest;

  /// When set to SINGLE_FILE, emits program as a single media resource (.ts)
  /// file, uses #EXT-X-BYTERANGE tags to index segment for playback.
  final HlsSegmentControl? segmentControl;

  /// Specify the length, in whole seconds, of each segment. When you don't
  /// specify a value, MediaConvert defaults to 10. Related settings: Use Segment
  /// length control to specify whether the encoder enforces this value strictly.
  /// Use Segment control to specify whether MediaConvert creates separate segment
  /// files or one content file that has metadata to mark the segment boundaries.
  final int? segmentLength;

  /// Specify how you want MediaConvert to determine the segment length. Choose
  /// Exact to have the encoder use the exact length that you specify with the
  /// setting Segment length. This might result in extra I-frames. Choose Multiple
  /// of GOP to have the encoder round up the segment lengths to match the next
  /// GOP boundary.
  final HlsSegmentLengthControl? segmentLengthControl;

  /// Specify the number of segments to write to a subdirectory before starting a
  /// new one. You must also set Directory structure to Subdirectory per stream
  /// for this setting to have an effect.
  final int? segmentsPerSubdirectory;

  /// Include or exclude RESOLUTION attribute for video in EXT-X-STREAM-INF tag of
  /// variant manifest.
  final HlsStreamInfResolution? streamInfResolution;

  /// When set to LEGACY, the segment target duration is always rounded up to the
  /// nearest integer value above its current value in seconds. When set to
  /// SPEC\\_COMPLIANT, the segment target duration is rounded up to the nearest
  /// integer value if fraction seconds are greater than or equal to 0.5 (>= 0.5)
  /// and rounded down if less than 0.5 (< 0.5). You may need to use LEGACY if
  /// your client needs to ensure that the target duration is always longer than
  /// the actual duration of the segment. Some older players may experience
  /// interrupted playback when the actual duration of a track in a segment is
  /// longer than the target duration.
  final HlsTargetDurationCompatibilityMode? targetDurationCompatibilityMode;

  /// Specify the type of the ID3 frame to use for ID3 timestamps in your output.
  /// To include ID3 timestamps: Specify PRIV or TDRL and set ID3 metadata to
  /// Passthrough. To exclude ID3 timestamps: Set ID3 timestamp frame type to
  /// None.
  final HlsTimedMetadataId3Frame? timedMetadataId3Frame;

  /// Specify the interval in seconds to write ID3 timestamps in your output. The
  /// first timestamp starts at the output timecode and date, and increases
  /// incrementally with each ID3 timestamp. To use the default interval of 10
  /// seconds: Leave blank. To include this metadata in your output: Set ID3
  /// timestamp frame type to PRIV or TDRL, and set ID3 metadata to Passthrough.
  final int? timedMetadataId3Period;

  /// Provides an extra millisecond delta offset to fine tune the timestamps.
  final int? timestampDeltaMilliseconds;

  HlsGroupSettings({
    this.adMarkers,
    this.additionalManifests,
    this.audioOnlyHeader,
    this.baseUrl,
    this.captionLanguageMappings,
    this.captionLanguageSetting,
    this.captionSegmentLengthControl,
    this.clientCache,
    this.codecSpecification,
    this.destination,
    this.destinationSettings,
    this.directoryStructure,
    this.encryption,
    this.imageBasedTrickPlay,
    this.imageBasedTrickPlaySettings,
    this.manifestCompression,
    this.manifestDurationFormat,
    this.minFinalSegmentLength,
    this.minSegmentLength,
    this.outputSelection,
    this.programDateTime,
    this.programDateTimePeriod,
    this.progressiveWriteHlsManifest,
    this.segmentControl,
    this.segmentLength,
    this.segmentLengthControl,
    this.segmentsPerSubdirectory,
    this.streamInfResolution,
    this.targetDurationCompatibilityMode,
    this.timedMetadataId3Frame,
    this.timedMetadataId3Period,
    this.timestampDeltaMilliseconds,
  });

  factory HlsGroupSettings.fromJson(Map<String, dynamic> json) {
    return HlsGroupSettings(
      adMarkers: (json['adMarkers'] as List?)
          ?.nonNulls
          .map((e) => HlsAdMarkers.fromString((e as String)))
          .toList(),
      additionalManifests: (json['additionalManifests'] as List?)
          ?.nonNulls
          .map((e) => HlsAdditionalManifest.fromJson(e as Map<String, dynamic>))
          .toList(),
      audioOnlyHeader: (json['audioOnlyHeader'] as String?)
          ?.let(HlsAudioOnlyHeader.fromString),
      baseUrl: json['baseUrl'] as String?,
      captionLanguageMappings: (json['captionLanguageMappings'] as List?)
          ?.nonNulls
          .map((e) =>
              HlsCaptionLanguageMapping.fromJson(e as Map<String, dynamic>))
          .toList(),
      captionLanguageSetting: (json['captionLanguageSetting'] as String?)
          ?.let(HlsCaptionLanguageSetting.fromString),
      captionSegmentLengthControl:
          (json['captionSegmentLengthControl'] as String?)
              ?.let(HlsCaptionSegmentLengthControl.fromString),
      clientCache:
          (json['clientCache'] as String?)?.let(HlsClientCache.fromString),
      codecSpecification: (json['codecSpecification'] as String?)
          ?.let(HlsCodecSpecification.fromString),
      destination: json['destination'] as String?,
      destinationSettings: json['destinationSettings'] != null
          ? DestinationSettings.fromJson(
              json['destinationSettings'] as Map<String, dynamic>)
          : null,
      directoryStructure: (json['directoryStructure'] as String?)
          ?.let(HlsDirectoryStructure.fromString),
      encryption: json['encryption'] != null
          ? HlsEncryptionSettings.fromJson(
              json['encryption'] as Map<String, dynamic>)
          : null,
      imageBasedTrickPlay: (json['imageBasedTrickPlay'] as String?)
          ?.let(HlsImageBasedTrickPlay.fromString),
      imageBasedTrickPlaySettings: json['imageBasedTrickPlaySettings'] != null
          ? HlsImageBasedTrickPlaySettings.fromJson(
              json['imageBasedTrickPlaySettings'] as Map<String, dynamic>)
          : null,
      manifestCompression: (json['manifestCompression'] as String?)
          ?.let(HlsManifestCompression.fromString),
      manifestDurationFormat: (json['manifestDurationFormat'] as String?)
          ?.let(HlsManifestDurationFormat.fromString),
      minFinalSegmentLength: json['minFinalSegmentLength'] as double?,
      minSegmentLength: json['minSegmentLength'] as int?,
      outputSelection: (json['outputSelection'] as String?)
          ?.let(HlsOutputSelection.fromString),
      programDateTime: (json['programDateTime'] as String?)
          ?.let(HlsProgramDateTime.fromString),
      programDateTimePeriod: json['programDateTimePeriod'] as int?,
      progressiveWriteHlsManifest:
          (json['progressiveWriteHlsManifest'] as String?)
              ?.let(HlsProgressiveWriteHlsManifest.fromString),
      segmentControl: (json['segmentControl'] as String?)
          ?.let(HlsSegmentControl.fromString),
      segmentLength: json['segmentLength'] as int?,
      segmentLengthControl: (json['segmentLengthControl'] as String?)
          ?.let(HlsSegmentLengthControl.fromString),
      segmentsPerSubdirectory: json['segmentsPerSubdirectory'] as int?,
      streamInfResolution: (json['streamInfResolution'] as String?)
          ?.let(HlsStreamInfResolution.fromString),
      targetDurationCompatibilityMode:
          (json['targetDurationCompatibilityMode'] as String?)
              ?.let(HlsTargetDurationCompatibilityMode.fromString),
      timedMetadataId3Frame: (json['timedMetadataId3Frame'] as String?)
          ?.let(HlsTimedMetadataId3Frame.fromString),
      timedMetadataId3Period: json['timedMetadataId3Period'] as int?,
      timestampDeltaMilliseconds: json['timestampDeltaMilliseconds'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final adMarkers = this.adMarkers;
    final additionalManifests = this.additionalManifests;
    final audioOnlyHeader = this.audioOnlyHeader;
    final baseUrl = this.baseUrl;
    final captionLanguageMappings = this.captionLanguageMappings;
    final captionLanguageSetting = this.captionLanguageSetting;
    final captionSegmentLengthControl = this.captionSegmentLengthControl;
    final clientCache = this.clientCache;
    final codecSpecification = this.codecSpecification;
    final destination = this.destination;
    final destinationSettings = this.destinationSettings;
    final directoryStructure = this.directoryStructure;
    final encryption = this.encryption;
    final imageBasedTrickPlay = this.imageBasedTrickPlay;
    final imageBasedTrickPlaySettings = this.imageBasedTrickPlaySettings;
    final manifestCompression = this.manifestCompression;
    final manifestDurationFormat = this.manifestDurationFormat;
    final minFinalSegmentLength = this.minFinalSegmentLength;
    final minSegmentLength = this.minSegmentLength;
    final outputSelection = this.outputSelection;
    final programDateTime = this.programDateTime;
    final programDateTimePeriod = this.programDateTimePeriod;
    final progressiveWriteHlsManifest = this.progressiveWriteHlsManifest;
    final segmentControl = this.segmentControl;
    final segmentLength = this.segmentLength;
    final segmentLengthControl = this.segmentLengthControl;
    final segmentsPerSubdirectory = this.segmentsPerSubdirectory;
    final streamInfResolution = this.streamInfResolution;
    final targetDurationCompatibilityMode =
        this.targetDurationCompatibilityMode;
    final timedMetadataId3Frame = this.timedMetadataId3Frame;
    final timedMetadataId3Period = this.timedMetadataId3Period;
    final timestampDeltaMilliseconds = this.timestampDeltaMilliseconds;
    return {
      if (adMarkers != null)
        'adMarkers': adMarkers.map((e) => e.value).toList(),
      if (additionalManifests != null)
        'additionalManifests': additionalManifests,
      if (audioOnlyHeader != null) 'audioOnlyHeader': audioOnlyHeader.value,
      if (baseUrl != null) 'baseUrl': baseUrl,
      if (captionLanguageMappings != null)
        'captionLanguageMappings': captionLanguageMappings,
      if (captionLanguageSetting != null)
        'captionLanguageSetting': captionLanguageSetting.value,
      if (captionSegmentLengthControl != null)
        'captionSegmentLengthControl': captionSegmentLengthControl.value,
      if (clientCache != null) 'clientCache': clientCache.value,
      if (codecSpecification != null)
        'codecSpecification': codecSpecification.value,
      if (destination != null) 'destination': destination,
      if (destinationSettings != null)
        'destinationSettings': destinationSettings,
      if (directoryStructure != null)
        'directoryStructure': directoryStructure.value,
      if (encryption != null) 'encryption': encryption,
      if (imageBasedTrickPlay != null)
        'imageBasedTrickPlay': imageBasedTrickPlay.value,
      if (imageBasedTrickPlaySettings != null)
        'imageBasedTrickPlaySettings': imageBasedTrickPlaySettings,
      if (manifestCompression != null)
        'manifestCompression': manifestCompression.value,
      if (manifestDurationFormat != null)
        'manifestDurationFormat': manifestDurationFormat.value,
      if (minFinalSegmentLength != null)
        'minFinalSegmentLength': minFinalSegmentLength,
      if (minSegmentLength != null) 'minSegmentLength': minSegmentLength,
      if (outputSelection != null) 'outputSelection': outputSelection.value,
      if (programDateTime != null) 'programDateTime': programDateTime.value,
      if (programDateTimePeriod != null)
        'programDateTimePeriod': programDateTimePeriod,
      if (progressiveWriteHlsManifest != null)
        'progressiveWriteHlsManifest': progressiveWriteHlsManifest.value,
      if (segmentControl != null) 'segmentControl': segmentControl.value,
      if (segmentLength != null) 'segmentLength': segmentLength,
      if (segmentLengthControl != null)
        'segmentLengthControl': segmentLengthControl.value,
      if (segmentsPerSubdirectory != null)
        'segmentsPerSubdirectory': segmentsPerSubdirectory,
      if (streamInfResolution != null)
        'streamInfResolution': streamInfResolution.value,
      if (targetDurationCompatibilityMode != null)
        'targetDurationCompatibilityMode':
            targetDurationCompatibilityMode.value,
      if (timedMetadataId3Frame != null)
        'timedMetadataId3Frame': timedMetadataId3Frame.value,
      if (timedMetadataId3Period != null)
        'timedMetadataId3Period': timedMetadataId3Period,
      if (timestampDeltaMilliseconds != null)
        'timestampDeltaMilliseconds': timestampDeltaMilliseconds,
    };
  }
}

/// Choose Include to have MediaConvert generate a child manifest that lists
/// only the I-frames for this rendition, in addition to your regular manifest
/// for this rendition. You might use this manifest as part of a workflow that
/// creates preview functions for your video. MediaConvert adds both the I-frame
/// only child manifest and the regular child manifest to the parent manifest.
/// When you don't need the I-frame only child manifest, keep the default value
/// Exclude.
enum HlsIFrameOnlyManifest {
  include('INCLUDE'),
  exclude('EXCLUDE'),
  ;

  final String value;

  const HlsIFrameOnlyManifest(this.value);

  static HlsIFrameOnlyManifest fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum HlsIFrameOnlyManifest'));
}

/// Specify whether MediaConvert generates images for trick play. Keep the
/// default value, None, to not generate any images. Choose Thumbnail to
/// generate tiled thumbnails. Choose Thumbnail and full frame to generate tiled
/// thumbnails and full-resolution images of single frames. MediaConvert creates
/// a child manifest for each set of images that you generate and adds
/// corresponding entries to the parent manifest. A common application for these
/// images is Roku trick mode. The thumbnails and full-frame images that
/// MediaConvert creates with this feature are compatible with this Roku
/// specification:
/// https://developer.roku.com/docs/developer-program/media-playback/trick-mode/hls-and-dash.md
enum HlsImageBasedTrickPlay {
  none('NONE'),
  thumbnail('THUMBNAIL'),
  thumbnailAndFullframe('THUMBNAIL_AND_FULLFRAME'),
  advanced('ADVANCED'),
  ;

  final String value;

  const HlsImageBasedTrickPlay(this.value);

  static HlsImageBasedTrickPlay fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum HlsImageBasedTrickPlay'));
}

/// Tile and thumbnail settings applicable when imageBasedTrickPlay is ADVANCED
class HlsImageBasedTrickPlaySettings {
  /// The cadence MediaConvert follows for generating thumbnails. If set to
  /// FOLLOW_IFRAME, MediaConvert generates thumbnails for each IDR frame in the
  /// output (matching the GOP cadence). If set to FOLLOW_CUSTOM, MediaConvert
  /// generates thumbnails according to the interval you specify in
  /// thumbnailInterval.
  final HlsIntervalCadence? intervalCadence;

  /// Height of each thumbnail within each tile image, in pixels. Leave blank to
  /// maintain aspect ratio with thumbnail width. If following the aspect ratio
  /// would lead to a total tile height greater than 4096, then the job will be
  /// rejected. Must be divisible by 2.
  final int? thumbnailHeight;

  /// Enter the interval, in seconds, that MediaConvert uses to generate
  /// thumbnails. If the interval you enter doesn't align with the output frame
  /// rate, MediaConvert automatically rounds the interval to align with the
  /// output frame rate. For example, if the output frame rate is 29.97 frames per
  /// second and you enter 5, MediaConvert uses a 150 frame interval to generate
  /// thumbnails.
  final double? thumbnailInterval;

  /// Width of each thumbnail within each tile image, in pixels. Default is 312.
  /// Must be divisible by 8.
  final int? thumbnailWidth;

  /// Number of thumbnails in each column of a tile image. Set a value between 2
  /// and 2048. Must be divisible by 2.
  final int? tileHeight;

  /// Number of thumbnails in each row of a tile image. Set a value between 1 and
  /// 512.
  final int? tileWidth;

  HlsImageBasedTrickPlaySettings({
    this.intervalCadence,
    this.thumbnailHeight,
    this.thumbnailInterval,
    this.thumbnailWidth,
    this.tileHeight,
    this.tileWidth,
  });

  factory HlsImageBasedTrickPlaySettings.fromJson(Map<String, dynamic> json) {
    return HlsImageBasedTrickPlaySettings(
      intervalCadence: (json['intervalCadence'] as String?)
          ?.let(HlsIntervalCadence.fromString),
      thumbnailHeight: json['thumbnailHeight'] as int?,
      thumbnailInterval: json['thumbnailInterval'] as double?,
      thumbnailWidth: json['thumbnailWidth'] as int?,
      tileHeight: json['tileHeight'] as int?,
      tileWidth: json['tileWidth'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final intervalCadence = this.intervalCadence;
    final thumbnailHeight = this.thumbnailHeight;
    final thumbnailInterval = this.thumbnailInterval;
    final thumbnailWidth = this.thumbnailWidth;
    final tileHeight = this.tileHeight;
    final tileWidth = this.tileWidth;
    return {
      if (intervalCadence != null) 'intervalCadence': intervalCadence.value,
      if (thumbnailHeight != null) 'thumbnailHeight': thumbnailHeight,
      if (thumbnailInterval != null) 'thumbnailInterval': thumbnailInterval,
      if (thumbnailWidth != null) 'thumbnailWidth': thumbnailWidth,
      if (tileHeight != null) 'tileHeight': tileHeight,
      if (tileWidth != null) 'tileWidth': tileWidth,
    };
  }
}

/// The Initialization Vector is a 128-bit number used in conjunction with the
/// key for encrypting blocks. If set to INCLUDE, Initialization Vector is
/// listed in the manifest. Otherwise Initialization Vector is not in the
/// manifest.
enum HlsInitializationVectorInManifest {
  include('INCLUDE'),
  exclude('EXCLUDE'),
  ;

  final String value;

  const HlsInitializationVectorInManifest(this.value);

  static HlsInitializationVectorInManifest fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum HlsInitializationVectorInManifest'));
}

/// The cadence MediaConvert follows for generating thumbnails. If set to
/// FOLLOW_IFRAME, MediaConvert generates thumbnails for each IDR frame in the
/// output (matching the GOP cadence). If set to FOLLOW_CUSTOM, MediaConvert
/// generates thumbnails according to the interval you specify in
/// thumbnailInterval.
enum HlsIntervalCadence {
  followIframe('FOLLOW_IFRAME'),
  followCustom('FOLLOW_CUSTOM'),
  ;

  final String value;

  const HlsIntervalCadence(this.value);

  static HlsIntervalCadence fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum HlsIntervalCadence'));
}

/// Specify whether your DRM encryption key is static or from a key provider
/// that follows the SPEKE standard. For more information about SPEKE, see
/// https://docs.aws.amazon.com/speke/latest/documentation/what-is-speke.html.
enum HlsKeyProviderType {
  speke('SPEKE'),
  staticKey('STATIC_KEY'),
  ;

  final String value;

  const HlsKeyProviderType(this.value);

  static HlsKeyProviderType fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum HlsKeyProviderType'));
}

/// When set to GZIP, compresses HLS playlist.
enum HlsManifestCompression {
  gzip('GZIP'),
  none('NONE'),
  ;

  final String value;

  const HlsManifestCompression(this.value);

  static HlsManifestCompression fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum HlsManifestCompression'));
}

/// Indicates whether the output manifest should use floating point values for
/// segment duration.
enum HlsManifestDurationFormat {
  floatingPoint('FLOATING_POINT'),
  integer('INTEGER'),
  ;

  final String value;

  const HlsManifestDurationFormat(this.value);

  static HlsManifestDurationFormat fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum HlsManifestDurationFormat'));
}

/// Enable this setting to insert the EXT-X-SESSION-KEY element into the master
/// playlist. This allows for offline Apple HLS FairPlay content protection.
enum HlsOfflineEncrypted {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const HlsOfflineEncrypted(this.value);

  static HlsOfflineEncrypted fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum HlsOfflineEncrypted'));
}

/// Indicates whether the .m3u8 manifest file should be generated for this HLS
/// output group.
enum HlsOutputSelection {
  manifestsAndSegments('MANIFESTS_AND_SEGMENTS'),
  segmentsOnly('SEGMENTS_ONLY'),
  ;

  final String value;

  const HlsOutputSelection(this.value);

  static HlsOutputSelection fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum HlsOutputSelection'));
}

/// Includes or excludes EXT-X-PROGRAM-DATE-TIME tag in .m3u8 manifest files.
/// The value is calculated as follows: either the program date and time are
/// initialized using the input timecode source, or the time is initialized
/// using the input timecode source and the date is initialized using the
/// timestamp_offset.
enum HlsProgramDateTime {
  include('INCLUDE'),
  exclude('EXCLUDE'),
  ;

  final String value;

  const HlsProgramDateTime(this.value);

  static HlsProgramDateTime fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum HlsProgramDateTime'));
}

/// Specify whether MediaConvert generates HLS manifests while your job is
/// running or when your job is complete. To generate HLS manifests while your
/// job is running: Choose Enabled. Use if you want to play back your content as
/// soon as it's available. MediaConvert writes the parent and child manifests
/// after the first three media segments are written to your destination S3
/// bucket. It then writes new updated manifests after each additional segment
/// is written. The parent manifest includes the latest BANDWIDTH and
/// AVERAGE-BANDWIDTH attributes, and child manifests include the latest
/// available media segment. When your job completes, the final child playlists
/// include an EXT-X-ENDLIST tag. To generate HLS manifests only when your job
/// completes: Choose Disabled.
enum HlsProgressiveWriteHlsManifest {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const HlsProgressiveWriteHlsManifest(this.value);

  static HlsProgressiveWriteHlsManifest fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum HlsProgressiveWriteHlsManifest'));
}

/// Settings specific to audio sources in an HLS alternate rendition group.
/// Specify the properties (renditionGroupId, renditionName or
/// renditionLanguageCode) to identify the unique audio track among the
/// alternative rendition groups present in the HLS manifest. If no unique track
/// is found, or multiple tracks match the properties provided, the job fails.
/// If no properties in hlsRenditionGroupSettings are specified, the default
/// audio track within the video segment is chosen. If there is no audio within
/// video segment, the alternative audio with DEFAULT=YES is chosen instead.
class HlsRenditionGroupSettings {
  /// Optional. Specify alternative group ID
  final String? renditionGroupId;

  /// Optional. Specify ISO 639-2 or ISO 639-3 code in the language property
  final LanguageCode? renditionLanguageCode;

  /// Optional. Specify media name
  final String? renditionName;

  HlsRenditionGroupSettings({
    this.renditionGroupId,
    this.renditionLanguageCode,
    this.renditionName,
  });

  factory HlsRenditionGroupSettings.fromJson(Map<String, dynamic> json) {
    return HlsRenditionGroupSettings(
      renditionGroupId: json['renditionGroupId'] as String?,
      renditionLanguageCode: (json['renditionLanguageCode'] as String?)
          ?.let(LanguageCode.fromString),
      renditionName: json['renditionName'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final renditionGroupId = this.renditionGroupId;
    final renditionLanguageCode = this.renditionLanguageCode;
    final renditionName = this.renditionName;
    return {
      if (renditionGroupId != null) 'renditionGroupId': renditionGroupId,
      if (renditionLanguageCode != null)
        'renditionLanguageCode': renditionLanguageCode.value,
      if (renditionName != null) 'renditionName': renditionName,
    };
  }
}

/// When set to SINGLE_FILE, emits program as a single media resource (.ts)
/// file, uses #EXT-X-BYTERANGE tags to index segment for playback.
enum HlsSegmentControl {
  singleFile('SINGLE_FILE'),
  segmentedFiles('SEGMENTED_FILES'),
  ;

  final String value;

  const HlsSegmentControl(this.value);

  static HlsSegmentControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum HlsSegmentControl'));
}

/// Specify how you want MediaConvert to determine the segment length. Choose
/// Exact to have the encoder use the exact length that you specify with the
/// setting Segment length. This might result in extra I-frames. Choose Multiple
/// of GOP to have the encoder round up the segment lengths to match the next
/// GOP boundary.
enum HlsSegmentLengthControl {
  exact('EXACT'),
  gopMultiple('GOP_MULTIPLE'),
  ;

  final String value;

  const HlsSegmentLengthControl(this.value);

  static HlsSegmentLengthControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum HlsSegmentLengthControl'));
}

/// Settings for HLS output groups
class HlsSettings {
  /// Specifies the group to which the audio rendition belongs.
  final String? audioGroupId;

  /// Use this setting only in audio-only outputs. Choose MPEG-2 Transport Stream
  /// (M2TS) to create a file in an MPEG2-TS container. Keep the default value
  /// Automatic to create an audio-only file in a raw container. Regardless of the
  /// value that you specify here, if this output has video, the service will
  /// place the output into an MPEG2-TS container.
  final HlsAudioOnlyContainer? audioOnlyContainer;

  /// List all the audio groups that are used with the video output stream. Input
  /// all the audio GROUP-IDs that are associated to the video, separate by ','.
  final String? audioRenditionSets;

  /// Four types of audio-only tracks are supported: Audio-Only Variant Stream The
  /// client can play back this audio-only stream instead of video in
  /// low-bandwidth scenarios. Represented as an EXT-X-STREAM-INF in the HLS
  /// manifest. Alternate Audio, Auto Select, Default Alternate rendition that the
  /// client should try to play back by default. Represented as an EXT-X-MEDIA in
  /// the HLS manifest with DEFAULT=YES, AUTOSELECT=YES Alternate Audio, Auto
  /// Select, Not Default Alternate rendition that the client may try to play back
  /// by default. Represented as an EXT-X-MEDIA in the HLS manifest with
  /// DEFAULT=NO, AUTOSELECT=YES Alternate Audio, not Auto Select Alternate
  /// rendition that the client will not try to play back by default. Represented
  /// as an EXT-X-MEDIA in the HLS manifest with DEFAULT=NO, AUTOSELECT=NO
  final HlsAudioTrackType? audioTrackType;

  /// Specify whether to flag this audio track as descriptive video service (DVS)
  /// in your HLS parent manifest. When you choose Flag, MediaConvert includes the
  /// parameter CHARACTERISTICS="public.accessibility.describes-video" in the
  /// EXT-X-MEDIA entry for this track. When you keep the default choice, Don't
  /// flag, MediaConvert leaves this parameter out. The DVS flag can help with
  /// accessibility on Apple devices. For more information, see the Apple
  /// documentation.
  final HlsDescriptiveVideoServiceFlag? descriptiveVideoServiceFlag;

  /// Choose Include to have MediaConvert generate a child manifest that lists
  /// only the I-frames for this rendition, in addition to your regular manifest
  /// for this rendition. You might use this manifest as part of a workflow that
  /// creates preview functions for your video. MediaConvert adds both the I-frame
  /// only child manifest and the regular child manifest to the parent manifest.
  /// When you don't need the I-frame only child manifest, keep the default value
  /// Exclude.
  final HlsIFrameOnlyManifest? iFrameOnlyManifest;

  /// Use this setting to add an identifying string to the filename of each
  /// segment. The service adds this string between the name modifier and segment
  /// index number. You can use format identifiers in the string. For more
  /// information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/using-variables-in-your-job-settings.html
  final String? segmentModifier;

  HlsSettings({
    this.audioGroupId,
    this.audioOnlyContainer,
    this.audioRenditionSets,
    this.audioTrackType,
    this.descriptiveVideoServiceFlag,
    this.iFrameOnlyManifest,
    this.segmentModifier,
  });

  factory HlsSettings.fromJson(Map<String, dynamic> json) {
    return HlsSettings(
      audioGroupId: json['audioGroupId'] as String?,
      audioOnlyContainer: (json['audioOnlyContainer'] as String?)
          ?.let(HlsAudioOnlyContainer.fromString),
      audioRenditionSets: json['audioRenditionSets'] as String?,
      audioTrackType: (json['audioTrackType'] as String?)
          ?.let(HlsAudioTrackType.fromString),
      descriptiveVideoServiceFlag:
          (json['descriptiveVideoServiceFlag'] as String?)
              ?.let(HlsDescriptiveVideoServiceFlag.fromString),
      iFrameOnlyManifest: (json['iFrameOnlyManifest'] as String?)
          ?.let(HlsIFrameOnlyManifest.fromString),
      segmentModifier: json['segmentModifier'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final audioGroupId = this.audioGroupId;
    final audioOnlyContainer = this.audioOnlyContainer;
    final audioRenditionSets = this.audioRenditionSets;
    final audioTrackType = this.audioTrackType;
    final descriptiveVideoServiceFlag = this.descriptiveVideoServiceFlag;
    final iFrameOnlyManifest = this.iFrameOnlyManifest;
    final segmentModifier = this.segmentModifier;
    return {
      if (audioGroupId != null) 'audioGroupId': audioGroupId,
      if (audioOnlyContainer != null)
        'audioOnlyContainer': audioOnlyContainer.value,
      if (audioRenditionSets != null) 'audioRenditionSets': audioRenditionSets,
      if (audioTrackType != null) 'audioTrackType': audioTrackType.value,
      if (descriptiveVideoServiceFlag != null)
        'descriptiveVideoServiceFlag': descriptiveVideoServiceFlag.value,
      if (iFrameOnlyManifest != null)
        'iFrameOnlyManifest': iFrameOnlyManifest.value,
      if (segmentModifier != null) 'segmentModifier': segmentModifier,
    };
  }
}

/// Include or exclude RESOLUTION attribute for video in EXT-X-STREAM-INF tag of
/// variant manifest.
enum HlsStreamInfResolution {
  include('INCLUDE'),
  exclude('EXCLUDE'),
  ;

  final String value;

  const HlsStreamInfResolution(this.value);

  static HlsStreamInfResolution fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum HlsStreamInfResolution'));
}

/// When set to LEGACY, the segment target duration is always rounded up to the
/// nearest integer value above its current value in seconds. When set to
/// SPEC\\_COMPLIANT, the segment target duration is rounded up to the nearest
/// integer value if fraction seconds are greater than or equal to 0.5 (>= 0.5)
/// and rounded down if less than 0.5 (< 0.5). You may need to use LEGACY if
/// your client needs to ensure that the target duration is always longer than
/// the actual duration of the segment. Some older players may experience
/// interrupted playback when the actual duration of a track in a segment is
/// longer than the target duration.
enum HlsTargetDurationCompatibilityMode {
  legacy('LEGACY'),
  specCompliant('SPEC_COMPLIANT'),
  ;

  final String value;

  const HlsTargetDurationCompatibilityMode(this.value);

  static HlsTargetDurationCompatibilityMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum HlsTargetDurationCompatibilityMode'));
}

/// Specify the type of the ID3 frame to use for ID3 timestamps in your output.
/// To include ID3 timestamps: Specify PRIV or TDRL and set ID3 metadata to
/// Passthrough. To exclude ID3 timestamps: Set ID3 timestamp frame type to
/// None.
enum HlsTimedMetadataId3Frame {
  none('NONE'),
  priv('PRIV'),
  tdrl('TDRL'),
  ;

  final String value;

  const HlsTimedMetadataId3Frame(this.value);

  static HlsTimedMetadataId3Frame fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum HlsTimedMetadataId3Frame'));
}

/// Optional. Configuration for a destination queue to which the job can hop
/// once a customer-defined minimum wait time has passed.
class HopDestination {
  /// Optional. When you set up a job to use queue hopping, you can specify a
  /// different relative priority for the job in the destination queue. If you
  /// don't specify, the relative priority will remain the same as in the previous
  /// queue.
  final int? priority;

  /// Optional unless the job is submitted on the default queue. When you set up a
  /// job to use queue hopping, you can specify a destination queue. This queue
  /// cannot be the original queue to which the job is submitted. If the original
  /// queue isn't the default queue and you don't specify the destination queue,
  /// the job will move to the default queue.
  final String? queue;

  /// Required for setting up a job to use queue hopping. Minimum wait time in
  /// minutes until the job can hop to the destination queue. Valid range is 1 to
  /// 4320 minutes, inclusive.
  final int? waitMinutes;

  HopDestination({
    this.priority,
    this.queue,
    this.waitMinutes,
  });

  factory HopDestination.fromJson(Map<String, dynamic> json) {
    return HopDestination(
      priority: json['priority'] as int?,
      queue: json['queue'] as String?,
      waitMinutes: json['waitMinutes'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final priority = this.priority;
    final queue = this.queue;
    final waitMinutes = this.waitMinutes;
    return {
      if (priority != null) 'priority': priority,
      if (queue != null) 'queue': queue,
      if (waitMinutes != null) 'waitMinutes': waitMinutes,
    };
  }
}

/// To insert ID3 tags in your output, specify two values. Use ID3 tag to
/// specify the base 64 encoded string and use Timecode to specify the time when
/// the tag should be inserted. To insert multiple ID3 tags in your output,
/// create multiple instances of ID3 insertion.
class Id3Insertion {
  /// Use ID3 tag to provide a fully formed ID3 tag in base64-encode format.
  final String? id3;

  /// Provide a Timecode in HH:MM:SS:FF or HH:MM:SS;FF format.
  final String? timecode;

  Id3Insertion({
    this.id3,
    this.timecode,
  });

  factory Id3Insertion.fromJson(Map<String, dynamic> json) {
    return Id3Insertion(
      id3: json['id3'] as String?,
      timecode: json['timecode'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final id3 = this.id3;
    final timecode = this.timecode;
    return {
      if (id3 != null) 'id3': id3,
      if (timecode != null) 'timecode': timecode,
    };
  }
}

/// Use the image inserter feature to include a graphic overlay on your video.
/// Enable or disable this feature for each input or output individually. For
/// more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/graphic-overlay.html.
/// This setting is disabled by default.
class ImageInserter {
  /// Specify the images that you want to overlay on your video. The images must
  /// be PNG or TGA files.
  final List<InsertableImage>? insertableImages;

  /// Specify the reference white level, in nits, for all of your image inserter
  /// images. Use to correct brightness levels within HDR10 outputs. For 1,000 nit
  /// peak brightness displays, we recommend that you set SDR reference white
  /// level to 203 (according to ITU-R BT.2408). Leave blank to use the default
  /// value of 100, or specify an integer from 100 to 1000.
  final int? sdrReferenceWhiteLevel;

  ImageInserter({
    this.insertableImages,
    this.sdrReferenceWhiteLevel,
  });

  factory ImageInserter.fromJson(Map<String, dynamic> json) {
    return ImageInserter(
      insertableImages: (json['insertableImages'] as List?)
          ?.nonNulls
          .map((e) => InsertableImage.fromJson(e as Map<String, dynamic>))
          .toList(),
      sdrReferenceWhiteLevel: json['sdrReferenceWhiteLevel'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final insertableImages = this.insertableImages;
    final sdrReferenceWhiteLevel = this.sdrReferenceWhiteLevel;
    return {
      if (insertableImages != null) 'insertableImages': insertableImages,
      if (sdrReferenceWhiteLevel != null)
        'sdrReferenceWhiteLevel': sdrReferenceWhiteLevel,
    };
  }
}

/// If the IMSC captions track is intended to provide accessibility for people
/// who are deaf or hard of hearing: Set Accessibility subtitles to Enabled.
/// When you do, MediaConvert adds accessibility attributes to your output HLS
/// or DASH manifest. For HLS manifests, MediaConvert adds the following
/// accessibility attributes under EXT-X-MEDIA for this track:
/// CHARACTERISTICS="public.accessibility.describes-spoken-dialog,public.accessibility.describes-music-and-sound"
/// and AUTOSELECT="YES". For DASH manifests, MediaConvert adds the following in
/// the adaptation set for this track: <Accessibility
/// schemeIdUri="urn:mpeg:dash:role:2011" value="caption"/>. If the captions
/// track is not intended to provide such accessibility: Keep the default value,
/// Disabled. When you do, for DASH manifests, MediaConvert instead adds the
/// following in the adaptation set for this track: <Role
/// schemeIDUri="urn:mpeg:dash:role:2011" value="subtitle"/>.
enum ImscAccessibilitySubs {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const ImscAccessibilitySubs(this.value);

  static ImscAccessibilitySubs fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum ImscAccessibilitySubs'));
}

/// Settings related to IMSC captions. IMSC is a sidecar format that holds
/// captions in a file that is separate from the video container. Set up sidecar
/// captions in the same output group, but different output from your video. For
/// more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/ttml-and-webvtt-output-captions.html.
class ImscDestinationSettings {
  /// If the IMSC captions track is intended to provide accessibility for people
  /// who are deaf or hard of hearing: Set Accessibility subtitles to Enabled.
  /// When you do, MediaConvert adds accessibility attributes to your output HLS
  /// or DASH manifest. For HLS manifests, MediaConvert adds the following
  /// accessibility attributes under EXT-X-MEDIA for this track:
  /// CHARACTERISTICS="public.accessibility.describes-spoken-dialog,public.accessibility.describes-music-and-sound"
  /// and AUTOSELECT="YES". For DASH manifests, MediaConvert adds the following in
  /// the adaptation set for this track: <Accessibility
  /// schemeIdUri="urn:mpeg:dash:role:2011" value="caption"/>. If the captions
  /// track is not intended to provide such accessibility: Keep the default value,
  /// Disabled. When you do, for DASH manifests, MediaConvert instead adds the
  /// following in the adaptation set for this track: <Role
  /// schemeIDUri="urn:mpeg:dash:role:2011" value="subtitle"/>.
  final ImscAccessibilitySubs? accessibility;

  /// Keep this setting enabled to have MediaConvert use the font style and
  /// position information from the captions source in the output. This option is
  /// available only when your input captions are IMSC, SMPTE-TT, or TTML. Disable
  /// this setting for simplified output captions.
  final ImscStylePassthrough? stylePassthrough;

  ImscDestinationSettings({
    this.accessibility,
    this.stylePassthrough,
  });

  factory ImscDestinationSettings.fromJson(Map<String, dynamic> json) {
    return ImscDestinationSettings(
      accessibility: (json['accessibility'] as String?)
          ?.let(ImscAccessibilitySubs.fromString),
      stylePassthrough: (json['stylePassthrough'] as String?)
          ?.let(ImscStylePassthrough.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final accessibility = this.accessibility;
    final stylePassthrough = this.stylePassthrough;
    return {
      if (accessibility != null) 'accessibility': accessibility.value,
      if (stylePassthrough != null) 'stylePassthrough': stylePassthrough.value,
    };
  }
}

/// Keep this setting enabled to have MediaConvert use the font style and
/// position information from the captions source in the output. This option is
/// available only when your input captions are IMSC, SMPTE-TT, or TTML. Disable
/// this setting for simplified output captions.
enum ImscStylePassthrough {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const ImscStylePassthrough(this.value);

  static ImscStylePassthrough fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum ImscStylePassthrough'));
}

/// Use inputs to define the source files used in your transcoding job. For more
/// information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/specify-input-settings.html.
/// You can use multiple video inputs to do input stitching. For more
/// information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/assembling-multiple-inputs-and-input-clips.html
class Input {
  /// Use to remove noise, blocking, blurriness, or ringing from your input as a
  /// pre-filter step before encoding. The Advanced input filter removes more
  /// types of compression artifacts and is an improvement when compared to basic
  /// Deblock and Denoise filters. To remove video compression artifacts from your
  /// input and improve the video quality: Choose Enabled. Additionally, this
  /// filter can help increase the video quality of your output relative to its
  /// bitrate, since noisy inputs are more complex and require more bits to
  /// encode. To help restore loss of detail after applying the filter, you can
  /// optionally add texture or sharpening as an additional step. Jobs that use
  /// this feature incur pro-tier pricing. To not apply advanced input filtering:
  /// Choose Disabled. Note that you can still apply basic filtering with Deblock
  /// and Denoise.
  final AdvancedInputFilter? advancedInputFilter;

  /// Optional settings for Advanced input filter when you set Advanced input
  /// filter to Enabled.
  final AdvancedInputFilterSettings? advancedInputFilterSettings;

  /// Use audio selector groups to combine multiple sidecar audio inputs so that
  /// you can assign them to a single output audio tab. Note that, if you're
  /// working with embedded audio, it's simpler to assign multiple input tracks
  /// into a single audio selector rather than use an audio selector group.
  final Map<String, AudioSelectorGroup>? audioSelectorGroups;

  /// Use Audio selectors to specify a track or set of tracks from the input that
  /// you will use in your outputs. You can use multiple Audio selectors per
  /// input.
  final Map<String, AudioSelector>? audioSelectors;

  /// Use captions selectors to specify the captions data from your input that you
  /// use in your outputs. You can use up to 100 captions selectors per input.
  final Map<String, CaptionSelector>? captionSelectors;

  /// Use Cropping selection to specify the video area that the service will
  /// include in the output video frame. If you specify a value here, it will
  /// override any value that you specify in the output setting Cropping
  /// selection.
  final Rectangle? crop;

  /// Enable Deblock to produce smoother motion in the output. Default is
  /// disabled. Only manually controllable for MPEG2 and uncompressed video
  /// inputs.
  final InputDeblockFilter? deblockFilter;

  /// Settings for decrypting any input files that you encrypt before you upload
  /// them to Amazon S3. MediaConvert can decrypt files only when you use AWS Key
  /// Management Service (KMS) to encrypt the data key that you use to encrypt
  /// your content.
  final InputDecryptionSettings? decryptionSettings;

  /// Enable Denoise to filter noise from the input. Default is disabled. Only
  /// applicable to MPEG2, H.264, H.265, and uncompressed video inputs.
  final InputDenoiseFilter? denoiseFilter;

  /// Use this setting only when your video source has Dolby Vision studio
  /// mastering metadata that is carried in a separate XML file. Specify the
  /// Amazon S3 location for the metadata XML file. MediaConvert uses this file to
  /// provide global and frame-level metadata for Dolby Vision preprocessing. When
  /// you specify a file here and your input also has interleaved global and frame
  /// level metadata, MediaConvert ignores the interleaved metadata and uses only
  /// the the metadata from this external XML file. Note that your IAM service
  /// role must grant MediaConvert read permissions to this file. For more
  /// information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/iam-role.html.
  final String? dolbyVisionMetadataXml;

  /// Specify the source file for your transcoding job. You can use multiple
  /// inputs in a single job. The service concatenates these inputs, in the order
  /// that you specify them in the job, to create the outputs. If your input
  /// format is IMF, specify your input by providing the path to your CPL. For
  /// example, "s3://bucket/vf/cpl.xml". If the CPL is in an incomplete IMP, make
  /// sure to use *Supplemental IMPs* to specify any supplemental IMPs that
  /// contain assets referenced by the CPL.
  final String? fileInput;

  /// Specify whether to apply input filtering to improve the video quality of
  /// your input. To apply filtering depending on your input type and quality:
  /// Choose Auto. To apply no filtering: Choose Disable. To apply filtering
  /// regardless of your input type and quality: Choose Force. When you do, you
  /// must also specify a value for Filter strength.
  final InputFilterEnable? filterEnable;

  /// Specify the strength of the input filter. To apply an automatic amount of
  /// filtering based the compression artifacts measured in your input: We
  /// recommend that you leave Filter strength blank and set Filter enable to
  /// Auto. To manually apply filtering: Enter a value from 1 to 5, where 1 is the
  /// least amount of filtering and 5 is the most. The value that you enter
  /// applies to the strength of the Deblock or Denoise filters, or to the
  /// strength of the Advanced input filter.
  final int? filterStrength;

  /// Enable the image inserter feature to include a graphic overlay on your
  /// video. Enable or disable this feature for each input individually. This
  /// setting is disabled by default.
  final ImageInserter? imageInserter;

  /// Contains sets of start and end times that together specify a portion of the
  /// input to be used in the outputs. If you provide only a start time, the clip
  /// will be the entire input from that point to the end. If you provide only an
  /// end time, it will be the entire input up to that point. When you specify
  /// more than one input clip, the transcoding service creates the job outputs by
  /// stringing the clips together in the order you specify them.
  final List<InputClipping>? inputClippings;

  /// When you have a progressive segmented frame (PsF) input, use this setting to
  /// flag the input as PsF. MediaConvert doesn't automatically detect PsF.
  /// Therefore, flagging your input as PsF results in better preservation of
  /// video quality when you do deinterlacing and frame rate conversion. If you
  /// don't specify, the default value is Auto. Auto is the correct setting for
  /// all inputs that are not PsF. Don't set this value to PsF when your input is
  /// interlaced. Doing so creates horizontal interlacing artifacts.
  final InputScanType? inputScanType;

  /// Use Selection placement to define the video area in your output frame. The
  /// area outside of the rectangle that you specify here is black. If you specify
  /// a value here, it will override any value that you specify in the output
  /// setting Selection placement. If you specify a value here, this will override
  /// any AFD values in your input, even if you set Respond to AFD to Respond. If
  /// you specify a value here, this will ignore anything that you specify for the
  /// setting Scaling Behavior.
  final Rectangle? position;

  /// Use Program to select a specific program from within a multi-program
  /// transport stream. Note that Quad 4K is not currently supported. Default is
  /// the first program within the transport stream. If the program you specify
  /// doesn't exist, the transcoding service will use this default.
  final int? programNumber;

  /// Set PSI control for transport stream inputs to specify which data the demux
  /// process to scans.
  /// * Ignore PSI - Scan all PIDs for audio and video.
  /// * Use PSI - Scan only PSI data.
  final InputPsiControl? psiControl;

  /// Provide a list of any necessary supplemental IMPs. You need supplemental
  /// IMPs if the CPL that you're using for your input is in an incomplete IMP.
  /// Specify either the supplemental IMP directories with a trailing slash or the
  /// ASSETMAP.xml files. For example ["s3://bucket/ov/",
  /// "s3://bucket/vf2/ASSETMAP.xml"]. You don't need to specify the IMP that
  /// contains your input CPL, because the service automatically detects it.
  final List<String>? supplementalImps;

  /// Use this Timecode source setting, located under the input settings, to
  /// specify how the service counts input video frames. This input frame count
  /// affects only the behavior of features that apply to a single input at a
  /// time, such as input clipping and synchronizing some captions formats. Choose
  /// Embedded to use the timecodes in your input video. Choose Start at zero to
  /// start the first frame at zero. Choose Specified start to start the first
  /// frame at the timecode that you specify in the setting Start timecode. If you
  /// don't specify a value for Timecode source, the service will use Embedded by
  /// default. For more information about timecodes, see
  /// https://docs.aws.amazon.com/console/mediaconvert/timecode.
  final InputTimecodeSource? timecodeSource;

  /// Specify the timecode that you want the service to use for this input's
  /// initial frame. To use this setting, you must set the Timecode source
  /// setting, located under the input settings, to Specified start. For more
  /// information about timecodes, see
  /// https://docs.aws.amazon.com/console/mediaconvert/timecode.
  final String? timecodeStart;

  /// When you include Video generator, MediaConvert creates a video input with
  /// black frames. Use this setting if you do not have a video input or if you
  /// want to add black video frames before, or after, other inputs. You can
  /// specify Video generator, or you can specify an Input file, but you cannot
  /// specify both. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/video-generator.html
  final InputVideoGenerator? videoGenerator;

  /// Contains an array of video overlays.
  final List<VideoOverlay>? videoOverlays;

  /// Input video selectors contain the video settings for the input. Each of your
  /// inputs can have up to one video selector.
  final VideoSelector? videoSelector;

  Input({
    this.advancedInputFilter,
    this.advancedInputFilterSettings,
    this.audioSelectorGroups,
    this.audioSelectors,
    this.captionSelectors,
    this.crop,
    this.deblockFilter,
    this.decryptionSettings,
    this.denoiseFilter,
    this.dolbyVisionMetadataXml,
    this.fileInput,
    this.filterEnable,
    this.filterStrength,
    this.imageInserter,
    this.inputClippings,
    this.inputScanType,
    this.position,
    this.programNumber,
    this.psiControl,
    this.supplementalImps,
    this.timecodeSource,
    this.timecodeStart,
    this.videoGenerator,
    this.videoOverlays,
    this.videoSelector,
  });

  factory Input.fromJson(Map<String, dynamic> json) {
    return Input(
      advancedInputFilter: (json['advancedInputFilter'] as String?)
          ?.let(AdvancedInputFilter.fromString),
      advancedInputFilterSettings: json['advancedInputFilterSettings'] != null
          ? AdvancedInputFilterSettings.fromJson(
              json['advancedInputFilterSettings'] as Map<String, dynamic>)
          : null,
      audioSelectorGroups:
          (json['audioSelectorGroups'] as Map<String, dynamic>?)?.map((k, e) =>
              MapEntry(
                  k, AudioSelectorGroup.fromJson(e as Map<String, dynamic>))),
      audioSelectors: (json['audioSelectors'] as Map<String, dynamic>?)?.map(
          (k, e) =>
              MapEntry(k, AudioSelector.fromJson(e as Map<String, dynamic>))),
      captionSelectors: (json['captionSelectors'] as Map<String, dynamic>?)
          ?.map((k, e) =>
              MapEntry(k, CaptionSelector.fromJson(e as Map<String, dynamic>))),
      crop: json['crop'] != null
          ? Rectangle.fromJson(json['crop'] as Map<String, dynamic>)
          : null,
      deblockFilter: (json['deblockFilter'] as String?)
          ?.let(InputDeblockFilter.fromString),
      decryptionSettings: json['decryptionSettings'] != null
          ? InputDecryptionSettings.fromJson(
              json['decryptionSettings'] as Map<String, dynamic>)
          : null,
      denoiseFilter: (json['denoiseFilter'] as String?)
          ?.let(InputDenoiseFilter.fromString),
      dolbyVisionMetadataXml: json['dolbyVisionMetadataXml'] as String?,
      fileInput: json['fileInput'] as String?,
      filterEnable:
          (json['filterEnable'] as String?)?.let(InputFilterEnable.fromString),
      filterStrength: json['filterStrength'] as int?,
      imageInserter: json['imageInserter'] != null
          ? ImageInserter.fromJson(
              json['imageInserter'] as Map<String, dynamic>)
          : null,
      inputClippings: (json['inputClippings'] as List?)
          ?.nonNulls
          .map((e) => InputClipping.fromJson(e as Map<String, dynamic>))
          .toList(),
      inputScanType:
          (json['inputScanType'] as String?)?.let(InputScanType.fromString),
      position: json['position'] != null
          ? Rectangle.fromJson(json['position'] as Map<String, dynamic>)
          : null,
      programNumber: json['programNumber'] as int?,
      psiControl:
          (json['psiControl'] as String?)?.let(InputPsiControl.fromString),
      supplementalImps: (json['supplementalImps'] as List?)
          ?.nonNulls
          .map((e) => e as String)
          .toList(),
      timecodeSource: (json['timecodeSource'] as String?)
          ?.let(InputTimecodeSource.fromString),
      timecodeStart: json['timecodeStart'] as String?,
      videoGenerator: json['videoGenerator'] != null
          ? InputVideoGenerator.fromJson(
              json['videoGenerator'] as Map<String, dynamic>)
          : null,
      videoOverlays: (json['videoOverlays'] as List?)
          ?.nonNulls
          .map((e) => VideoOverlay.fromJson(e as Map<String, dynamic>))
          .toList(),
      videoSelector: json['videoSelector'] != null
          ? VideoSelector.fromJson(
              json['videoSelector'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final advancedInputFilter = this.advancedInputFilter;
    final advancedInputFilterSettings = this.advancedInputFilterSettings;
    final audioSelectorGroups = this.audioSelectorGroups;
    final audioSelectors = this.audioSelectors;
    final captionSelectors = this.captionSelectors;
    final crop = this.crop;
    final deblockFilter = this.deblockFilter;
    final decryptionSettings = this.decryptionSettings;
    final denoiseFilter = this.denoiseFilter;
    final dolbyVisionMetadataXml = this.dolbyVisionMetadataXml;
    final fileInput = this.fileInput;
    final filterEnable = this.filterEnable;
    final filterStrength = this.filterStrength;
    final imageInserter = this.imageInserter;
    final inputClippings = this.inputClippings;
    final inputScanType = this.inputScanType;
    final position = this.position;
    final programNumber = this.programNumber;
    final psiControl = this.psiControl;
    final supplementalImps = this.supplementalImps;
    final timecodeSource = this.timecodeSource;
    final timecodeStart = this.timecodeStart;
    final videoGenerator = this.videoGenerator;
    final videoOverlays = this.videoOverlays;
    final videoSelector = this.videoSelector;
    return {
      if (advancedInputFilter != null)
        'advancedInputFilter': advancedInputFilter.value,
      if (advancedInputFilterSettings != null)
        'advancedInputFilterSettings': advancedInputFilterSettings,
      if (audioSelectorGroups != null)
        'audioSelectorGroups': audioSelectorGroups,
      if (audioSelectors != null) 'audioSelectors': audioSelectors,
      if (captionSelectors != null) 'captionSelectors': captionSelectors,
      if (crop != null) 'crop': crop,
      if (deblockFilter != null) 'deblockFilter': deblockFilter.value,
      if (decryptionSettings != null) 'decryptionSettings': decryptionSettings,
      if (denoiseFilter != null) 'denoiseFilter': denoiseFilter.value,
      if (dolbyVisionMetadataXml != null)
        'dolbyVisionMetadataXml': dolbyVisionMetadataXml,
      if (fileInput != null) 'fileInput': fileInput,
      if (filterEnable != null) 'filterEnable': filterEnable.value,
      if (filterStrength != null) 'filterStrength': filterStrength,
      if (imageInserter != null) 'imageInserter': imageInserter,
      if (inputClippings != null) 'inputClippings': inputClippings,
      if (inputScanType != null) 'inputScanType': inputScanType.value,
      if (position != null) 'position': position,
      if (programNumber != null) 'programNumber': programNumber,
      if (psiControl != null) 'psiControl': psiControl.value,
      if (supplementalImps != null) 'supplementalImps': supplementalImps,
      if (timecodeSource != null) 'timecodeSource': timecodeSource.value,
      if (timecodeStart != null) 'timecodeStart': timecodeStart,
      if (videoGenerator != null) 'videoGenerator': videoGenerator,
      if (videoOverlays != null) 'videoOverlays': videoOverlays,
      if (videoSelector != null) 'videoSelector': videoSelector,
    };
  }
}

/// To transcode only portions of your input, include one input clip for each
/// part of your input that you want in your output. All input clips that you
/// specify will be included in every output of the job. For more information,
/// see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/assembling-multiple-inputs-and-input-clips.html.
class InputClipping {
  /// Set End timecode to the end of the portion of the input you are clipping.
  /// The frame corresponding to the End timecode value is included in the clip.
  /// Start timecode or End timecode may be left blank, but not both. Use the
  /// format HH:MM:SS:FF or HH:MM:SS;FF, where HH is the hour, MM is the minute,
  /// SS is the second, and FF is the frame number. When choosing this value, take
  /// into account your setting for timecode source under input settings. For
  /// example, if you have embedded timecodes that start at 01:00:00:00 and you
  /// want your clip to end six minutes into the video, use 01:06:00:00.
  final String? endTimecode;

  /// Set Start timecode to the beginning of the portion of the input you are
  /// clipping. The frame corresponding to the Start timecode value is included in
  /// the clip. Start timecode or End timecode may be left blank, but not both.
  /// Use the format HH:MM:SS:FF or HH:MM:SS;FF, where HH is the hour, MM is the
  /// minute, SS is the second, and FF is the frame number. When choosing this
  /// value, take into account your setting for Input timecode source. For
  /// example, if you have embedded timecodes that start at 01:00:00:00 and you
  /// want your clip to begin five minutes into the video, use 01:05:00:00.
  final String? startTimecode;

  InputClipping({
    this.endTimecode,
    this.startTimecode,
  });

  factory InputClipping.fromJson(Map<String, dynamic> json) {
    return InputClipping(
      endTimecode: json['endTimecode'] as String?,
      startTimecode: json['startTimecode'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final endTimecode = this.endTimecode;
    final startTimecode = this.startTimecode;
    return {
      if (endTimecode != null) 'endTimecode': endTimecode,
      if (startTimecode != null) 'startTimecode': startTimecode,
    };
  }
}

/// Enable Deblock to produce smoother motion in the output. Default is
/// disabled. Only manually controllable for MPEG2 and uncompressed video
/// inputs.
enum InputDeblockFilter {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const InputDeblockFilter(this.value);

  static InputDeblockFilter fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum InputDeblockFilter'));
}

/// Settings for decrypting any input files that you encrypt before you upload
/// them to Amazon S3. MediaConvert can decrypt files only when you use AWS Key
/// Management Service (KMS) to encrypt the data key that you use to encrypt
/// your content.
class InputDecryptionSettings {
  /// Specify the encryption mode that you used to encrypt your input files.
  final DecryptionMode? decryptionMode;

  /// Warning! Don't provide your encryption key in plaintext. Your job settings
  /// could be intercepted, making your encrypted content vulnerable. Specify the
  /// encrypted version of the data key that you used to encrypt your content. The
  /// data key must be encrypted by AWS Key Management Service (KMS). The key can
  /// be 128, 192, or 256 bits.
  final String? encryptedDecryptionKey;

  /// Specify the initialization vector that you used when you encrypted your
  /// content before uploading it to Amazon S3. You can use a 16-byte
  /// initialization vector with any encryption mode. Or, you can use a 12-byte
  /// initialization vector with GCM or CTR. MediaConvert accepts only
  /// initialization vectors that are base64-encoded.
  final String? initializationVector;

  /// Specify the AWS Region for AWS Key Management Service (KMS) that you used to
  /// encrypt your data key, if that Region is different from the one you are
  /// using for AWS Elemental MediaConvert.
  final String? kmsKeyRegion;

  InputDecryptionSettings({
    this.decryptionMode,
    this.encryptedDecryptionKey,
    this.initializationVector,
    this.kmsKeyRegion,
  });

  factory InputDecryptionSettings.fromJson(Map<String, dynamic> json) {
    return InputDecryptionSettings(
      decryptionMode:
          (json['decryptionMode'] as String?)?.let(DecryptionMode.fromString),
      encryptedDecryptionKey: json['encryptedDecryptionKey'] as String?,
      initializationVector: json['initializationVector'] as String?,
      kmsKeyRegion: json['kmsKeyRegion'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final decryptionMode = this.decryptionMode;
    final encryptedDecryptionKey = this.encryptedDecryptionKey;
    final initializationVector = this.initializationVector;
    final kmsKeyRegion = this.kmsKeyRegion;
    return {
      if (decryptionMode != null) 'decryptionMode': decryptionMode.value,
      if (encryptedDecryptionKey != null)
        'encryptedDecryptionKey': encryptedDecryptionKey,
      if (initializationVector != null)
        'initializationVector': initializationVector,
      if (kmsKeyRegion != null) 'kmsKeyRegion': kmsKeyRegion,
    };
  }
}

/// Enable Denoise to filter noise from the input. Default is disabled. Only
/// applicable to MPEG2, H.264, H.265, and uncompressed video inputs.
enum InputDenoiseFilter {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const InputDenoiseFilter(this.value);

  static InputDenoiseFilter fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum InputDenoiseFilter'));
}

/// Specify whether to apply input filtering to improve the video quality of
/// your input. To apply filtering depending on your input type and quality:
/// Choose Auto. To apply no filtering: Choose Disable. To apply filtering
/// regardless of your input type and quality: Choose Force. When you do, you
/// must also specify a value for Filter strength.
enum InputFilterEnable {
  auto('AUTO'),
  disable('DISABLE'),
  force('FORCE'),
  ;

  final String value;

  const InputFilterEnable(this.value);

  static InputFilterEnable fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum InputFilterEnable'));
}

/// An input policy allows or disallows a job you submit to run based on the
/// conditions that you specify.
enum InputPolicy {
  allowed('ALLOWED'),
  disallowed('DISALLOWED'),
  ;

  final String value;

  const InputPolicy(this.value);

  static InputPolicy fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum InputPolicy'));
}

/// Set PSI control for transport stream inputs to specify which data the demux
/// process to scans.
/// * Ignore PSI - Scan all PIDs for audio and video.
/// * Use PSI - Scan only PSI data.
enum InputPsiControl {
  ignorePsi('IGNORE_PSI'),
  usePsi('USE_PSI'),
  ;

  final String value;

  const InputPsiControl(this.value);

  static InputPsiControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum InputPsiControl'));
}

/// Use Rotate to specify how the service rotates your video. You can choose
/// automatic rotation or specify a rotation. You can specify a clockwise
/// rotation of 0, 90, 180, or 270 degrees. If your input video container is
/// .mov or .mp4 and your input has rotation metadata, you can choose Automatic
/// to have the service rotate your video according to the rotation specified in
/// the metadata. The rotation must be within one degree of 90, 180, or 270
/// degrees. If the rotation metadata specifies any other rotation, the service
/// will default to no rotation. By default, the service does no rotation, even
/// if your input video has rotation metadata. The service doesn't pass through
/// rotation metadata.
enum InputRotate {
  degree_0('DEGREE_0'),
  degrees_90('DEGREES_90'),
  degrees_180('DEGREES_180'),
  degrees_270('DEGREES_270'),
  auto('AUTO'),
  ;

  final String value;

  const InputRotate(this.value);

  static InputRotate fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum InputRotate'));
}

/// If the sample range metadata in your input video is accurate, or if you
/// don't know about sample range, keep the default value, Follow, for this
/// setting. When you do, the service automatically detects your input sample
/// range. If your input video has metadata indicating the wrong sample range,
/// specify the accurate sample range here. When you do, MediaConvert ignores
/// any sample range information in the input metadata. Regardless of whether
/// MediaConvert uses the input sample range or the sample range that you
/// specify, MediaConvert uses the sample range for transcoding and also writes
/// it to the output metadata.
enum InputSampleRange {
  follow('FOLLOW'),
  fullRange('FULL_RANGE'),
  limitedRange('LIMITED_RANGE'),
  ;

  final String value;

  const InputSampleRange(this.value);

  static InputSampleRange fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum InputSampleRange'));
}

/// When you have a progressive segmented frame (PsF) input, use this setting to
/// flag the input as PsF. MediaConvert doesn't automatically detect PsF.
/// Therefore, flagging your input as PsF results in better preservation of
/// video quality when you do deinterlacing and frame rate conversion. If you
/// don't specify, the default value is Auto. Auto is the correct setting for
/// all inputs that are not PsF. Don't set this value to PsF when your input is
/// interlaced. Doing so creates horizontal interlacing artifacts.
enum InputScanType {
  auto('AUTO'),
  psf('PSF'),
  ;

  final String value;

  const InputScanType(this.value);

  static InputScanType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum InputScanType'));
}

/// Specified video input in a template.
class InputTemplate {
  /// Use to remove noise, blocking, blurriness, or ringing from your input as a
  /// pre-filter step before encoding. The Advanced input filter removes more
  /// types of compression artifacts and is an improvement when compared to basic
  /// Deblock and Denoise filters. To remove video compression artifacts from your
  /// input and improve the video quality: Choose Enabled. Additionally, this
  /// filter can help increase the video quality of your output relative to its
  /// bitrate, since noisy inputs are more complex and require more bits to
  /// encode. To help restore loss of detail after applying the filter, you can
  /// optionally add texture or sharpening as an additional step. Jobs that use
  /// this feature incur pro-tier pricing. To not apply advanced input filtering:
  /// Choose Disabled. Note that you can still apply basic filtering with Deblock
  /// and Denoise.
  final AdvancedInputFilter? advancedInputFilter;

  /// Optional settings for Advanced input filter when you set Advanced input
  /// filter to Enabled.
  final AdvancedInputFilterSettings? advancedInputFilterSettings;

  /// Use audio selector groups to combine multiple sidecar audio inputs so that
  /// you can assign them to a single output audio tab. Note that, if you're
  /// working with embedded audio, it's simpler to assign multiple input tracks
  /// into a single audio selector rather than use an audio selector group.
  final Map<String, AudioSelectorGroup>? audioSelectorGroups;

  /// Use Audio selectors to specify a track or set of tracks from the input that
  /// you will use in your outputs. You can use multiple Audio selectors per
  /// input.
  final Map<String, AudioSelector>? audioSelectors;

  /// Use captions selectors to specify the captions data from your input that you
  /// use in your outputs. You can use up to 100 captions selectors per input.
  final Map<String, CaptionSelector>? captionSelectors;

  /// Use Cropping selection to specify the video area that the service will
  /// include in the output video frame. If you specify a value here, it will
  /// override any value that you specify in the output setting Cropping
  /// selection.
  final Rectangle? crop;

  /// Enable Deblock to produce smoother motion in the output. Default is
  /// disabled. Only manually controllable for MPEG2 and uncompressed video
  /// inputs.
  final InputDeblockFilter? deblockFilter;

  /// Enable Denoise to filter noise from the input. Default is disabled. Only
  /// applicable to MPEG2, H.264, H.265, and uncompressed video inputs.
  final InputDenoiseFilter? denoiseFilter;

  /// Use this setting only when your video source has Dolby Vision studio
  /// mastering metadata that is carried in a separate XML file. Specify the
  /// Amazon S3 location for the metadata XML file. MediaConvert uses this file to
  /// provide global and frame-level metadata for Dolby Vision preprocessing. When
  /// you specify a file here and your input also has interleaved global and frame
  /// level metadata, MediaConvert ignores the interleaved metadata and uses only
  /// the the metadata from this external XML file. Note that your IAM service
  /// role must grant MediaConvert read permissions to this file. For more
  /// information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/iam-role.html.
  final String? dolbyVisionMetadataXml;

  /// Specify whether to apply input filtering to improve the video quality of
  /// your input. To apply filtering depending on your input type and quality:
  /// Choose Auto. To apply no filtering: Choose Disable. To apply filtering
  /// regardless of your input type and quality: Choose Force. When you do, you
  /// must also specify a value for Filter strength.
  final InputFilterEnable? filterEnable;

  /// Specify the strength of the input filter. To apply an automatic amount of
  /// filtering based the compression artifacts measured in your input: We
  /// recommend that you leave Filter strength blank and set Filter enable to
  /// Auto. To manually apply filtering: Enter a value from 1 to 5, where 1 is the
  /// least amount of filtering and 5 is the most. The value that you enter
  /// applies to the strength of the Deblock or Denoise filters, or to the
  /// strength of the Advanced input filter.
  final int? filterStrength;

  /// Enable the image inserter feature to include a graphic overlay on your
  /// video. Enable or disable this feature for each input individually. This
  /// setting is disabled by default.
  final ImageInserter? imageInserter;

  /// Contains sets of start and end times that together specify a portion of the
  /// input to be used in the outputs. If you provide only a start time, the clip
  /// will be the entire input from that point to the end. If you provide only an
  /// end time, it will be the entire input up to that point. When you specify
  /// more than one input clip, the transcoding service creates the job outputs by
  /// stringing the clips together in the order you specify them.
  final List<InputClipping>? inputClippings;

  /// When you have a progressive segmented frame (PsF) input, use this setting to
  /// flag the input as PsF. MediaConvert doesn't automatically detect PsF.
  /// Therefore, flagging your input as PsF results in better preservation of
  /// video quality when you do deinterlacing and frame rate conversion. If you
  /// don't specify, the default value is Auto. Auto is the correct setting for
  /// all inputs that are not PsF. Don't set this value to PsF when your input is
  /// interlaced. Doing so creates horizontal interlacing artifacts.
  final InputScanType? inputScanType;

  /// Use Selection placement to define the video area in your output frame. The
  /// area outside of the rectangle that you specify here is black. If you specify
  /// a value here, it will override any value that you specify in the output
  /// setting Selection placement. If you specify a value here, this will override
  /// any AFD values in your input, even if you set Respond to AFD to Respond. If
  /// you specify a value here, this will ignore anything that you specify for the
  /// setting Scaling Behavior.
  final Rectangle? position;

  /// Use Program to select a specific program from within a multi-program
  /// transport stream. Note that Quad 4K is not currently supported. Default is
  /// the first program within the transport stream. If the program you specify
  /// doesn't exist, the transcoding service will use this default.
  final int? programNumber;

  /// Set PSI control for transport stream inputs to specify which data the demux
  /// process to scans.
  /// * Ignore PSI - Scan all PIDs for audio and video.
  /// * Use PSI - Scan only PSI data.
  final InputPsiControl? psiControl;

  /// Use this Timecode source setting, located under the input settings, to
  /// specify how the service counts input video frames. This input frame count
  /// affects only the behavior of features that apply to a single input at a
  /// time, such as input clipping and synchronizing some captions formats. Choose
  /// Embedded to use the timecodes in your input video. Choose Start at zero to
  /// start the first frame at zero. Choose Specified start to start the first
  /// frame at the timecode that you specify in the setting Start timecode. If you
  /// don't specify a value for Timecode source, the service will use Embedded by
  /// default. For more information about timecodes, see
  /// https://docs.aws.amazon.com/console/mediaconvert/timecode.
  final InputTimecodeSource? timecodeSource;

  /// Specify the timecode that you want the service to use for this input's
  /// initial frame. To use this setting, you must set the Timecode source
  /// setting, located under the input settings, to Specified start. For more
  /// information about timecodes, see
  /// https://docs.aws.amazon.com/console/mediaconvert/timecode.
  final String? timecodeStart;

  /// Contains an array of video overlays.
  final List<VideoOverlay>? videoOverlays;

  /// Input video selectors contain the video settings for the input. Each of your
  /// inputs can have up to one video selector.
  final VideoSelector? videoSelector;

  InputTemplate({
    this.advancedInputFilter,
    this.advancedInputFilterSettings,
    this.audioSelectorGroups,
    this.audioSelectors,
    this.captionSelectors,
    this.crop,
    this.deblockFilter,
    this.denoiseFilter,
    this.dolbyVisionMetadataXml,
    this.filterEnable,
    this.filterStrength,
    this.imageInserter,
    this.inputClippings,
    this.inputScanType,
    this.position,
    this.programNumber,
    this.psiControl,
    this.timecodeSource,
    this.timecodeStart,
    this.videoOverlays,
    this.videoSelector,
  });

  factory InputTemplate.fromJson(Map<String, dynamic> json) {
    return InputTemplate(
      advancedInputFilter: (json['advancedInputFilter'] as String?)
          ?.let(AdvancedInputFilter.fromString),
      advancedInputFilterSettings: json['advancedInputFilterSettings'] != null
          ? AdvancedInputFilterSettings.fromJson(
              json['advancedInputFilterSettings'] as Map<String, dynamic>)
          : null,
      audioSelectorGroups:
          (json['audioSelectorGroups'] as Map<String, dynamic>?)?.map((k, e) =>
              MapEntry(
                  k, AudioSelectorGroup.fromJson(e as Map<String, dynamic>))),
      audioSelectors: (json['audioSelectors'] as Map<String, dynamic>?)?.map(
          (k, e) =>
              MapEntry(k, AudioSelector.fromJson(e as Map<String, dynamic>))),
      captionSelectors: (json['captionSelectors'] as Map<String, dynamic>?)
          ?.map((k, e) =>
              MapEntry(k, CaptionSelector.fromJson(e as Map<String, dynamic>))),
      crop: json['crop'] != null
          ? Rectangle.fromJson(json['crop'] as Map<String, dynamic>)
          : null,
      deblockFilter: (json['deblockFilter'] as String?)
          ?.let(InputDeblockFilter.fromString),
      denoiseFilter: (json['denoiseFilter'] as String?)
          ?.let(InputDenoiseFilter.fromString),
      dolbyVisionMetadataXml: json['dolbyVisionMetadataXml'] as String?,
      filterEnable:
          (json['filterEnable'] as String?)?.let(InputFilterEnable.fromString),
      filterStrength: json['filterStrength'] as int?,
      imageInserter: json['imageInserter'] != null
          ? ImageInserter.fromJson(
              json['imageInserter'] as Map<String, dynamic>)
          : null,
      inputClippings: (json['inputClippings'] as List?)
          ?.nonNulls
          .map((e) => InputClipping.fromJson(e as Map<String, dynamic>))
          .toList(),
      inputScanType:
          (json['inputScanType'] as String?)?.let(InputScanType.fromString),
      position: json['position'] != null
          ? Rectangle.fromJson(json['position'] as Map<String, dynamic>)
          : null,
      programNumber: json['programNumber'] as int?,
      psiControl:
          (json['psiControl'] as String?)?.let(InputPsiControl.fromString),
      timecodeSource: (json['timecodeSource'] as String?)
          ?.let(InputTimecodeSource.fromString),
      timecodeStart: json['timecodeStart'] as String?,
      videoOverlays: (json['videoOverlays'] as List?)
          ?.nonNulls
          .map((e) => VideoOverlay.fromJson(e as Map<String, dynamic>))
          .toList(),
      videoSelector: json['videoSelector'] != null
          ? VideoSelector.fromJson(
              json['videoSelector'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final advancedInputFilter = this.advancedInputFilter;
    final advancedInputFilterSettings = this.advancedInputFilterSettings;
    final audioSelectorGroups = this.audioSelectorGroups;
    final audioSelectors = this.audioSelectors;
    final captionSelectors = this.captionSelectors;
    final crop = this.crop;
    final deblockFilter = this.deblockFilter;
    final denoiseFilter = this.denoiseFilter;
    final dolbyVisionMetadataXml = this.dolbyVisionMetadataXml;
    final filterEnable = this.filterEnable;
    final filterStrength = this.filterStrength;
    final imageInserter = this.imageInserter;
    final inputClippings = this.inputClippings;
    final inputScanType = this.inputScanType;
    final position = this.position;
    final programNumber = this.programNumber;
    final psiControl = this.psiControl;
    final timecodeSource = this.timecodeSource;
    final timecodeStart = this.timecodeStart;
    final videoOverlays = this.videoOverlays;
    final videoSelector = this.videoSelector;
    return {
      if (advancedInputFilter != null)
        'advancedInputFilter': advancedInputFilter.value,
      if (advancedInputFilterSettings != null)
        'advancedInputFilterSettings': advancedInputFilterSettings,
      if (audioSelectorGroups != null)
        'audioSelectorGroups': audioSelectorGroups,
      if (audioSelectors != null) 'audioSelectors': audioSelectors,
      if (captionSelectors != null) 'captionSelectors': captionSelectors,
      if (crop != null) 'crop': crop,
      if (deblockFilter != null) 'deblockFilter': deblockFilter.value,
      if (denoiseFilter != null) 'denoiseFilter': denoiseFilter.value,
      if (dolbyVisionMetadataXml != null)
        'dolbyVisionMetadataXml': dolbyVisionMetadataXml,
      if (filterEnable != null) 'filterEnable': filterEnable.value,
      if (filterStrength != null) 'filterStrength': filterStrength,
      if (imageInserter != null) 'imageInserter': imageInserter,
      if (inputClippings != null) 'inputClippings': inputClippings,
      if (inputScanType != null) 'inputScanType': inputScanType.value,
      if (position != null) 'position': position,
      if (programNumber != null) 'programNumber': programNumber,
      if (psiControl != null) 'psiControl': psiControl.value,
      if (timecodeSource != null) 'timecodeSource': timecodeSource.value,
      if (timecodeStart != null) 'timecodeStart': timecodeStart,
      if (videoOverlays != null) 'videoOverlays': videoOverlays,
      if (videoSelector != null) 'videoSelector': videoSelector,
    };
  }
}

/// Use this Timecode source setting, located under the input settings, to
/// specify how the service counts input video frames. This input frame count
/// affects only the behavior of features that apply to a single input at a
/// time, such as input clipping and synchronizing some captions formats. Choose
/// Embedded to use the timecodes in your input video. Choose Start at zero to
/// start the first frame at zero. Choose Specified start to start the first
/// frame at the timecode that you specify in the setting Start timecode. If you
/// don't specify a value for Timecode source, the service will use Embedded by
/// default. For more information about timecodes, see
/// https://docs.aws.amazon.com/console/mediaconvert/timecode.
enum InputTimecodeSource {
  embedded('EMBEDDED'),
  zerobased('ZEROBASED'),
  specifiedstart('SPECIFIEDSTART'),
  ;

  final String value;

  const InputTimecodeSource(this.value);

  static InputTimecodeSource fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum InputTimecodeSource'));
}

/// When you include Video generator, MediaConvert creates a video input with
/// black frames. Use this setting if you do not have a video input or if you
/// want to add black video frames before, or after, other inputs. You can
/// specify Video generator, or you can specify an Input file, but you cannot
/// specify both. For more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/video-generator.html
class InputVideoGenerator {
  /// Specify the number of audio channels to include in your video generator
  /// input. MediaConvert creates these audio channels as silent audio within a
  /// single audio track. Enter an integer from 1 to 32.
  final int? channels;

  /// Specify the duration, in milliseconds, for your video generator input.
  /// Enter an integer from 50 to 86400000.
  final int? duration;

  /// Specify the denominator of the fraction that represents the frame rate for
  /// your video generator input. When you do, you must also specify a value for
  /// Frame rate numerator. MediaConvert uses a default frame rate of 29.97 when
  /// you leave Frame rate numerator and Frame rate denominator blank.
  final int? framerateDenominator;

  /// Specify the numerator of the fraction that represents the frame rate for
  /// your video generator input. When you do, you must also specify a value for
  /// Frame rate denominator. MediaConvert uses a default frame rate of 29.97 when
  /// you leave Frame rate numerator and Frame rate denominator blank.
  final int? framerateNumerator;

  /// Specify the audio sample rate, in Hz, for the silent audio in your video
  /// generator input.
  /// Enter an integer from 32000 to 48000.
  final int? sampleRate;

  InputVideoGenerator({
    this.channels,
    this.duration,
    this.framerateDenominator,
    this.framerateNumerator,
    this.sampleRate,
  });

  factory InputVideoGenerator.fromJson(Map<String, dynamic> json) {
    return InputVideoGenerator(
      channels: json['channels'] as int?,
      duration: json['duration'] as int?,
      framerateDenominator: json['framerateDenominator'] as int?,
      framerateNumerator: json['framerateNumerator'] as int?,
      sampleRate: json['sampleRate'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final channels = this.channels;
    final duration = this.duration;
    final framerateDenominator = this.framerateDenominator;
    final framerateNumerator = this.framerateNumerator;
    final sampleRate = this.sampleRate;
    return {
      if (channels != null) 'channels': channels,
      if (duration != null) 'duration': duration,
      if (framerateDenominator != null)
        'framerateDenominator': framerateDenominator,
      if (framerateNumerator != null) 'framerateNumerator': framerateNumerator,
      if (sampleRate != null) 'sampleRate': sampleRate,
    };
  }
}

/// These settings apply to a specific graphic overlay. You can include multiple
/// overlays in your job.
class InsertableImage {
  /// Specify the time, in milliseconds, for the image to remain on the output
  /// video. This duration includes fade-in time but not fade-out time.
  final int? duration;

  /// Specify the length of time, in milliseconds, between the Start time that you
  /// specify for the image insertion and the time that the image appears at full
  /// opacity. Full opacity is the level that you specify for the opacity setting.
  /// If you don't specify a value for Fade-in, the image will appear abruptly at
  /// the overlay start time.
  final int? fadeIn;

  /// Specify the length of time, in milliseconds, between the end of the time
  /// that you have specified for the image overlay Duration and when the overlaid
  /// image has faded to total transparency. If you don't specify a value for
  /// Fade-out, the image will disappear abruptly at the end of the inserted image
  /// duration.
  final int? fadeOut;

  /// Specify the height of the inserted image in pixels. If you specify a value
  /// that's larger than the video resolution height, the service will crop your
  /// overlaid image to fit. To use the native height of the image, keep this
  /// setting blank.
  final int? height;

  /// Specify the HTTP, HTTPS, or Amazon S3 location of the image that you want to
  /// overlay on the video. Use a PNG or TGA file.
  final String? imageInserterInput;

  /// Specify the distance, in pixels, between the inserted image and the left
  /// edge of the video frame. Required for any image overlay that you specify.
  final int? imageX;

  /// Specify the distance, in pixels, between the overlaid image and the top edge
  /// of the video frame. Required for any image overlay that you specify.
  final int? imageY;

  /// Specify how overlapping inserted images appear. Images with higher values
  /// for Layer appear on top of images with lower values for Layer.
  final int? layer;

  /// Use Opacity to specify how much of the underlying video shows through the
  /// inserted image. 0 is transparent and 100 is fully opaque. Default is 50.
  final int? opacity;

  /// Specify the timecode of the frame that you want the overlay to first appear
  /// on. This must be in timecode (HH:MM:SS:FF or HH:MM:SS;FF) format. Remember
  /// to take into account your timecode source settings.
  final String? startTime;

  /// Specify the width of the inserted image in pixels. If you specify a value
  /// that's larger than the video resolution width, the service will crop your
  /// overlaid image to fit. To use the native width of the image, keep this
  /// setting blank.
  final int? width;

  InsertableImage({
    this.duration,
    this.fadeIn,
    this.fadeOut,
    this.height,
    this.imageInserterInput,
    this.imageX,
    this.imageY,
    this.layer,
    this.opacity,
    this.startTime,
    this.width,
  });

  factory InsertableImage.fromJson(Map<String, dynamic> json) {
    return InsertableImage(
      duration: json['duration'] as int?,
      fadeIn: json['fadeIn'] as int?,
      fadeOut: json['fadeOut'] as int?,
      height: json['height'] as int?,
      imageInserterInput: json['imageInserterInput'] as String?,
      imageX: json['imageX'] as int?,
      imageY: json['imageY'] as int?,
      layer: json['layer'] as int?,
      opacity: json['opacity'] as int?,
      startTime: json['startTime'] as String?,
      width: json['width'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final duration = this.duration;
    final fadeIn = this.fadeIn;
    final fadeOut = this.fadeOut;
    final height = this.height;
    final imageInserterInput = this.imageInserterInput;
    final imageX = this.imageX;
    final imageY = this.imageY;
    final layer = this.layer;
    final opacity = this.opacity;
    final startTime = this.startTime;
    final width = this.width;
    return {
      if (duration != null) 'duration': duration,
      if (fadeIn != null) 'fadeIn': fadeIn,
      if (fadeOut != null) 'fadeOut': fadeOut,
      if (height != null) 'height': height,
      if (imageInserterInput != null) 'imageInserterInput': imageInserterInput,
      if (imageX != null) 'imageX': imageX,
      if (imageY != null) 'imageY': imageY,
      if (layer != null) 'layer': layer,
      if (opacity != null) 'opacity': opacity,
      if (startTime != null) 'startTime': startTime,
      if (width != null) 'width': width,
    };
  }
}

/// Each job converts an input file into an output file or files. For more
/// information, see the User Guide at
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/what-is.html
class Job {
  /// The IAM role you use for creating this job. For details about permissions,
  /// see the User Guide topic at the User Guide at
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/iam-role.html
  final String role;

  /// JobSettings contains all the transcode settings for a job.
  final JobSettings settings;

  /// Accelerated transcoding can significantly speed up jobs with long, visually
  /// complex content.
  final AccelerationSettings? accelerationSettings;

  /// Describes whether the current job is running with accelerated transcoding.
  /// For jobs that have Acceleration (AccelerationMode) set to DISABLED,
  /// AccelerationStatus is always NOT_APPLICABLE. For jobs that have Acceleration
  /// (AccelerationMode) set to ENABLED or PREFERRED, AccelerationStatus is one of
  /// the other states. AccelerationStatus is IN_PROGRESS initially, while the
  /// service determines whether the input files and job settings are compatible
  /// with accelerated transcoding. If they are, AcclerationStatus is ACCELERATED.
  /// If your input files and job settings aren't compatible with accelerated
  /// transcoding, the service either fails your job or runs it without
  /// accelerated transcoding, depending on how you set Acceleration
  /// (AccelerationMode). When the service runs your job without accelerated
  /// transcoding, AccelerationStatus is NOT_ACCELERATED.
  final AccelerationStatus? accelerationStatus;

  /// An identifier for this resource that is unique within all of AWS.
  final String? arn;

  /// The tag type that AWS Billing and Cost Management will use to sort your AWS
  /// Elemental MediaConvert costs on any billing report that you set up.
  final BillingTagsSource? billingTagsSource;

  /// Prevent duplicate jobs from being created and ensure idempotency for your
  /// requests. A client request token can be any string that includes up to 64
  /// ASCII characters. If you reuse a client request token within one minute of a
  /// successful request, the API returns the job details of the original request
  /// instead. For more information see
  /// https://docs.aws.amazon.com/mediaconvert/latest/apireference/idempotency.html.
  final String? clientRequestToken;

  /// The time, in Unix epoch format in seconds, when the job got created.
  final DateTime? createdAt;

  /// A job's phase can be PROBING, TRANSCODING OR UPLOADING
  final JobPhase? currentPhase;

  /// Error code for the job
  final int? errorCode;

  /// Error message of Job
  final String? errorMessage;

  /// Optional list of hop destinations.
  final List<HopDestination>? hopDestinations;

  /// A portion of the job's ARN, unique within your AWS Elemental MediaConvert
  /// resources
  final String? id;

  /// An estimate of how far your job has progressed. This estimate is shown as a
  /// percentage of the total time from when your job leaves its queue to when
  /// your output files appear in your output Amazon S3 bucket. AWS Elemental
  /// MediaConvert provides jobPercentComplete in CloudWatch STATUS_UPDATE events
  /// and in the response to GetJob and ListJobs requests. The jobPercentComplete
  /// estimate is reliable for the following input containers: Quicktime,
  /// Transport Stream, MP4, and MXF. For some jobs, the service can't provide
  /// information about job progress. In those cases, jobPercentComplete returns a
  /// null value.
  final int? jobPercentComplete;

  /// The job template that the job is created from, if it is created from a job
  /// template.
  final String? jobTemplate;

  /// Provides messages from the service about jobs that you have already
  /// successfully submitted.
  final JobMessages? messages;

  /// List of output group details
  final List<OutputGroupDetail>? outputGroupDetails;

  /// Relative priority on the job.
  final int? priority;

  /// When you create a job, you can specify a queue to send it to. If you don't
  /// specify, the job will go to the default queue. For more about queues, see
  /// the User Guide topic at
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/what-is.html
  final String? queue;

  /// The job's queue hopping history.
  final List<QueueTransition>? queueTransitions;

  /// The number of times that the service automatically attempted to process your
  /// job after encountering an error.
  final int? retryCount;

  /// Enable this setting when you run a test job to estimate how many reserved
  /// transcoding slots (RTS) you need. When this is enabled, MediaConvert runs
  /// your job from an on-demand queue with similar performance to what you will
  /// see with one RTS in a reserved queue. This setting is disabled by default.
  final SimulateReservedQueue? simulateReservedQueue;

  /// A job's status can be SUBMITTED, PROGRESSING, COMPLETE, CANCELED, or ERROR.
  final JobStatus? status;

  /// Specify how often MediaConvert sends STATUS_UPDATE events to Amazon
  /// CloudWatch Events. Set the interval, in seconds, between status updates.
  /// MediaConvert sends an update at this interval from the time the service
  /// begins processing your job to the time it completes the transcode or
  /// encounters an error.
  final StatusUpdateInterval? statusUpdateInterval;

  /// Information about when jobs are submitted, started, and finished is
  /// specified in Unix epoch format in seconds.
  final Timing? timing;

  /// User-defined metadata that you want to associate with an MediaConvert job.
  /// You specify metadata in key/value pairs.
  final Map<String, String>? userMetadata;

  /// Contains any warning messages for the job. Use to help identify potential
  /// issues with your input, output, or job. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/warning_codes.html
  final List<WarningGroup>? warnings;

  Job({
    required this.role,
    required this.settings,
    this.accelerationSettings,
    this.accelerationStatus,
    this.arn,
    this.billingTagsSource,
    this.clientRequestToken,
    this.createdAt,
    this.currentPhase,
    this.errorCode,
    this.errorMessage,
    this.hopDestinations,
    this.id,
    this.jobPercentComplete,
    this.jobTemplate,
    this.messages,
    this.outputGroupDetails,
    this.priority,
    this.queue,
    this.queueTransitions,
    this.retryCount,
    this.simulateReservedQueue,
    this.status,
    this.statusUpdateInterval,
    this.timing,
    this.userMetadata,
    this.warnings,
  });

  factory Job.fromJson(Map<String, dynamic> json) {
    return Job(
      role: json['role'] as String,
      settings: JobSettings.fromJson(json['settings'] as Map<String, dynamic>),
      accelerationSettings: json['accelerationSettings'] != null
          ? AccelerationSettings.fromJson(
              json['accelerationSettings'] as Map<String, dynamic>)
          : null,
      accelerationStatus: (json['accelerationStatus'] as String?)
          ?.let(AccelerationStatus.fromString),
      arn: json['arn'] as String?,
      billingTagsSource: (json['billingTagsSource'] as String?)
          ?.let(BillingTagsSource.fromString),
      clientRequestToken: json['clientRequestToken'] as String?,
      createdAt: timeStampFromJson(json['createdAt']),
      currentPhase: (json['currentPhase'] as String?)?.let(JobPhase.fromString),
      errorCode: json['errorCode'] as int?,
      errorMessage: json['errorMessage'] as String?,
      hopDestinations: (json['hopDestinations'] as List?)
          ?.nonNulls
          .map((e) => HopDestination.fromJson(e as Map<String, dynamic>))
          .toList(),
      id: json['id'] as String?,
      jobPercentComplete: json['jobPercentComplete'] as int?,
      jobTemplate: json['jobTemplate'] as String?,
      messages: json['messages'] != null
          ? JobMessages.fromJson(json['messages'] as Map<String, dynamic>)
          : null,
      outputGroupDetails: (json['outputGroupDetails'] as List?)
          ?.nonNulls
          .map((e) => OutputGroupDetail.fromJson(e as Map<String, dynamic>))
          .toList(),
      priority: json['priority'] as int?,
      queue: json['queue'] as String?,
      queueTransitions: (json['queueTransitions'] as List?)
          ?.nonNulls
          .map((e) => QueueTransition.fromJson(e as Map<String, dynamic>))
          .toList(),
      retryCount: json['retryCount'] as int?,
      simulateReservedQueue: (json['simulateReservedQueue'] as String?)
          ?.let(SimulateReservedQueue.fromString),
      status: (json['status'] as String?)?.let(JobStatus.fromString),
      statusUpdateInterval: (json['statusUpdateInterval'] as String?)
          ?.let(StatusUpdateInterval.fromString),
      timing: json['timing'] != null
          ? Timing.fromJson(json['timing'] as Map<String, dynamic>)
          : null,
      userMetadata: (json['userMetadata'] as Map<String, dynamic>?)
          ?.map((k, e) => MapEntry(k, e as String)),
      warnings: (json['warnings'] as List?)
          ?.nonNulls
          .map((e) => WarningGroup.fromJson(e as Map<String, dynamic>))
          .toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final role = this.role;
    final settings = this.settings;
    final accelerationSettings = this.accelerationSettings;
    final accelerationStatus = this.accelerationStatus;
    final arn = this.arn;
    final billingTagsSource = this.billingTagsSource;
    final clientRequestToken = this.clientRequestToken;
    final createdAt = this.createdAt;
    final currentPhase = this.currentPhase;
    final errorCode = this.errorCode;
    final errorMessage = this.errorMessage;
    final hopDestinations = this.hopDestinations;
    final id = this.id;
    final jobPercentComplete = this.jobPercentComplete;
    final jobTemplate = this.jobTemplate;
    final messages = this.messages;
    final outputGroupDetails = this.outputGroupDetails;
    final priority = this.priority;
    final queue = this.queue;
    final queueTransitions = this.queueTransitions;
    final retryCount = this.retryCount;
    final simulateReservedQueue = this.simulateReservedQueue;
    final status = this.status;
    final statusUpdateInterval = this.statusUpdateInterval;
    final timing = this.timing;
    final userMetadata = this.userMetadata;
    final warnings = this.warnings;
    return {
      'role': role,
      'settings': settings,
      if (accelerationSettings != null)
        'accelerationSettings': accelerationSettings,
      if (accelerationStatus != null)
        'accelerationStatus': accelerationStatus.value,
      if (arn != null) 'arn': arn,
      if (billingTagsSource != null)
        'billingTagsSource': billingTagsSource.value,
      if (clientRequestToken != null) 'clientRequestToken': clientRequestToken,
      if (createdAt != null) 'createdAt': unixTimestampToJson(createdAt),
      if (currentPhase != null) 'currentPhase': currentPhase.value,
      if (errorCode != null) 'errorCode': errorCode,
      if (errorMessage != null) 'errorMessage': errorMessage,
      if (hopDestinations != null) 'hopDestinations': hopDestinations,
      if (id != null) 'id': id,
      if (jobPercentComplete != null) 'jobPercentComplete': jobPercentComplete,
      if (jobTemplate != null) 'jobTemplate': jobTemplate,
      if (messages != null) 'messages': messages,
      if (outputGroupDetails != null) 'outputGroupDetails': outputGroupDetails,
      if (priority != null) 'priority': priority,
      if (queue != null) 'queue': queue,
      if (queueTransitions != null) 'queueTransitions': queueTransitions,
      if (retryCount != null) 'retryCount': retryCount,
      if (simulateReservedQueue != null)
        'simulateReservedQueue': simulateReservedQueue.value,
      if (status != null) 'status': status.value,
      if (statusUpdateInterval != null)
        'statusUpdateInterval': statusUpdateInterval.value,
      if (timing != null) 'timing': timing,
      if (userMetadata != null) 'userMetadata': userMetadata,
      if (warnings != null) 'warnings': warnings,
    };
  }
}

/// Provides messages from the service about jobs that you have already
/// successfully submitted.
class JobMessages {
  /// List of messages that are informational only and don't indicate a problem
  /// with your job.
  final List<String>? info;

  /// List of messages that warn about conditions that might cause your job not to
  /// run or to fail.
  final List<String>? warning;

  JobMessages({
    this.info,
    this.warning,
  });

  factory JobMessages.fromJson(Map<String, dynamic> json) {
    return JobMessages(
      info: (json['info'] as List?)?.nonNulls.map((e) => e as String).toList(),
      warning:
          (json['warning'] as List?)?.nonNulls.map((e) => e as String).toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final info = this.info;
    final warning = this.warning;
    return {
      if (info != null) 'info': info,
      if (warning != null) 'warning': warning,
    };
  }
}

/// A job's phase can be PROBING, TRANSCODING OR UPLOADING
enum JobPhase {
  probing('PROBING'),
  transcoding('TRANSCODING'),
  uploading('UPLOADING'),
  ;

  final String value;

  const JobPhase(this.value);

  static JobPhase fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum JobPhase'));
}

/// JobSettings contains all the transcode settings for a job.
class JobSettings {
  /// When specified, this offset (in milliseconds) is added to the input Ad Avail
  /// PTS time.
  final int? adAvailOffset;

  /// Settings for ad avail blanking. Video can be blanked or overlaid with an
  /// image, and audio muted during SCTE-35 triggered ad avails.
  final AvailBlanking? availBlanking;

  /// Use 3D LUTs to specify custom color mapping behavior when you convert from
  /// one color space into another. You can include up to 8 different 3D LUTs. For
  /// more information, see:
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/3d-luts.html
  final List<ColorConversion3DLUTSetting>? colorConversion3DLUTSettings;

  /// Settings for Event Signaling And Messaging (ESAM). If you don't do ad
  /// insertion, you can ignore these settings.
  final EsamSettings? esam;

  /// If your source content has EIA-608 Line 21 Data Services, enable this
  /// feature to specify what MediaConvert does with the Extended Data Services
  /// (XDS) packets. You can choose to pass through XDS packets, or remove them
  /// from the output. For more information about XDS, see EIA-608 Line Data
  /// Services, section 9.5.1.5 05h Content Advisory.
  final ExtendedDataServices? extendedDataServices;

  /// Specify the input that MediaConvert references for your default output
  /// settings.  MediaConvert uses this input's Resolution, Frame rate, and Pixel
  /// aspect ratio for all  outputs that you don't manually specify different
  /// output settings for. Enabling this setting will disable "Follow source" for
  /// all other inputs.  If MediaConvert cannot follow your source, for example if
  /// you specify an audio-only input,  MediaConvert uses the first followable
  /// input instead. In your JSON job specification, enter an integer from 1 to
  /// 150 corresponding  to the order of your inputs.
  final int? followSource;

  /// Use Inputs to define source file used in the transcode job. There can be
  /// multiple inputs add in a job. These inputs will be concantenated together to
  /// create the output.
  final List<Input>? inputs;

  /// Use these settings only when you use Kantar watermarking. Specify the values
  /// that MediaConvert uses to generate and place Kantar watermarks in your
  /// output audio. These settings apply to every output in your job. In addition
  /// to specifying these values, you also need to store your Kantar credentials
  /// in AWS Secrets Manager. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/kantar-watermarking.html.
  final KantarWatermarkSettings? kantarWatermark;

  /// Overlay motion graphics on top of your video. The motion graphics that you
  /// specify here appear on all outputs in all output groups. For more
  /// information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/motion-graphic-overlay.html.
  final MotionImageInserter? motionImageInserter;

  /// Settings for your Nielsen configuration. If you don't do Nielsen measurement
  /// and analytics, ignore these settings. When you enable Nielsen configuration,
  /// MediaConvert enables PCM to ID3 tagging for all outputs in the job.
  final NielsenConfiguration? nielsenConfiguration;

  /// Ignore these settings unless you are using Nielsen non-linear watermarking.
  /// Specify the values that MediaConvert uses to generate and place Nielsen
  /// watermarks in your output audio. In addition to specifying these values, you
  /// also need to set up your cloud TIC server. These settings apply to every
  /// output in your job. The MediaConvert implementation is currently with the
  /// following Nielsen versions: Nielsen Watermark SDK Version 5.2.1 Nielsen NLM
  /// Watermark Engine Version 1.2.7 Nielsen Watermark Authenticator [SID_TIC]
  /// Version [5.0.0]
  final NielsenNonLinearWatermarkSettings? nielsenNonLinearWatermark;

  /// Contains one group of settings for each set of outputs that share a common
  /// package type. All unpackaged files (MPEG-4, MPEG-2 TS, Quicktime, MXF, and
  /// no container) are grouped in a single output group as well. Required in is a
  /// group of settings that apply to the whole group. This required object
  /// depends on the value you set for Type. Type, settings object pairs are as
  /// follows. * FILE_GROUP_SETTINGS, FileGroupSettings * HLS_GROUP_SETTINGS,
  /// HlsGroupSettings * DASH_ISO_GROUP_SETTINGS, DashIsoGroupSettings *
  /// MS_SMOOTH_GROUP_SETTINGS, MsSmoothGroupSettings * CMAF_GROUP_SETTINGS,
  /// CmafGroupSettings
  final List<OutputGroup>? outputGroups;

  /// These settings control how the service handles timecodes throughout the job.
  /// These settings don't affect input clipping.
  final TimecodeConfig? timecodeConfig;

  /// Insert user-defined custom ID3 metadata at timecodes that you specify. In
  /// each output that you want to include this metadata, you must set ID3
  /// metadata to Passthrough.
  final TimedMetadataInsertion? timedMetadataInsertion;

  JobSettings({
    this.adAvailOffset,
    this.availBlanking,
    this.colorConversion3DLUTSettings,
    this.esam,
    this.extendedDataServices,
    this.followSource,
    this.inputs,
    this.kantarWatermark,
    this.motionImageInserter,
    this.nielsenConfiguration,
    this.nielsenNonLinearWatermark,
    this.outputGroups,
    this.timecodeConfig,
    this.timedMetadataInsertion,
  });

  factory JobSettings.fromJson(Map<String, dynamic> json) {
    return JobSettings(
      adAvailOffset: json['adAvailOffset'] as int?,
      availBlanking: json['availBlanking'] != null
          ? AvailBlanking.fromJson(
              json['availBlanking'] as Map<String, dynamic>)
          : null,
      colorConversion3DLUTSettings: (json['colorConversion3DLUTSettings']
              as List?)
          ?.nonNulls
          .map((e) =>
              ColorConversion3DLUTSetting.fromJson(e as Map<String, dynamic>))
          .toList(),
      esam: json['esam'] != null
          ? EsamSettings.fromJson(json['esam'] as Map<String, dynamic>)
          : null,
      extendedDataServices: json['extendedDataServices'] != null
          ? ExtendedDataServices.fromJson(
              json['extendedDataServices'] as Map<String, dynamic>)
          : null,
      followSource: json['followSource'] as int?,
      inputs: (json['inputs'] as List?)
          ?.nonNulls
          .map((e) => Input.fromJson(e as Map<String, dynamic>))
          .toList(),
      kantarWatermark: json['kantarWatermark'] != null
          ? KantarWatermarkSettings.fromJson(
              json['kantarWatermark'] as Map<String, dynamic>)
          : null,
      motionImageInserter: json['motionImageInserter'] != null
          ? MotionImageInserter.fromJson(
              json['motionImageInserter'] as Map<String, dynamic>)
          : null,
      nielsenConfiguration: json['nielsenConfiguration'] != null
          ? NielsenConfiguration.fromJson(
              json['nielsenConfiguration'] as Map<String, dynamic>)
          : null,
      nielsenNonLinearWatermark: json['nielsenNonLinearWatermark'] != null
          ? NielsenNonLinearWatermarkSettings.fromJson(
              json['nielsenNonLinearWatermark'] as Map<String, dynamic>)
          : null,
      outputGroups: (json['outputGroups'] as List?)
          ?.nonNulls
          .map((e) => OutputGroup.fromJson(e as Map<String, dynamic>))
          .toList(),
      timecodeConfig: json['timecodeConfig'] != null
          ? TimecodeConfig.fromJson(
              json['timecodeConfig'] as Map<String, dynamic>)
          : null,
      timedMetadataInsertion: json['timedMetadataInsertion'] != null
          ? TimedMetadataInsertion.fromJson(
              json['timedMetadataInsertion'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final adAvailOffset = this.adAvailOffset;
    final availBlanking = this.availBlanking;
    final colorConversion3DLUTSettings = this.colorConversion3DLUTSettings;
    final esam = this.esam;
    final extendedDataServices = this.extendedDataServices;
    final followSource = this.followSource;
    final inputs = this.inputs;
    final kantarWatermark = this.kantarWatermark;
    final motionImageInserter = this.motionImageInserter;
    final nielsenConfiguration = this.nielsenConfiguration;
    final nielsenNonLinearWatermark = this.nielsenNonLinearWatermark;
    final outputGroups = this.outputGroups;
    final timecodeConfig = this.timecodeConfig;
    final timedMetadataInsertion = this.timedMetadataInsertion;
    return {
      if (adAvailOffset != null) 'adAvailOffset': adAvailOffset,
      if (availBlanking != null) 'availBlanking': availBlanking,
      if (colorConversion3DLUTSettings != null)
        'colorConversion3DLUTSettings': colorConversion3DLUTSettings,
      if (esam != null) 'esam': esam,
      if (extendedDataServices != null)
        'extendedDataServices': extendedDataServices,
      if (followSource != null) 'followSource': followSource,
      if (inputs != null) 'inputs': inputs,
      if (kantarWatermark != null) 'kantarWatermark': kantarWatermark,
      if (motionImageInserter != null)
        'motionImageInserter': motionImageInserter,
      if (nielsenConfiguration != null)
        'nielsenConfiguration': nielsenConfiguration,
      if (nielsenNonLinearWatermark != null)
        'nielsenNonLinearWatermark': nielsenNonLinearWatermark,
      if (outputGroups != null) 'outputGroups': outputGroups,
      if (timecodeConfig != null) 'timecodeConfig': timecodeConfig,
      if (timedMetadataInsertion != null)
        'timedMetadataInsertion': timedMetadataInsertion,
    };
  }
}

/// A job's status can be SUBMITTED, PROGRESSING, COMPLETE, CANCELED, or ERROR.
enum JobStatus {
  submitted('SUBMITTED'),
  progressing('PROGRESSING'),
  complete('COMPLETE'),
  canceled('CANCELED'),
  error('ERROR'),
  ;

  final String value;

  const JobStatus(this.value);

  static JobStatus fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum JobStatus'));
}

/// A job template is a pre-made set of encoding instructions that you can use
/// to quickly create a job.
class JobTemplate {
  /// A name you create for each job template. Each name must be unique within
  /// your account.
  final String name;

  /// JobTemplateSettings contains all the transcode settings saved in the
  /// template that will be applied to jobs created from it.
  final JobTemplateSettings settings;

  /// Accelerated transcoding can significantly speed up jobs with long, visually
  /// complex content.
  final AccelerationSettings? accelerationSettings;

  /// An identifier for this resource that is unique within all of AWS.
  final String? arn;

  /// An optional category you create to organize your job templates.
  final String? category;

  /// The timestamp in epoch seconds for Job template creation.
  final DateTime? createdAt;

  /// An optional description you create for each job template.
  final String? description;

  /// Optional list of hop destinations.
  final List<HopDestination>? hopDestinations;

  /// The timestamp in epoch seconds when the Job template was last updated.
  final DateTime? lastUpdated;

  /// Relative priority on the job.
  final int? priority;

  /// Optional. The queue that jobs created from this template are assigned to. If
  /// you don't specify this, jobs will go to the default queue.
  final String? queue;

  /// Specify how often MediaConvert sends STATUS_UPDATE events to Amazon
  /// CloudWatch Events. Set the interval, in seconds, between status updates.
  /// MediaConvert sends an update at this interval from the time the service
  /// begins processing your job to the time it completes the transcode or
  /// encounters an error.
  final StatusUpdateInterval? statusUpdateInterval;

  /// A job template can be of two types: system or custom. System or built-in job
  /// templates can't be modified or deleted by the user.
  final Type? type;

  JobTemplate({
    required this.name,
    required this.settings,
    this.accelerationSettings,
    this.arn,
    this.category,
    this.createdAt,
    this.description,
    this.hopDestinations,
    this.lastUpdated,
    this.priority,
    this.queue,
    this.statusUpdateInterval,
    this.type,
  });

  factory JobTemplate.fromJson(Map<String, dynamic> json) {
    return JobTemplate(
      name: json['name'] as String,
      settings: JobTemplateSettings.fromJson(
          json['settings'] as Map<String, dynamic>),
      accelerationSettings: json['accelerationSettings'] != null
          ? AccelerationSettings.fromJson(
              json['accelerationSettings'] as Map<String, dynamic>)
          : null,
      arn: json['arn'] as String?,
      category: json['category'] as String?,
      createdAt: timeStampFromJson(json['createdAt']),
      description: json['description'] as String?,
      hopDestinations: (json['hopDestinations'] as List?)
          ?.nonNulls
          .map((e) => HopDestination.fromJson(e as Map<String, dynamic>))
          .toList(),
      lastUpdated: timeStampFromJson(json['lastUpdated']),
      priority: json['priority'] as int?,
      queue: json['queue'] as String?,
      statusUpdateInterval: (json['statusUpdateInterval'] as String?)
          ?.let(StatusUpdateInterval.fromString),
      type: (json['type'] as String?)?.let(Type.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final name = this.name;
    final settings = this.settings;
    final accelerationSettings = this.accelerationSettings;
    final arn = this.arn;
    final category = this.category;
    final createdAt = this.createdAt;
    final description = this.description;
    final hopDestinations = this.hopDestinations;
    final lastUpdated = this.lastUpdated;
    final priority = this.priority;
    final queue = this.queue;
    final statusUpdateInterval = this.statusUpdateInterval;
    final type = this.type;
    return {
      'name': name,
      'settings': settings,
      if (accelerationSettings != null)
        'accelerationSettings': accelerationSettings,
      if (arn != null) 'arn': arn,
      if (category != null) 'category': category,
      if (createdAt != null) 'createdAt': unixTimestampToJson(createdAt),
      if (description != null) 'description': description,
      if (hopDestinations != null) 'hopDestinations': hopDestinations,
      if (lastUpdated != null) 'lastUpdated': unixTimestampToJson(lastUpdated),
      if (priority != null) 'priority': priority,
      if (queue != null) 'queue': queue,
      if (statusUpdateInterval != null)
        'statusUpdateInterval': statusUpdateInterval.value,
      if (type != null) 'type': type.value,
    };
  }
}

/// Optional. When you request a list of job templates, you can choose to list
/// them alphabetically by NAME or chronologically by CREATION_DATE. If you
/// don't specify, the service will list them by name.
enum JobTemplateListBy {
  name('NAME'),
  creationDate('CREATION_DATE'),
  system('SYSTEM'),
  ;

  final String value;

  const JobTemplateListBy(this.value);

  static JobTemplateListBy fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum JobTemplateListBy'));
}

/// JobTemplateSettings contains all the transcode settings saved in the
/// template that will be applied to jobs created from it.
class JobTemplateSettings {
  /// When specified, this offset (in milliseconds) is added to the input Ad Avail
  /// PTS time.
  final int? adAvailOffset;

  /// Settings for ad avail blanking. Video can be blanked or overlaid with an
  /// image, and audio muted during SCTE-35 triggered ad avails.
  final AvailBlanking? availBlanking;

  /// Use 3D LUTs to specify custom color mapping behavior when you convert from
  /// one color space into another. You can include up to 8 different 3D LUTs. For
  /// more information, see:
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/3d-luts.html
  final List<ColorConversion3DLUTSetting>? colorConversion3DLUTSettings;

  /// Settings for Event Signaling And Messaging (ESAM). If you don't do ad
  /// insertion, you can ignore these settings.
  final EsamSettings? esam;

  /// If your source content has EIA-608 Line 21 Data Services, enable this
  /// feature to specify what MediaConvert does with the Extended Data Services
  /// (XDS) packets. You can choose to pass through XDS packets, or remove them
  /// from the output. For more information about XDS, see EIA-608 Line Data
  /// Services, section 9.5.1.5 05h Content Advisory.
  final ExtendedDataServices? extendedDataServices;

  /// Specify the input that MediaConvert references for your default output
  /// settings.  MediaConvert uses this input's Resolution, Frame rate, and Pixel
  /// aspect ratio for all  outputs that you don't manually specify different
  /// output settings for. Enabling this setting will disable "Follow source" for
  /// all other inputs.  If MediaConvert cannot follow your source, for example if
  /// you specify an audio-only input,  MediaConvert uses the first followable
  /// input instead. In your JSON job specification, enter an integer from 1 to
  /// 150 corresponding  to the order of your inputs.
  final int? followSource;

  /// Use Inputs to define the source file used in the transcode job. There can
  /// only be one input in a job template. Using the API, you can include multiple
  /// inputs when referencing a job template.
  final List<InputTemplate>? inputs;

  /// Use these settings only when you use Kantar watermarking. Specify the values
  /// that MediaConvert uses to generate and place Kantar watermarks in your
  /// output audio. These settings apply to every output in your job. In addition
  /// to specifying these values, you also need to store your Kantar credentials
  /// in AWS Secrets Manager. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/kantar-watermarking.html.
  final KantarWatermarkSettings? kantarWatermark;

  /// Overlay motion graphics on top of your video. The motion graphics that you
  /// specify here appear on all outputs in all output groups. For more
  /// information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/motion-graphic-overlay.html.
  final MotionImageInserter? motionImageInserter;

  /// Settings for your Nielsen configuration. If you don't do Nielsen measurement
  /// and analytics, ignore these settings. When you enable Nielsen configuration,
  /// MediaConvert enables PCM to ID3 tagging for all outputs in the job.
  final NielsenConfiguration? nielsenConfiguration;

  /// Ignore these settings unless you are using Nielsen non-linear watermarking.
  /// Specify the values that MediaConvert uses to generate and place Nielsen
  /// watermarks in your output audio. In addition to specifying these values, you
  /// also need to set up your cloud TIC server. These settings apply to every
  /// output in your job. The MediaConvert implementation is currently with the
  /// following Nielsen versions: Nielsen Watermark SDK Version 5.2.1 Nielsen NLM
  /// Watermark Engine Version 1.2.7 Nielsen Watermark Authenticator [SID_TIC]
  /// Version [5.0.0]
  final NielsenNonLinearWatermarkSettings? nielsenNonLinearWatermark;

  /// Contains one group of settings for each set of outputs that share a common
  /// package type. All unpackaged files (MPEG-4, MPEG-2 TS, Quicktime, MXF, and
  /// no container) are grouped in a single output group as well. Required in is a
  /// group of settings that apply to the whole group. This required object
  /// depends on the value you set for Type. Type, settings object pairs are as
  /// follows. * FILE_GROUP_SETTINGS, FileGroupSettings * HLS_GROUP_SETTINGS,
  /// HlsGroupSettings * DASH_ISO_GROUP_SETTINGS, DashIsoGroupSettings *
  /// MS_SMOOTH_GROUP_SETTINGS, MsSmoothGroupSettings * CMAF_GROUP_SETTINGS,
  /// CmafGroupSettings
  final List<OutputGroup>? outputGroups;

  /// These settings control how the service handles timecodes throughout the job.
  /// These settings don't affect input clipping.
  final TimecodeConfig? timecodeConfig;

  /// Insert user-defined custom ID3 metadata at timecodes that you specify. In
  /// each output that you want to include this metadata, you must set ID3
  /// metadata to Passthrough.
  final TimedMetadataInsertion? timedMetadataInsertion;

  JobTemplateSettings({
    this.adAvailOffset,
    this.availBlanking,
    this.colorConversion3DLUTSettings,
    this.esam,
    this.extendedDataServices,
    this.followSource,
    this.inputs,
    this.kantarWatermark,
    this.motionImageInserter,
    this.nielsenConfiguration,
    this.nielsenNonLinearWatermark,
    this.outputGroups,
    this.timecodeConfig,
    this.timedMetadataInsertion,
  });

  factory JobTemplateSettings.fromJson(Map<String, dynamic> json) {
    return JobTemplateSettings(
      adAvailOffset: json['adAvailOffset'] as int?,
      availBlanking: json['availBlanking'] != null
          ? AvailBlanking.fromJson(
              json['availBlanking'] as Map<String, dynamic>)
          : null,
      colorConversion3DLUTSettings: (json['colorConversion3DLUTSettings']
              as List?)
          ?.nonNulls
          .map((e) =>
              ColorConversion3DLUTSetting.fromJson(e as Map<String, dynamic>))
          .toList(),
      esam: json['esam'] != null
          ? EsamSettings.fromJson(json['esam'] as Map<String, dynamic>)
          : null,
      extendedDataServices: json['extendedDataServices'] != null
          ? ExtendedDataServices.fromJson(
              json['extendedDataServices'] as Map<String, dynamic>)
          : null,
      followSource: json['followSource'] as int?,
      inputs: (json['inputs'] as List?)
          ?.nonNulls
          .map((e) => InputTemplate.fromJson(e as Map<String, dynamic>))
          .toList(),
      kantarWatermark: json['kantarWatermark'] != null
          ? KantarWatermarkSettings.fromJson(
              json['kantarWatermark'] as Map<String, dynamic>)
          : null,
      motionImageInserter: json['motionImageInserter'] != null
          ? MotionImageInserter.fromJson(
              json['motionImageInserter'] as Map<String, dynamic>)
          : null,
      nielsenConfiguration: json['nielsenConfiguration'] != null
          ? NielsenConfiguration.fromJson(
              json['nielsenConfiguration'] as Map<String, dynamic>)
          : null,
      nielsenNonLinearWatermark: json['nielsenNonLinearWatermark'] != null
          ? NielsenNonLinearWatermarkSettings.fromJson(
              json['nielsenNonLinearWatermark'] as Map<String, dynamic>)
          : null,
      outputGroups: (json['outputGroups'] as List?)
          ?.nonNulls
          .map((e) => OutputGroup.fromJson(e as Map<String, dynamic>))
          .toList(),
      timecodeConfig: json['timecodeConfig'] != null
          ? TimecodeConfig.fromJson(
              json['timecodeConfig'] as Map<String, dynamic>)
          : null,
      timedMetadataInsertion: json['timedMetadataInsertion'] != null
          ? TimedMetadataInsertion.fromJson(
              json['timedMetadataInsertion'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final adAvailOffset = this.adAvailOffset;
    final availBlanking = this.availBlanking;
    final colorConversion3DLUTSettings = this.colorConversion3DLUTSettings;
    final esam = this.esam;
    final extendedDataServices = this.extendedDataServices;
    final followSource = this.followSource;
    final inputs = this.inputs;
    final kantarWatermark = this.kantarWatermark;
    final motionImageInserter = this.motionImageInserter;
    final nielsenConfiguration = this.nielsenConfiguration;
    final nielsenNonLinearWatermark = this.nielsenNonLinearWatermark;
    final outputGroups = this.outputGroups;
    final timecodeConfig = this.timecodeConfig;
    final timedMetadataInsertion = this.timedMetadataInsertion;
    return {
      if (adAvailOffset != null) 'adAvailOffset': adAvailOffset,
      if (availBlanking != null) 'availBlanking': availBlanking,
      if (colorConversion3DLUTSettings != null)
        'colorConversion3DLUTSettings': colorConversion3DLUTSettings,
      if (esam != null) 'esam': esam,
      if (extendedDataServices != null)
        'extendedDataServices': extendedDataServices,
      if (followSource != null) 'followSource': followSource,
      if (inputs != null) 'inputs': inputs,
      if (kantarWatermark != null) 'kantarWatermark': kantarWatermark,
      if (motionImageInserter != null)
        'motionImageInserter': motionImageInserter,
      if (nielsenConfiguration != null)
        'nielsenConfiguration': nielsenConfiguration,
      if (nielsenNonLinearWatermark != null)
        'nielsenNonLinearWatermark': nielsenNonLinearWatermark,
      if (outputGroups != null) 'outputGroups': outputGroups,
      if (timecodeConfig != null) 'timecodeConfig': timecodeConfig,
      if (timedMetadataInsertion != null)
        'timedMetadataInsertion': timedMetadataInsertion,
    };
  }
}

/// Use these settings only when you use Kantar watermarking. Specify the values
/// that MediaConvert uses to generate and place Kantar watermarks in your
/// output audio. These settings apply to every output in your job. In addition
/// to specifying these values, you also need to store your Kantar credentials
/// in AWS Secrets Manager. For more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/kantar-watermarking.html.
class KantarWatermarkSettings {
  /// Provide an audio channel name from your Kantar audio license.
  final String? channelName;

  /// Specify a unique identifier for Kantar to use for this piece of content.
  final String? contentReference;

  /// Provide the name of the AWS Secrets Manager secret where your Kantar
  /// credentials are stored. Note that your MediaConvert service role must
  /// provide access to this secret. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/granting-permissions-for-mediaconvert-to-access-secrets-manager-secret.html.
  /// For instructions on creating a secret, see
  /// https://docs.aws.amazon.com/secretsmanager/latest/userguide/tutorials_basic.html,
  /// in the AWS Secrets Manager User Guide.
  final String? credentialsSecretName;

  /// Optional. Specify an offset, in whole seconds, from the start of your output
  /// and the beginning of the watermarking. When you don't specify an offset,
  /// Kantar defaults to zero.
  final double? fileOffset;

  /// Provide your Kantar license ID number. You should get this number from
  /// Kantar.
  final int? kantarLicenseId;

  /// Provide the HTTPS endpoint to the Kantar server. You should get this
  /// endpoint from Kantar.
  final String? kantarServerUrl;

  /// Optional. Specify the Amazon S3 bucket where you want MediaConvert to store
  /// your Kantar watermark XML logs. When you don't specify a bucket,
  /// MediaConvert doesn't save these logs. Note that your MediaConvert service
  /// role must provide access to this location. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/iam-role.html
  final String? logDestination;

  /// You can optionally use this field to specify the first timestamp that Kantar
  /// embeds during watermarking. Kantar suggests that you be very cautious when
  /// using this Kantar feature, and that you use it only on channels that are
  /// managed specifically for use with this feature by your Audience Measurement
  /// Operator. For more information about this feature, contact Kantar technical
  /// support.
  final String? metadata3;

  /// Additional metadata that MediaConvert sends to Kantar. Maximum length is 50
  /// characters.
  final String? metadata4;

  /// Additional metadata that MediaConvert sends to Kantar. Maximum length is 50
  /// characters.
  final String? metadata5;

  /// Additional metadata that MediaConvert sends to Kantar. Maximum length is 50
  /// characters.
  final String? metadata6;

  /// Additional metadata that MediaConvert sends to Kantar. Maximum length is 50
  /// characters.
  final String? metadata7;

  /// Additional metadata that MediaConvert sends to Kantar. Maximum length is 50
  /// characters.
  final String? metadata8;

  KantarWatermarkSettings({
    this.channelName,
    this.contentReference,
    this.credentialsSecretName,
    this.fileOffset,
    this.kantarLicenseId,
    this.kantarServerUrl,
    this.logDestination,
    this.metadata3,
    this.metadata4,
    this.metadata5,
    this.metadata6,
    this.metadata7,
    this.metadata8,
  });

  factory KantarWatermarkSettings.fromJson(Map<String, dynamic> json) {
    return KantarWatermarkSettings(
      channelName: json['channelName'] as String?,
      contentReference: json['contentReference'] as String?,
      credentialsSecretName: json['credentialsSecretName'] as String?,
      fileOffset: json['fileOffset'] as double?,
      kantarLicenseId: json['kantarLicenseId'] as int?,
      kantarServerUrl: json['kantarServerUrl'] as String?,
      logDestination: json['logDestination'] as String?,
      metadata3: json['metadata3'] as String?,
      metadata4: json['metadata4'] as String?,
      metadata5: json['metadata5'] as String?,
      metadata6: json['metadata6'] as String?,
      metadata7: json['metadata7'] as String?,
      metadata8: json['metadata8'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final channelName = this.channelName;
    final contentReference = this.contentReference;
    final credentialsSecretName = this.credentialsSecretName;
    final fileOffset = this.fileOffset;
    final kantarLicenseId = this.kantarLicenseId;
    final kantarServerUrl = this.kantarServerUrl;
    final logDestination = this.logDestination;
    final metadata3 = this.metadata3;
    final metadata4 = this.metadata4;
    final metadata5 = this.metadata5;
    final metadata6 = this.metadata6;
    final metadata7 = this.metadata7;
    final metadata8 = this.metadata8;
    return {
      if (channelName != null) 'channelName': channelName,
      if (contentReference != null) 'contentReference': contentReference,
      if (credentialsSecretName != null)
        'credentialsSecretName': credentialsSecretName,
      if (fileOffset != null) 'fileOffset': fileOffset,
      if (kantarLicenseId != null) 'kantarLicenseId': kantarLicenseId,
      if (kantarServerUrl != null) 'kantarServerUrl': kantarServerUrl,
      if (logDestination != null) 'logDestination': logDestination,
      if (metadata3 != null) 'metadata3': metadata3,
      if (metadata4 != null) 'metadata4': metadata4,
      if (metadata5 != null) 'metadata5': metadata5,
      if (metadata6 != null) 'metadata6': metadata6,
      if (metadata7 != null) 'metadata7': metadata7,
      if (metadata8 != null) 'metadata8': metadata8,
    };
  }
}

/// Specify the language, using the ISO 639-2 three-letter code listed at
/// https://www.loc.gov/standards/iso639-2/php/code_list.php.
enum LanguageCode {
  eng('ENG'),
  spa('SPA'),
  fra('FRA'),
  deu('DEU'),
  ger('GER'),
  zho('ZHO'),
  ara('ARA'),
  hin('HIN'),
  jpn('JPN'),
  rus('RUS'),
  por('POR'),
  ita('ITA'),
  urd('URD'),
  vie('VIE'),
  kor('KOR'),
  pan('PAN'),
  abk('ABK'),
  aar('AAR'),
  afr('AFR'),
  aka('AKA'),
  sqi('SQI'),
  amh('AMH'),
  arg('ARG'),
  hye('HYE'),
  asm('ASM'),
  ava('AVA'),
  ave('AVE'),
  aym('AYM'),
  aze('AZE'),
  bam('BAM'),
  bak('BAK'),
  eus('EUS'),
  bel('BEL'),
  ben('BEN'),
  bih('BIH'),
  bis('BIS'),
  bos('BOS'),
  bre('BRE'),
  bul('BUL'),
  mya('MYA'),
  cat('CAT'),
  khm('KHM'),
  cha('CHA'),
  che('CHE'),
  nya('NYA'),
  chu('CHU'),
  chv('CHV'),
  cor('COR'),
  cos('COS'),
  cre('CRE'),
  hrv('HRV'),
  ces('CES'),
  dan('DAN'),
  div('DIV'),
  nld('NLD'),
  dzo('DZO'),
  enm('ENM'),
  epo('EPO'),
  est('EST'),
  ewe('EWE'),
  fao('FAO'),
  fij('FIJ'),
  fin('FIN'),
  frm('FRM'),
  ful('FUL'),
  gla('GLA'),
  glg('GLG'),
  lug('LUG'),
  kat('KAT'),
  ell('ELL'),
  grn('GRN'),
  guj('GUJ'),
  hat('HAT'),
  hau('HAU'),
  heb('HEB'),
  her('HER'),
  hmo('HMO'),
  hun('HUN'),
  isl('ISL'),
  ido('IDO'),
  ibo('IBO'),
  ind('IND'),
  ina('INA'),
  ile('ILE'),
  iku('IKU'),
  ipk('IPK'),
  gle('GLE'),
  jav('JAV'),
  kal('KAL'),
  kan('KAN'),
  kau('KAU'),
  kas('KAS'),
  kaz('KAZ'),
  kik('KIK'),
  kin('KIN'),
  kir('KIR'),
  kom('KOM'),
  kon('KON'),
  kua('KUA'),
  kur('KUR'),
  lao('LAO'),
  lat('LAT'),
  lav('LAV'),
  lim('LIM'),
  lin('LIN'),
  lit('LIT'),
  lub('LUB'),
  ltz('LTZ'),
  mkd('MKD'),
  mlg('MLG'),
  msa('MSA'),
  mal('MAL'),
  mlt('MLT'),
  glv('GLV'),
  mri('MRI'),
  mar('MAR'),
  mah('MAH'),
  mon('MON'),
  nau('NAU'),
  nav('NAV'),
  nde('NDE'),
  nbl('NBL'),
  ndo('NDO'),
  nep('NEP'),
  sme('SME'),
  nor('NOR'),
  nob('NOB'),
  nno('NNO'),
  oci('OCI'),
  oji('OJI'),
  ori('ORI'),
  orm('ORM'),
  oss('OSS'),
  pli('PLI'),
  fas('FAS'),
  pol('POL'),
  pus('PUS'),
  que('QUE'),
  qaa('QAA'),
  ron('RON'),
  roh('ROH'),
  run('RUN'),
  smo('SMO'),
  sag('SAG'),
  san('SAN'),
  srd('SRD'),
  srb('SRB'),
  sna('SNA'),
  iii('III'),
  snd('SND'),
  sin('SIN'),
  slk('SLK'),
  slv('SLV'),
  som('SOM'),
  sot('SOT'),
  sun('SUN'),
  swa('SWA'),
  ssw('SSW'),
  swe('SWE'),
  tgl('TGL'),
  tah('TAH'),
  tgk('TGK'),
  tam('TAM'),
  tat('TAT'),
  tel('TEL'),
  tha('THA'),
  bod('BOD'),
  tir('TIR'),
  ton('TON'),
  tso('TSO'),
  tsn('TSN'),
  tur('TUR'),
  tuk('TUK'),
  twi('TWI'),
  uig('UIG'),
  ukr('UKR'),
  uzb('UZB'),
  ven('VEN'),
  vol('VOL'),
  wln('WLN'),
  cym('CYM'),
  fry('FRY'),
  wol('WOL'),
  xho('XHO'),
  yid('YID'),
  yor('YOR'),
  zha('ZHA'),
  zul('ZUL'),
  orj('ORJ'),
  qpc('QPC'),
  tng('TNG'),
  srp('SRP'),
  ;

  final String value;

  const LanguageCode(this.value);

  static LanguageCode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum LanguageCode'));
}

class ListJobTemplatesResponse {
  /// List of Job templates.
  final List<JobTemplate>? jobTemplates;

  /// Use this string to request the next batch of job templates.
  final String? nextToken;

  ListJobTemplatesResponse({
    this.jobTemplates,
    this.nextToken,
  });

  factory ListJobTemplatesResponse.fromJson(Map<String, dynamic> json) {
    return ListJobTemplatesResponse(
      jobTemplates: (json['jobTemplates'] as List?)
          ?.nonNulls
          .map((e) => JobTemplate.fromJson(e as Map<String, dynamic>))
          .toList(),
      nextToken: json['nextToken'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final jobTemplates = this.jobTemplates;
    final nextToken = this.nextToken;
    return {
      if (jobTemplates != null) 'jobTemplates': jobTemplates,
      if (nextToken != null) 'nextToken': nextToken,
    };
  }
}

class ListJobsResponse {
  /// List of jobs
  final List<Job>? jobs;

  /// Use this string to request the next batch of jobs.
  final String? nextToken;

  ListJobsResponse({
    this.jobs,
    this.nextToken,
  });

  factory ListJobsResponse.fromJson(Map<String, dynamic> json) {
    return ListJobsResponse(
      jobs: (json['jobs'] as List?)
          ?.nonNulls
          .map((e) => Job.fromJson(e as Map<String, dynamic>))
          .toList(),
      nextToken: json['nextToken'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final jobs = this.jobs;
    final nextToken = this.nextToken;
    return {
      if (jobs != null) 'jobs': jobs,
      if (nextToken != null) 'nextToken': nextToken,
    };
  }
}

class ListPresetsResponse {
  /// Use this string to request the next batch of presets.
  final String? nextToken;

  /// List of presets
  final List<Preset>? presets;

  ListPresetsResponse({
    this.nextToken,
    this.presets,
  });

  factory ListPresetsResponse.fromJson(Map<String, dynamic> json) {
    return ListPresetsResponse(
      nextToken: json['nextToken'] as String?,
      presets: (json['presets'] as List?)
          ?.nonNulls
          .map((e) => Preset.fromJson(e as Map<String, dynamic>))
          .toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final nextToken = this.nextToken;
    final presets = this.presets;
    return {
      if (nextToken != null) 'nextToken': nextToken,
      if (presets != null) 'presets': presets,
    };
  }
}

class ListQueuesResponse {
  /// Use this string to request the next batch of queues.
  final String? nextToken;

  /// List of queues.
  final List<Queue>? queues;

  ListQueuesResponse({
    this.nextToken,
    this.queues,
  });

  factory ListQueuesResponse.fromJson(Map<String, dynamic> json) {
    return ListQueuesResponse(
      nextToken: json['nextToken'] as String?,
      queues: (json['queues'] as List?)
          ?.nonNulls
          .map((e) => Queue.fromJson(e as Map<String, dynamic>))
          .toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final nextToken = this.nextToken;
    final queues = this.queues;
    return {
      if (nextToken != null) 'nextToken': nextToken,
      if (queues != null) 'queues': queues,
    };
  }
}

class ListTagsForResourceResponse {
  /// The Amazon Resource Name (ARN) and tags for an AWS Elemental MediaConvert
  /// resource.
  final ResourceTags? resourceTags;

  ListTagsForResourceResponse({
    this.resourceTags,
  });

  factory ListTagsForResourceResponse.fromJson(Map<String, dynamic> json) {
    return ListTagsForResourceResponse(
      resourceTags: json['resourceTags'] != null
          ? ResourceTags.fromJson(json['resourceTags'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final resourceTags = this.resourceTags;
    return {
      if (resourceTags != null) 'resourceTags': resourceTags,
    };
  }
}

/// Selects between the DVB and ATSC buffer models for Dolby Digital audio.
enum M2tsAudioBufferModel {
  dvb('DVB'),
  atsc('ATSC'),
  ;

  final String value;

  const M2tsAudioBufferModel(this.value);

  static M2tsAudioBufferModel fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum M2tsAudioBufferModel'));
}

/// Specify this setting only when your output will be consumed by a downstream
/// repackaging workflow that is sensitive to very small duration differences
/// between video and audio. For this situation, choose Match video duration. In
/// all other cases, keep the default value, Default codec duration. When you
/// choose Match video duration, MediaConvert pads the output audio streams with
/// silence or trims them to ensure that the total duration of each audio stream
/// is at least as long as the total duration of the video stream. After padding
/// or trimming, the audio stream duration is no more than one frame longer than
/// the video stream. MediaConvert applies audio padding or trimming only to the
/// end of the last segment of the output. For unsegmented outputs, MediaConvert
/// adds padding only to the end of the file. When you keep the default value,
/// any minor discrepancies between audio and video duration will depend on your
/// output audio codec.
enum M2tsAudioDuration {
  defaultCodecDuration('DEFAULT_CODEC_DURATION'),
  matchVideoDuration('MATCH_VIDEO_DURATION'),
  ;

  final String value;

  const M2tsAudioDuration(this.value);

  static M2tsAudioDuration fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum M2tsAudioDuration'));
}

/// Controls what buffer model to use for accurate interleaving. If set to
/// MULTIPLEX, use multiplex buffer model. If set to NONE, this can lead to
/// lower latency, but low-memory devices may not be able to play back the
/// stream without interruptions.
enum M2tsBufferModel {
  multiplex('MULTIPLEX'),
  none('NONE'),
  ;

  final String value;

  const M2tsBufferModel(this.value);

  static M2tsBufferModel fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum M2tsBufferModel'));
}

/// If you select ALIGN_TO_VIDEO, MediaConvert writes captions and data packets
/// with Presentation Timestamp (PTS) values greater than or equal to the first
/// video packet PTS (MediaConvert drops captions and data packets with lesser
/// PTS values). Keep the default value to allow all PTS values.
enum M2tsDataPtsControl {
  auto('AUTO'),
  alignToVideo('ALIGN_TO_VIDEO'),
  ;

  final String value;

  const M2tsDataPtsControl(this.value);

  static M2tsDataPtsControl fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum M2tsDataPtsControl'));
}

/// When set to VIDEO_AND_FIXED_INTERVALS, audio EBP markers will be added to
/// partitions 3 and 4. The interval between these additional markers will be
/// fixed, and will be slightly shorter than the video EBP marker interval. When
/// set to VIDEO_INTERVAL, these additional markers will not be inserted. Only
/// applicable when EBP segmentation markers are is selected
/// (segmentationMarkers is EBP or EBP_LEGACY).
enum M2tsEbpAudioInterval {
  videoAndFixedIntervals('VIDEO_AND_FIXED_INTERVALS'),
  videoInterval('VIDEO_INTERVAL'),
  ;

  final String value;

  const M2tsEbpAudioInterval(this.value);

  static M2tsEbpAudioInterval fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum M2tsEbpAudioInterval'));
}

/// Selects which PIDs to place EBP markers on. They can either be placed only
/// on the video PID, or on both the video PID and all audio PIDs. Only
/// applicable when EBP segmentation markers are is selected
/// (segmentationMarkers is EBP or EBP_LEGACY).
enum M2tsEbpPlacement {
  videoAndAudioPids('VIDEO_AND_AUDIO_PIDS'),
  videoPid('VIDEO_PID'),
  ;

  final String value;

  const M2tsEbpPlacement(this.value);

  static M2tsEbpPlacement fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum M2tsEbpPlacement'));
}

/// Controls whether to include the ES Rate field in the PES header.
enum M2tsEsRateInPes {
  include('INCLUDE'),
  exclude('EXCLUDE'),
  ;

  final String value;

  const M2tsEsRateInPes(this.value);

  static M2tsEsRateInPes fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum M2tsEsRateInPes'));
}

/// Keep the default value unless you know that your audio EBP markers are
/// incorrectly appearing before your video EBP markers. To correct this
/// problem, set this value to Force.
enum M2tsForceTsVideoEbpOrder {
  force('FORCE'),
  $default('DEFAULT'),
  ;

  final String value;

  const M2tsForceTsVideoEbpOrder(this.value);

  static M2tsForceTsVideoEbpOrder fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum M2tsForceTsVideoEbpOrder'));
}

/// To include key-length-value metadata in this output: Set KLV metadata
/// insertion to Passthrough. MediaConvert reads KLV metadata present in your
/// input and passes it through to the output transport stream. To exclude this
/// KLV metadata: Set KLV metadata insertion to None or leave blank.
enum M2tsKlvMetadata {
  passthrough('PASSTHROUGH'),
  none('NONE'),
  ;

  final String value;

  const M2tsKlvMetadata(this.value);

  static M2tsKlvMetadata fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum M2tsKlvMetadata'));
}

/// If INSERT, Nielsen inaudible tones for media tracking will be detected in
/// the input audio and an equivalent ID3 tag will be inserted in the output.
enum M2tsNielsenId3 {
  insert('INSERT'),
  none('NONE'),
  ;

  final String value;

  const M2tsNielsenId3(this.value);

  static M2tsNielsenId3 fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum M2tsNielsenId3'));
}

/// When set to PCR_EVERY_PES_PACKET, a Program Clock Reference value is
/// inserted for every Packetized Elementary Stream (PES) header. This is
/// effective only when the PCR PID is the same as the video or audio elementary
/// stream.
enum M2tsPcrControl {
  pcrEveryPesPacket('PCR_EVERY_PES_PACKET'),
  configuredPcrPeriod('CONFIGURED_PCR_PERIOD'),
  ;

  final String value;

  const M2tsPcrControl(this.value);

  static M2tsPcrControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum M2tsPcrControl'));
}

/// Specify whether MediaConvert automatically attempts to prevent decoder
/// buffer underflows in your transport stream output. Use if you are seeing
/// decoder buffer underflows in your output and are unable to increase your
/// transport stream's bitrate. For most workflows: We recommend that you keep
/// the default value, Disabled. To prevent decoder buffer underflows in your
/// output, when possible: Choose Enabled. Note that if MediaConvert prevents a
/// decoder buffer underflow in your output, output video quality is reduced and
/// your job will take longer to complete.
enum M2tsPreventBufferUnderflow {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const M2tsPreventBufferUnderflow(this.value);

  static M2tsPreventBufferUnderflow fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum M2tsPreventBufferUnderflow'));
}

/// When set to CBR, inserts null packets into transport stream to fill
/// specified bitrate. When set to VBR, the bitrate setting acts as the maximum
/// bitrate, but the output will not be padded up to that bitrate.
enum M2tsRateMode {
  vbr('VBR'),
  cbr('CBR'),
  ;

  final String value;

  const M2tsRateMode(this.value);

  static M2tsRateMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum M2tsRateMode'));
}

/// Settings for SCTE-35 signals from ESAM. Include this in your job settings to
/// put SCTE-35 markers in your HLS and transport stream outputs at the
/// insertion points that you specify in an ESAM XML document. Provide the
/// document in the setting SCC XML.
class M2tsScte35Esam {
  /// Packet Identifier (PID) of the SCTE-35 stream in the transport stream
  /// generated by ESAM.
  final int? scte35EsamPid;

  M2tsScte35Esam({
    this.scte35EsamPid,
  });

  factory M2tsScte35Esam.fromJson(Map<String, dynamic> json) {
    return M2tsScte35Esam(
      scte35EsamPid: json['scte35EsamPid'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final scte35EsamPid = this.scte35EsamPid;
    return {
      if (scte35EsamPid != null) 'scte35EsamPid': scte35EsamPid,
    };
  }
}

/// For SCTE-35 markers from your input-- Choose Passthrough if you want SCTE-35
/// markers that appear in your input to also appear in this output. Choose None
/// if you don't want SCTE-35 markers in this output. For SCTE-35 markers from
/// an ESAM XML document-- Choose None. Also provide the ESAM XML as a string in
/// the setting Signal processing notification XML. Also enable ESAM SCTE-35
/// (include the property scte35Esam).
enum M2tsScte35Source {
  passthrough('PASSTHROUGH'),
  none('NONE'),
  ;

  final String value;

  const M2tsScte35Source(this.value);

  static M2tsScte35Source fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum M2tsScte35Source'));
}

/// Inserts segmentation markers at each segmentation_time period. rai_segstart
/// sets the Random Access Indicator bit in the adaptation field. rai_adapt sets
/// the RAI bit and adds the current timecode in the private data bytes.
/// psi_segstart inserts PAT and PMT tables at the start of segments. ebp adds
/// Encoder Boundary Point information to the adaptation field as per OpenCable
/// specification OC-SP-EBP-I01-130118. ebp_legacy adds Encoder Boundary Point
/// information to the adaptation field using a legacy proprietary format.
enum M2tsSegmentationMarkers {
  none('NONE'),
  raiSegstart('RAI_SEGSTART'),
  raiAdapt('RAI_ADAPT'),
  psiSegstart('PSI_SEGSTART'),
  ebp('EBP'),
  ebpLegacy('EBP_LEGACY'),
  ;

  final String value;

  const M2tsSegmentationMarkers(this.value);

  static M2tsSegmentationMarkers fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum M2tsSegmentationMarkers'));
}

/// The segmentation style parameter controls how segmentation markers are
/// inserted into the transport stream. With avails, it is possible that
/// segments may be truncated, which can influence where future segmentation
/// markers are inserted. When a segmentation style of "reset_cadence" is
/// selected and a segment is truncated due to an avail, we will reset the
/// segmentation cadence. This means the subsequent segment will have a duration
/// of of $segmentation_time seconds. When a segmentation style of
/// "maintain_cadence" is selected and a segment is truncated due to an avail,
/// we will not reset the segmentation cadence. This means the subsequent
/// segment will likely be truncated as well. However, all segments after that
/// will have a duration of $segmentation_time seconds. Note that EBP lookahead
/// is a slight exception to this rule.
enum M2tsSegmentationStyle {
  maintainCadence('MAINTAIN_CADENCE'),
  resetCadence('RESET_CADENCE'),
  ;

  final String value;

  const M2tsSegmentationStyle(this.value);

  static M2tsSegmentationStyle fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum M2tsSegmentationStyle'));
}

/// MPEG-2 TS container settings. These apply to outputs in a File output group
/// when the output's container is MPEG-2 Transport Stream (M2TS). In these
/// assets, data is organized by the program map table (PMT). Each transport
/// stream program contains subsets of data, including audio, video, and
/// metadata. Each of these subsets of data has a numerical label called a
/// packet identifier (PID). Each transport stream program corresponds to one
/// MediaConvert output. The PMT lists the types of data in a program along with
/// their PID. Downstream systems and players use the program map table to look
/// up the PID for each type of data it accesses and then uses the PIDs to
/// locate specific data within the asset.
class M2tsSettings {
  /// Selects between the DVB and ATSC buffer models for Dolby Digital audio.
  final M2tsAudioBufferModel? audioBufferModel;

  /// Specify this setting only when your output will be consumed by a downstream
  /// repackaging workflow that is sensitive to very small duration differences
  /// between video and audio. For this situation, choose Match video duration. In
  /// all other cases, keep the default value, Default codec duration. When you
  /// choose Match video duration, MediaConvert pads the output audio streams with
  /// silence or trims them to ensure that the total duration of each audio stream
  /// is at least as long as the total duration of the video stream. After padding
  /// or trimming, the audio stream duration is no more than one frame longer than
  /// the video stream. MediaConvert applies audio padding or trimming only to the
  /// end of the last segment of the output. For unsegmented outputs, MediaConvert
  /// adds padding only to the end of the file. When you keep the default value,
  /// any minor discrepancies between audio and video duration will depend on your
  /// output audio codec.
  final M2tsAudioDuration? audioDuration;

  /// The number of audio frames to insert for each PES packet.
  final int? audioFramesPerPes;

  /// Specify the packet identifiers (PIDs) for any elementary audio streams you
  /// include in this output. Specify multiple PIDs as a JSON array. Default is
  /// the range 482-492.
  final List<int>? audioPids;

  /// Specify the output bitrate of the transport stream in bits per second.
  /// Setting to 0 lets the muxer automatically determine the appropriate bitrate.
  /// Other common values are 3750000, 7500000, and 15000000.
  final int? bitrate;

  /// Controls what buffer model to use for accurate interleaving. If set to
  /// MULTIPLEX, use multiplex buffer model. If set to NONE, this can lead to
  /// lower latency, but low-memory devices may not be able to play back the
  /// stream without interruptions.
  final M2tsBufferModel? bufferModel;

  /// If you select ALIGN_TO_VIDEO, MediaConvert writes captions and data packets
  /// with Presentation Timestamp (PTS) values greater than or equal to the first
  /// video packet PTS (MediaConvert drops captions and data packets with lesser
  /// PTS values). Keep the default value to allow all PTS values.
  final M2tsDataPtsControl? dataPTSControl;

  /// Use these settings to insert a DVB Network Information Table (NIT) in the
  /// transport stream of this output.
  final DvbNitSettings? dvbNitSettings;

  /// Use these settings to insert a DVB Service Description Table (SDT) in the
  /// transport stream of this output.
  final DvbSdtSettings? dvbSdtSettings;

  /// Specify the packet identifiers (PIDs) for DVB subtitle data included in this
  /// output. Specify multiple PIDs as a JSON array. Default is the range 460-479.
  final List<int>? dvbSubPids;

  /// Use these settings to insert a DVB Time and Date Table (TDT) in the
  /// transport stream of this output.
  final DvbTdtSettings? dvbTdtSettings;

  /// Specify the packet identifier (PID) for DVB teletext data you include in
  /// this output. Default is 499.
  final int? dvbTeletextPid;

  /// When set to VIDEO_AND_FIXED_INTERVALS, audio EBP markers will be added to
  /// partitions 3 and 4. The interval between these additional markers will be
  /// fixed, and will be slightly shorter than the video EBP marker interval. When
  /// set to VIDEO_INTERVAL, these additional markers will not be inserted. Only
  /// applicable when EBP segmentation markers are is selected
  /// (segmentationMarkers is EBP or EBP_LEGACY).
  final M2tsEbpAudioInterval? ebpAudioInterval;

  /// Selects which PIDs to place EBP markers on. They can either be placed only
  /// on the video PID, or on both the video PID and all audio PIDs. Only
  /// applicable when EBP segmentation markers are is selected
  /// (segmentationMarkers is EBP or EBP_LEGACY).
  final M2tsEbpPlacement? ebpPlacement;

  /// Controls whether to include the ES Rate field in the PES header.
  final M2tsEsRateInPes? esRateInPes;

  /// Keep the default value unless you know that your audio EBP markers are
  /// incorrectly appearing before your video EBP markers. To correct this
  /// problem, set this value to Force.
  final M2tsForceTsVideoEbpOrder? forceTsVideoEbpOrder;

  /// The length, in seconds, of each fragment. Only used with EBP markers.
  final double? fragmentTime;

  /// To include key-length-value metadata in this output: Set KLV metadata
  /// insertion to Passthrough. MediaConvert reads KLV metadata present in your
  /// input and passes it through to the output transport stream. To exclude this
  /// KLV metadata: Set KLV metadata insertion to None or leave blank.
  final M2tsKlvMetadata? klvMetadata;

  /// Specify the maximum time, in milliseconds, between Program Clock References
  /// (PCRs) inserted into the transport stream.
  final int? maxPcrInterval;

  /// When set, enforces that Encoder Boundary Points do not come within the
  /// specified time interval of each other by looking ahead at input video. If
  /// another EBP is going to come in within the specified time interval, the
  /// current EBP is not emitted, and the segment is "stretched" to the next
  /// marker. The lookahead value does not add latency to the system. The Live
  /// Event must be configured elsewhere to create sufficient latency to make the
  /// lookahead accurate.
  final int? minEbpInterval;

  /// If INSERT, Nielsen inaudible tones for media tracking will be detected in
  /// the input audio and an equivalent ID3 tag will be inserted in the output.
  final M2tsNielsenId3? nielsenId3;

  /// Value in bits per second of extra null packets to insert into the transport
  /// stream. This can be used if a downstream encryption system requires periodic
  /// null packets.
  final double? nullPacketBitrate;

  /// The number of milliseconds between instances of this table in the output
  /// transport stream.
  final int? patInterval;

  /// When set to PCR_EVERY_PES_PACKET, a Program Clock Reference value is
  /// inserted for every Packetized Elementary Stream (PES) header. This is
  /// effective only when the PCR PID is the same as the video or audio elementary
  /// stream.
  final M2tsPcrControl? pcrControl;

  /// Specify the packet identifier (PID) for the program clock reference (PCR) in
  /// this output. If you do not specify a value, the service will use the value
  /// for Video PID.
  final int? pcrPid;

  /// Specify the number of milliseconds between instances of the program map
  /// table (PMT) in the output transport stream.
  final int? pmtInterval;

  /// Specify the packet identifier (PID) for the program map table (PMT) itself.
  /// Default is 480.
  final int? pmtPid;

  /// Specify whether MediaConvert automatically attempts to prevent decoder
  /// buffer underflows in your transport stream output. Use if you are seeing
  /// decoder buffer underflows in your output and are unable to increase your
  /// transport stream's bitrate. For most workflows: We recommend that you keep
  /// the default value, Disabled. To prevent decoder buffer underflows in your
  /// output, when possible: Choose Enabled. Note that if MediaConvert prevents a
  /// decoder buffer underflow in your output, output video quality is reduced and
  /// your job will take longer to complete.
  final M2tsPreventBufferUnderflow? preventBufferUnderflow;

  /// Specify the packet identifier (PID) of the private metadata stream. Default
  /// is 503.
  final int? privateMetadataPid;

  /// Use Program number to specify the program number used in the program map
  /// table (PMT) for this output. Default is 1. Program numbers and program map
  /// tables are parts of MPEG-2 transport stream containers, used for organizing
  /// data.
  final int? programNumber;

  /// Manually specify the initial PTS offset, in seconds, when you set PTS offset
  /// to Seconds. Enter an integer from 0 to 3600. Leave blank to keep the default
  /// value 2.
  final int? ptsOffset;

  /// Specify the initial presentation timestamp (PTS) offset for your transport
  /// stream output. To let MediaConvert automatically determine the initial PTS
  /// offset: Keep the default value, Auto. We recommend that you choose Auto for
  /// the widest player compatibility. The initial PTS will be at least two
  /// seconds and vary depending on your output's bitrate, HRD buffer size and HRD
  /// buffer initial fill percentage. To manually specify an initial PTS offset:
  /// Choose Seconds. Then specify the number of seconds with PTS offset.
  final TsPtsOffset? ptsOffsetMode;

  /// When set to CBR, inserts null packets into transport stream to fill
  /// specified bitrate. When set to VBR, the bitrate setting acts as the maximum
  /// bitrate, but the output will not be padded up to that bitrate.
  final M2tsRateMode? rateMode;

  /// Include this in your job settings to put SCTE-35 markers in your HLS and
  /// transport stream outputs at the insertion points that you specify in an ESAM
  /// XML document. Provide the document in the setting SCC XML.
  final M2tsScte35Esam? scte35Esam;

  /// Specify the packet identifier (PID) of the SCTE-35 stream in the transport
  /// stream.
  final int? scte35Pid;

  /// For SCTE-35 markers from your input-- Choose Passthrough if you want SCTE-35
  /// markers that appear in your input to also appear in this output. Choose None
  /// if you don't want SCTE-35 markers in this output. For SCTE-35 markers from
  /// an ESAM XML document-- Choose None. Also provide the ESAM XML as a string in
  /// the setting Signal processing notification XML. Also enable ESAM SCTE-35
  /// (include the property scte35Esam).
  final M2tsScte35Source? scte35Source;

  /// Inserts segmentation markers at each segmentation_time period. rai_segstart
  /// sets the Random Access Indicator bit in the adaptation field. rai_adapt sets
  /// the RAI bit and adds the current timecode in the private data bytes.
  /// psi_segstart inserts PAT and PMT tables at the start of segments. ebp adds
  /// Encoder Boundary Point information to the adaptation field as per OpenCable
  /// specification OC-SP-EBP-I01-130118. ebp_legacy adds Encoder Boundary Point
  /// information to the adaptation field using a legacy proprietary format.
  final M2tsSegmentationMarkers? segmentationMarkers;

  /// The segmentation style parameter controls how segmentation markers are
  /// inserted into the transport stream. With avails, it is possible that
  /// segments may be truncated, which can influence where future segmentation
  /// markers are inserted. When a segmentation style of "reset_cadence" is
  /// selected and a segment is truncated due to an avail, we will reset the
  /// segmentation cadence. This means the subsequent segment will have a duration
  /// of of $segmentation_time seconds. When a segmentation style of
  /// "maintain_cadence" is selected and a segment is truncated due to an avail,
  /// we will not reset the segmentation cadence. This means the subsequent
  /// segment will likely be truncated as well. However, all segments after that
  /// will have a duration of $segmentation_time seconds. Note that EBP lookahead
  /// is a slight exception to this rule.
  final M2tsSegmentationStyle? segmentationStyle;

  /// Specify the length, in seconds, of each segment. Required unless markers is
  /// set to _none_.
  final double? segmentationTime;

  /// Packet Identifier (PID) of the ID3 metadata stream in the transport stream.
  final int? timedMetadataPid;

  /// Specify the ID for the transport stream itself in the program map table for
  /// this output. Transport stream IDs and program map tables are parts of MPEG-2
  /// transport stream containers, used for organizing data.
  final int? transportStreamId;

  /// Specify the packet identifier (PID) of the elementary video stream in the
  /// transport stream.
  final int? videoPid;

  M2tsSettings({
    this.audioBufferModel,
    this.audioDuration,
    this.audioFramesPerPes,
    this.audioPids,
    this.bitrate,
    this.bufferModel,
    this.dataPTSControl,
    this.dvbNitSettings,
    this.dvbSdtSettings,
    this.dvbSubPids,
    this.dvbTdtSettings,
    this.dvbTeletextPid,
    this.ebpAudioInterval,
    this.ebpPlacement,
    this.esRateInPes,
    this.forceTsVideoEbpOrder,
    this.fragmentTime,
    this.klvMetadata,
    this.maxPcrInterval,
    this.minEbpInterval,
    this.nielsenId3,
    this.nullPacketBitrate,
    this.patInterval,
    this.pcrControl,
    this.pcrPid,
    this.pmtInterval,
    this.pmtPid,
    this.preventBufferUnderflow,
    this.privateMetadataPid,
    this.programNumber,
    this.ptsOffset,
    this.ptsOffsetMode,
    this.rateMode,
    this.scte35Esam,
    this.scte35Pid,
    this.scte35Source,
    this.segmentationMarkers,
    this.segmentationStyle,
    this.segmentationTime,
    this.timedMetadataPid,
    this.transportStreamId,
    this.videoPid,
  });

  factory M2tsSettings.fromJson(Map<String, dynamic> json) {
    return M2tsSettings(
      audioBufferModel: (json['audioBufferModel'] as String?)
          ?.let(M2tsAudioBufferModel.fromString),
      audioDuration:
          (json['audioDuration'] as String?)?.let(M2tsAudioDuration.fromString),
      audioFramesPerPes: json['audioFramesPerPes'] as int?,
      audioPids:
          (json['audioPids'] as List?)?.nonNulls.map((e) => e as int).toList(),
      bitrate: json['bitrate'] as int?,
      bufferModel:
          (json['bufferModel'] as String?)?.let(M2tsBufferModel.fromString),
      dataPTSControl: (json['dataPTSControl'] as String?)
          ?.let(M2tsDataPtsControl.fromString),
      dvbNitSettings: json['dvbNitSettings'] != null
          ? DvbNitSettings.fromJson(
              json['dvbNitSettings'] as Map<String, dynamic>)
          : null,
      dvbSdtSettings: json['dvbSdtSettings'] != null
          ? DvbSdtSettings.fromJson(
              json['dvbSdtSettings'] as Map<String, dynamic>)
          : null,
      dvbSubPids:
          (json['dvbSubPids'] as List?)?.nonNulls.map((e) => e as int).toList(),
      dvbTdtSettings: json['dvbTdtSettings'] != null
          ? DvbTdtSettings.fromJson(
              json['dvbTdtSettings'] as Map<String, dynamic>)
          : null,
      dvbTeletextPid: json['dvbTeletextPid'] as int?,
      ebpAudioInterval: (json['ebpAudioInterval'] as String?)
          ?.let(M2tsEbpAudioInterval.fromString),
      ebpPlacement:
          (json['ebpPlacement'] as String?)?.let(M2tsEbpPlacement.fromString),
      esRateInPes:
          (json['esRateInPes'] as String?)?.let(M2tsEsRateInPes.fromString),
      forceTsVideoEbpOrder: (json['forceTsVideoEbpOrder'] as String?)
          ?.let(M2tsForceTsVideoEbpOrder.fromString),
      fragmentTime: json['fragmentTime'] as double?,
      klvMetadata:
          (json['klvMetadata'] as String?)?.let(M2tsKlvMetadata.fromString),
      maxPcrInterval: json['maxPcrInterval'] as int?,
      minEbpInterval: json['minEbpInterval'] as int?,
      nielsenId3:
          (json['nielsenId3'] as String?)?.let(M2tsNielsenId3.fromString),
      nullPacketBitrate: json['nullPacketBitrate'] as double?,
      patInterval: json['patInterval'] as int?,
      pcrControl:
          (json['pcrControl'] as String?)?.let(M2tsPcrControl.fromString),
      pcrPid: json['pcrPid'] as int?,
      pmtInterval: json['pmtInterval'] as int?,
      pmtPid: json['pmtPid'] as int?,
      preventBufferUnderflow: (json['preventBufferUnderflow'] as String?)
          ?.let(M2tsPreventBufferUnderflow.fromString),
      privateMetadataPid: json['privateMetadataPid'] as int?,
      programNumber: json['programNumber'] as int?,
      ptsOffset: json['ptsOffset'] as int?,
      ptsOffsetMode:
          (json['ptsOffsetMode'] as String?)?.let(TsPtsOffset.fromString),
      rateMode: (json['rateMode'] as String?)?.let(M2tsRateMode.fromString),
      scte35Esam: json['scte35Esam'] != null
          ? M2tsScte35Esam.fromJson(json['scte35Esam'] as Map<String, dynamic>)
          : null,
      scte35Pid: json['scte35Pid'] as int?,
      scte35Source:
          (json['scte35Source'] as String?)?.let(M2tsScte35Source.fromString),
      segmentationMarkers: (json['segmentationMarkers'] as String?)
          ?.let(M2tsSegmentationMarkers.fromString),
      segmentationStyle: (json['segmentationStyle'] as String?)
          ?.let(M2tsSegmentationStyle.fromString),
      segmentationTime: json['segmentationTime'] as double?,
      timedMetadataPid: json['timedMetadataPid'] as int?,
      transportStreamId: json['transportStreamId'] as int?,
      videoPid: json['videoPid'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final audioBufferModel = this.audioBufferModel;
    final audioDuration = this.audioDuration;
    final audioFramesPerPes = this.audioFramesPerPes;
    final audioPids = this.audioPids;
    final bitrate = this.bitrate;
    final bufferModel = this.bufferModel;
    final dataPTSControl = this.dataPTSControl;
    final dvbNitSettings = this.dvbNitSettings;
    final dvbSdtSettings = this.dvbSdtSettings;
    final dvbSubPids = this.dvbSubPids;
    final dvbTdtSettings = this.dvbTdtSettings;
    final dvbTeletextPid = this.dvbTeletextPid;
    final ebpAudioInterval = this.ebpAudioInterval;
    final ebpPlacement = this.ebpPlacement;
    final esRateInPes = this.esRateInPes;
    final forceTsVideoEbpOrder = this.forceTsVideoEbpOrder;
    final fragmentTime = this.fragmentTime;
    final klvMetadata = this.klvMetadata;
    final maxPcrInterval = this.maxPcrInterval;
    final minEbpInterval = this.minEbpInterval;
    final nielsenId3 = this.nielsenId3;
    final nullPacketBitrate = this.nullPacketBitrate;
    final patInterval = this.patInterval;
    final pcrControl = this.pcrControl;
    final pcrPid = this.pcrPid;
    final pmtInterval = this.pmtInterval;
    final pmtPid = this.pmtPid;
    final preventBufferUnderflow = this.preventBufferUnderflow;
    final privateMetadataPid = this.privateMetadataPid;
    final programNumber = this.programNumber;
    final ptsOffset = this.ptsOffset;
    final ptsOffsetMode = this.ptsOffsetMode;
    final rateMode = this.rateMode;
    final scte35Esam = this.scte35Esam;
    final scte35Pid = this.scte35Pid;
    final scte35Source = this.scte35Source;
    final segmentationMarkers = this.segmentationMarkers;
    final segmentationStyle = this.segmentationStyle;
    final segmentationTime = this.segmentationTime;
    final timedMetadataPid = this.timedMetadataPid;
    final transportStreamId = this.transportStreamId;
    final videoPid = this.videoPid;
    return {
      if (audioBufferModel != null) 'audioBufferModel': audioBufferModel.value,
      if (audioDuration != null) 'audioDuration': audioDuration.value,
      if (audioFramesPerPes != null) 'audioFramesPerPes': audioFramesPerPes,
      if (audioPids != null) 'audioPids': audioPids,
      if (bitrate != null) 'bitrate': bitrate,
      if (bufferModel != null) 'bufferModel': bufferModel.value,
      if (dataPTSControl != null) 'dataPTSControl': dataPTSControl.value,
      if (dvbNitSettings != null) 'dvbNitSettings': dvbNitSettings,
      if (dvbSdtSettings != null) 'dvbSdtSettings': dvbSdtSettings,
      if (dvbSubPids != null) 'dvbSubPids': dvbSubPids,
      if (dvbTdtSettings != null) 'dvbTdtSettings': dvbTdtSettings,
      if (dvbTeletextPid != null) 'dvbTeletextPid': dvbTeletextPid,
      if (ebpAudioInterval != null) 'ebpAudioInterval': ebpAudioInterval.value,
      if (ebpPlacement != null) 'ebpPlacement': ebpPlacement.value,
      if (esRateInPes != null) 'esRateInPes': esRateInPes.value,
      if (forceTsVideoEbpOrder != null)
        'forceTsVideoEbpOrder': forceTsVideoEbpOrder.value,
      if (fragmentTime != null) 'fragmentTime': fragmentTime,
      if (klvMetadata != null) 'klvMetadata': klvMetadata.value,
      if (maxPcrInterval != null) 'maxPcrInterval': maxPcrInterval,
      if (minEbpInterval != null) 'minEbpInterval': minEbpInterval,
      if (nielsenId3 != null) 'nielsenId3': nielsenId3.value,
      if (nullPacketBitrate != null) 'nullPacketBitrate': nullPacketBitrate,
      if (patInterval != null) 'patInterval': patInterval,
      if (pcrControl != null) 'pcrControl': pcrControl.value,
      if (pcrPid != null) 'pcrPid': pcrPid,
      if (pmtInterval != null) 'pmtInterval': pmtInterval,
      if (pmtPid != null) 'pmtPid': pmtPid,
      if (preventBufferUnderflow != null)
        'preventBufferUnderflow': preventBufferUnderflow.value,
      if (privateMetadataPid != null) 'privateMetadataPid': privateMetadataPid,
      if (programNumber != null) 'programNumber': programNumber,
      if (ptsOffset != null) 'ptsOffset': ptsOffset,
      if (ptsOffsetMode != null) 'ptsOffsetMode': ptsOffsetMode.value,
      if (rateMode != null) 'rateMode': rateMode.value,
      if (scte35Esam != null) 'scte35Esam': scte35Esam,
      if (scte35Pid != null) 'scte35Pid': scte35Pid,
      if (scte35Source != null) 'scte35Source': scte35Source.value,
      if (segmentationMarkers != null)
        'segmentationMarkers': segmentationMarkers.value,
      if (segmentationStyle != null)
        'segmentationStyle': segmentationStyle.value,
      if (segmentationTime != null) 'segmentationTime': segmentationTime,
      if (timedMetadataPid != null) 'timedMetadataPid': timedMetadataPid,
      if (transportStreamId != null) 'transportStreamId': transportStreamId,
      if (videoPid != null) 'videoPid': videoPid,
    };
  }
}

/// Specify this setting only when your output will be consumed by a downstream
/// repackaging workflow that is sensitive to very small duration differences
/// between video and audio. For this situation, choose Match video duration. In
/// all other cases, keep the default value, Default codec duration. When you
/// choose Match video duration, MediaConvert pads the output audio streams with
/// silence or trims them to ensure that the total duration of each audio stream
/// is at least as long as the total duration of the video stream. After padding
/// or trimming, the audio stream duration is no more than one frame longer than
/// the video stream. MediaConvert applies audio padding or trimming only to the
/// end of the last segment of the output. For unsegmented outputs, MediaConvert
/// adds padding only to the end of the file. When you keep the default value,
/// any minor discrepancies between audio and video duration will depend on your
/// output audio codec.
enum M3u8AudioDuration {
  defaultCodecDuration('DEFAULT_CODEC_DURATION'),
  matchVideoDuration('MATCH_VIDEO_DURATION'),
  ;

  final String value;

  const M3u8AudioDuration(this.value);

  static M3u8AudioDuration fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum M3u8AudioDuration'));
}

/// If you select ALIGN_TO_VIDEO, MediaConvert writes captions and data packets
/// with Presentation Timestamp (PTS) values greater than or equal to the first
/// video packet PTS (MediaConvert drops captions and data packets with lesser
/// PTS values). Keep the default value AUTO to allow all PTS values.
enum M3u8DataPtsControl {
  auto('AUTO'),
  alignToVideo('ALIGN_TO_VIDEO'),
  ;

  final String value;

  const M3u8DataPtsControl(this.value);

  static M3u8DataPtsControl fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum M3u8DataPtsControl'));
}

/// If INSERT, Nielsen inaudible tones for media tracking will be detected in
/// the input audio and an equivalent ID3 tag will be inserted in the output.
enum M3u8NielsenId3 {
  insert('INSERT'),
  none('NONE'),
  ;

  final String value;

  const M3u8NielsenId3(this.value);

  static M3u8NielsenId3 fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum M3u8NielsenId3'));
}

/// When set to PCR_EVERY_PES_PACKET a Program Clock Reference value is inserted
/// for every Packetized Elementary Stream (PES) header. This parameter is
/// effective only when the PCR PID is the same as the video or audio elementary
/// stream.
enum M3u8PcrControl {
  pcrEveryPesPacket('PCR_EVERY_PES_PACKET'),
  configuredPcrPeriod('CONFIGURED_PCR_PERIOD'),
  ;

  final String value;

  const M3u8PcrControl(this.value);

  static M3u8PcrControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum M3u8PcrControl'));
}

/// For SCTE-35 markers from your input-- Choose Passthrough if you want SCTE-35
/// markers that appear in your input to also appear in this output. Choose None
/// if you don't want SCTE-35 markers in this output. For SCTE-35 markers from
/// an ESAM XML document-- Choose None if you don't want manifest conditioning.
/// Choose Passthrough and choose Ad markers if you do want manifest
/// conditioning. In both cases, also provide the ESAM XML as a string in the
/// setting Signal processing notification XML.
enum M3u8Scte35Source {
  passthrough('PASSTHROUGH'),
  none('NONE'),
  ;

  final String value;

  const M3u8Scte35Source(this.value);

  static M3u8Scte35Source fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum M3u8Scte35Source'));
}

/// These settings relate to the MPEG-2 transport stream (MPEG2-TS) container
/// for the MPEG2-TS segments in your HLS outputs.
class M3u8Settings {
  /// Specify this setting only when your output will be consumed by a downstream
  /// repackaging workflow that is sensitive to very small duration differences
  /// between video and audio. For this situation, choose Match video duration. In
  /// all other cases, keep the default value, Default codec duration. When you
  /// choose Match video duration, MediaConvert pads the output audio streams with
  /// silence or trims them to ensure that the total duration of each audio stream
  /// is at least as long as the total duration of the video stream. After padding
  /// or trimming, the audio stream duration is no more than one frame longer than
  /// the video stream. MediaConvert applies audio padding or trimming only to the
  /// end of the last segment of the output. For unsegmented outputs, MediaConvert
  /// adds padding only to the end of the file. When you keep the default value,
  /// any minor discrepancies between audio and video duration will depend on your
  /// output audio codec.
  final M3u8AudioDuration? audioDuration;

  /// The number of audio frames to insert for each PES packet.
  final int? audioFramesPerPes;

  /// Packet Identifier (PID) of the elementary audio stream(s) in the transport
  /// stream. Multiple values are accepted, and can be entered in ranges and/or by
  /// comma separation.
  final List<int>? audioPids;

  /// If you select ALIGN_TO_VIDEO, MediaConvert writes captions and data packets
  /// with Presentation Timestamp (PTS) values greater than or equal to the first
  /// video packet PTS (MediaConvert drops captions and data packets with lesser
  /// PTS values). Keep the default value AUTO to allow all PTS values.
  final M3u8DataPtsControl? dataPTSControl;

  /// Specify the maximum time, in milliseconds, between Program Clock References
  /// (PCRs) inserted into the transport stream.
  final int? maxPcrInterval;

  /// If INSERT, Nielsen inaudible tones for media tracking will be detected in
  /// the input audio and an equivalent ID3 tag will be inserted in the output.
  final M3u8NielsenId3? nielsenId3;

  /// The number of milliseconds between instances of this table in the output
  /// transport stream.
  final int? patInterval;

  /// When set to PCR_EVERY_PES_PACKET a Program Clock Reference value is inserted
  /// for every Packetized Elementary Stream (PES) header. This parameter is
  /// effective only when the PCR PID is the same as the video or audio elementary
  /// stream.
  final M3u8PcrControl? pcrControl;

  /// Packet Identifier (PID) of the Program Clock Reference (PCR) in the
  /// transport stream. When no value is given, the encoder will assign the same
  /// value as the Video PID.
  final int? pcrPid;

  /// The number of milliseconds between instances of this table in the output
  /// transport stream.
  final int? pmtInterval;

  /// Packet Identifier (PID) for the Program Map Table (PMT) in the transport
  /// stream.
  final int? pmtPid;

  /// Packet Identifier (PID) of the private metadata stream in the transport
  /// stream.
  final int? privateMetadataPid;

  /// The value of the program number field in the Program Map Table.
  final int? programNumber;

  /// Manually specify the initial PTS offset, in seconds, when you set PTS offset
  /// to Seconds. Enter an integer from 0 to 3600. Leave blank to keep the default
  /// value 2.
  final int? ptsOffset;

  /// Specify the initial presentation timestamp (PTS) offset for your transport
  /// stream output. To let MediaConvert automatically determine the initial PTS
  /// offset: Keep the default value, Auto. We recommend that you choose Auto for
  /// the widest player compatibility. The initial PTS will be at least two
  /// seconds and vary depending on your output's bitrate, HRD buffer size and HRD
  /// buffer initial fill percentage. To manually specify an initial PTS offset:
  /// Choose Seconds. Then specify the number of seconds with PTS offset.
  final TsPtsOffset? ptsOffsetMode;

  /// Packet Identifier (PID) of the SCTE-35 stream in the transport stream.
  final int? scte35Pid;

  /// For SCTE-35 markers from your input-- Choose Passthrough if you want SCTE-35
  /// markers that appear in your input to also appear in this output. Choose None
  /// if you don't want SCTE-35 markers in this output. For SCTE-35 markers from
  /// an ESAM XML document-- Choose None if you don't want manifest conditioning.
  /// Choose Passthrough and choose Ad markers if you do want manifest
  /// conditioning. In both cases, also provide the ESAM XML as a string in the
  /// setting Signal processing notification XML.
  final M3u8Scte35Source? scte35Source;

  /// Set ID3 metadata to Passthrough to include ID3 metadata in this output. This
  /// includes ID3 metadata from the following features: ID3 timestamp period, and
  /// Custom ID3 metadata inserter. To exclude this ID3 metadata in this output:
  /// set ID3 metadata to None or leave blank.
  final TimedMetadata? timedMetadata;

  /// Packet Identifier (PID) of the ID3 metadata stream in the transport stream.
  final int? timedMetadataPid;

  /// The value of the transport stream ID field in the Program Map Table.
  final int? transportStreamId;

  /// Packet Identifier (PID) of the elementary video stream in the transport
  /// stream.
  final int? videoPid;

  M3u8Settings({
    this.audioDuration,
    this.audioFramesPerPes,
    this.audioPids,
    this.dataPTSControl,
    this.maxPcrInterval,
    this.nielsenId3,
    this.patInterval,
    this.pcrControl,
    this.pcrPid,
    this.pmtInterval,
    this.pmtPid,
    this.privateMetadataPid,
    this.programNumber,
    this.ptsOffset,
    this.ptsOffsetMode,
    this.scte35Pid,
    this.scte35Source,
    this.timedMetadata,
    this.timedMetadataPid,
    this.transportStreamId,
    this.videoPid,
  });

  factory M3u8Settings.fromJson(Map<String, dynamic> json) {
    return M3u8Settings(
      audioDuration:
          (json['audioDuration'] as String?)?.let(M3u8AudioDuration.fromString),
      audioFramesPerPes: json['audioFramesPerPes'] as int?,
      audioPids:
          (json['audioPids'] as List?)?.nonNulls.map((e) => e as int).toList(),
      dataPTSControl: (json['dataPTSControl'] as String?)
          ?.let(M3u8DataPtsControl.fromString),
      maxPcrInterval: json['maxPcrInterval'] as int?,
      nielsenId3:
          (json['nielsenId3'] as String?)?.let(M3u8NielsenId3.fromString),
      patInterval: json['patInterval'] as int?,
      pcrControl:
          (json['pcrControl'] as String?)?.let(M3u8PcrControl.fromString),
      pcrPid: json['pcrPid'] as int?,
      pmtInterval: json['pmtInterval'] as int?,
      pmtPid: json['pmtPid'] as int?,
      privateMetadataPid: json['privateMetadataPid'] as int?,
      programNumber: json['programNumber'] as int?,
      ptsOffset: json['ptsOffset'] as int?,
      ptsOffsetMode:
          (json['ptsOffsetMode'] as String?)?.let(TsPtsOffset.fromString),
      scte35Pid: json['scte35Pid'] as int?,
      scte35Source:
          (json['scte35Source'] as String?)?.let(M3u8Scte35Source.fromString),
      timedMetadata:
          (json['timedMetadata'] as String?)?.let(TimedMetadata.fromString),
      timedMetadataPid: json['timedMetadataPid'] as int?,
      transportStreamId: json['transportStreamId'] as int?,
      videoPid: json['videoPid'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final audioDuration = this.audioDuration;
    final audioFramesPerPes = this.audioFramesPerPes;
    final audioPids = this.audioPids;
    final dataPTSControl = this.dataPTSControl;
    final maxPcrInterval = this.maxPcrInterval;
    final nielsenId3 = this.nielsenId3;
    final patInterval = this.patInterval;
    final pcrControl = this.pcrControl;
    final pcrPid = this.pcrPid;
    final pmtInterval = this.pmtInterval;
    final pmtPid = this.pmtPid;
    final privateMetadataPid = this.privateMetadataPid;
    final programNumber = this.programNumber;
    final ptsOffset = this.ptsOffset;
    final ptsOffsetMode = this.ptsOffsetMode;
    final scte35Pid = this.scte35Pid;
    final scte35Source = this.scte35Source;
    final timedMetadata = this.timedMetadata;
    final timedMetadataPid = this.timedMetadataPid;
    final transportStreamId = this.transportStreamId;
    final videoPid = this.videoPid;
    return {
      if (audioDuration != null) 'audioDuration': audioDuration.value,
      if (audioFramesPerPes != null) 'audioFramesPerPes': audioFramesPerPes,
      if (audioPids != null) 'audioPids': audioPids,
      if (dataPTSControl != null) 'dataPTSControl': dataPTSControl.value,
      if (maxPcrInterval != null) 'maxPcrInterval': maxPcrInterval,
      if (nielsenId3 != null) 'nielsenId3': nielsenId3.value,
      if (patInterval != null) 'patInterval': patInterval,
      if (pcrControl != null) 'pcrControl': pcrControl.value,
      if (pcrPid != null) 'pcrPid': pcrPid,
      if (pmtInterval != null) 'pmtInterval': pmtInterval,
      if (pmtPid != null) 'pmtPid': pmtPid,
      if (privateMetadataPid != null) 'privateMetadataPid': privateMetadataPid,
      if (programNumber != null) 'programNumber': programNumber,
      if (ptsOffset != null) 'ptsOffset': ptsOffset,
      if (ptsOffsetMode != null) 'ptsOffsetMode': ptsOffsetMode.value,
      if (scte35Pid != null) 'scte35Pid': scte35Pid,
      if (scte35Source != null) 'scte35Source': scte35Source.value,
      if (timedMetadata != null) 'timedMetadata': timedMetadata.value,
      if (timedMetadataPid != null) 'timedMetadataPid': timedMetadataPid,
      if (transportStreamId != null) 'transportStreamId': transportStreamId,
      if (videoPid != null) 'videoPid': videoPid,
    };
  }
}

/// Use Min bottom rendition size to specify a minimum size for the lowest
/// resolution in your ABR stack. * The lowest resolution in your ABR stack will
/// be equal to or greater than the value that you enter. For example: If you
/// specify 640x360 the lowest resolution in your ABR stack will be equal to or
/// greater than to 640x360. * If you specify a Min top rendition size rule, the
/// value that you specify for Min bottom rendition size must be less than, or
/// equal to, Min top rendition size.
class MinBottomRenditionSize {
  /// Use Height to define the video resolution height, in pixels, for this rule.
  final int? height;

  /// Use Width to define the video resolution width, in pixels, for this rule.
  final int? width;

  MinBottomRenditionSize({
    this.height,
    this.width,
  });

  factory MinBottomRenditionSize.fromJson(Map<String, dynamic> json) {
    return MinBottomRenditionSize(
      height: json['height'] as int?,
      width: json['width'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final height = this.height;
    final width = this.width;
    return {
      if (height != null) 'height': height,
      if (width != null) 'width': width,
    };
  }
}

/// Use Min top rendition size to specify a minimum size for the highest
/// resolution in your ABR stack. * The highest resolution in your ABR stack
/// will be equal to or greater than the value that you enter. For example: If
/// you specify 1280x720 the highest resolution in your ABR stack will be equal
/// to or greater than 1280x720. * If you specify a value for Max resolution,
/// the value that you specify for Min top rendition size must be less than, or
/// equal to, Max resolution.
class MinTopRenditionSize {
  /// Use Height to define the video resolution height, in pixels, for this rule.
  final int? height;

  /// Use Width to define the video resolution width, in pixels, for this rule.
  final int? width;

  MinTopRenditionSize({
    this.height,
    this.width,
  });

  factory MinTopRenditionSize.fromJson(Map<String, dynamic> json) {
    return MinTopRenditionSize(
      height: json['height'] as int?,
      width: json['width'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final height = this.height;
    final width = this.width;
    return {
      if (height != null) 'height': height,
      if (width != null) 'width': width,
    };
  }
}

/// Overlay motion graphics on top of your video. The motion graphics that you
/// specify here appear on all outputs in all output groups. For more
/// information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/motion-graphic-overlay.html.
class MotionImageInserter {
  /// If your motion graphic asset is a .mov file, keep this setting unspecified.
  /// If your motion graphic asset is a series of .png files, specify the frame
  /// rate of the overlay in frames per second, as a fraction. For example,
  /// specify 24 fps as 24/1. Make sure that the number of images in your series
  /// matches the frame rate and your intended overlay duration. For example, if
  /// you want a 30-second overlay at 30 fps, you should have 900 .png images.
  /// This overlay frame rate doesn't need to match the frame rate of the
  /// underlying video.
  final MotionImageInsertionFramerate? framerate;

  /// Specify the .mov file or series of .png files that you want to overlay on
  /// your video. For .png files, provide the file name of the first file in the
  /// series. Make sure that the names of the .png files end with sequential
  /// numbers that specify the order that they are played in. For example,
  /// overlay_000.png, overlay_001.png, overlay_002.png, and so on. The sequence
  /// must start at zero, and each image file name must have the same number of
  /// digits. Pad your initial file names with enough zeros to complete the
  /// sequence. For example, if the first image is overlay_0.png, there can be
  /// only 10 images in the sequence, with the last image being overlay_9.png. But
  /// if the first image is overlay_00.png, there can be 100 images in the
  /// sequence.
  final String? input;

  /// Choose the type of motion graphic asset that you are providing for your
  /// overlay. You can choose either a .mov file or a series of .png files.
  final MotionImageInsertionMode? insertionMode;

  /// Use Offset to specify the placement of your motion graphic overlay on the
  /// video frame. Specify in pixels, from the upper-left corner of the frame. If
  /// you don't specify an offset, the service scales your overlay to the full
  /// size of the frame. Otherwise, the service inserts the overlay at its native
  /// resolution and scales the size up or down with any video scaling.
  final MotionImageInsertionOffset? offset;

  /// Specify whether your motion graphic overlay repeats on a loop or plays only
  /// once.
  final MotionImagePlayback? playback;

  /// Specify when the motion overlay begins. Use timecode format (HH:MM:SS:FF or
  /// HH:MM:SS;FF). Make sure that the timecode you provide here takes into
  /// account how you have set up your timecode configuration under both job
  /// settings and input settings. The simplest way to do that is to set both to
  /// start at 0. If you need to set up your job to follow timecodes embedded in
  /// your source that don't start at zero, make sure that you specify a start
  /// time that is after the first embedded timecode. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/setting-up-timecode.html
  final String? startTime;

  MotionImageInserter({
    this.framerate,
    this.input,
    this.insertionMode,
    this.offset,
    this.playback,
    this.startTime,
  });

  factory MotionImageInserter.fromJson(Map<String, dynamic> json) {
    return MotionImageInserter(
      framerate: json['framerate'] != null
          ? MotionImageInsertionFramerate.fromJson(
              json['framerate'] as Map<String, dynamic>)
          : null,
      input: json['input'] as String?,
      insertionMode: (json['insertionMode'] as String?)
          ?.let(MotionImageInsertionMode.fromString),
      offset: json['offset'] != null
          ? MotionImageInsertionOffset.fromJson(
              json['offset'] as Map<String, dynamic>)
          : null,
      playback:
          (json['playback'] as String?)?.let(MotionImagePlayback.fromString),
      startTime: json['startTime'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final framerate = this.framerate;
    final input = this.input;
    final insertionMode = this.insertionMode;
    final offset = this.offset;
    final playback = this.playback;
    final startTime = this.startTime;
    return {
      if (framerate != null) 'framerate': framerate,
      if (input != null) 'input': input,
      if (insertionMode != null) 'insertionMode': insertionMode.value,
      if (offset != null) 'offset': offset,
      if (playback != null) 'playback': playback.value,
      if (startTime != null) 'startTime': startTime,
    };
  }
}

/// For motion overlays that don't have a built-in frame rate, specify the frame
/// rate of the overlay in frames per second, as a fraction. For example,
/// specify 24 fps as 24/1. The overlay frame rate doesn't need to match the
/// frame rate of the underlying video.
class MotionImageInsertionFramerate {
  /// The bottom of the fraction that expresses your overlay frame rate. For
  /// example, if your frame rate is 24 fps, set this value to 1.
  final int? framerateDenominator;

  /// The top of the fraction that expresses your overlay frame rate. For example,
  /// if your frame rate is 24 fps, set this value to 24.
  final int? framerateNumerator;

  MotionImageInsertionFramerate({
    this.framerateDenominator,
    this.framerateNumerator,
  });

  factory MotionImageInsertionFramerate.fromJson(Map<String, dynamic> json) {
    return MotionImageInsertionFramerate(
      framerateDenominator: json['framerateDenominator'] as int?,
      framerateNumerator: json['framerateNumerator'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final framerateDenominator = this.framerateDenominator;
    final framerateNumerator = this.framerateNumerator;
    return {
      if (framerateDenominator != null)
        'framerateDenominator': framerateDenominator,
      if (framerateNumerator != null) 'framerateNumerator': framerateNumerator,
    };
  }
}

/// Choose the type of motion graphic asset that you are providing for your
/// overlay. You can choose either a .mov file or a series of .png files.
enum MotionImageInsertionMode {
  mov('MOV'),
  png('PNG'),
  ;

  final String value;

  const MotionImageInsertionMode(this.value);

  static MotionImageInsertionMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum MotionImageInsertionMode'));
}

/// Specify the offset between the upper-left corner of the video frame and the
/// top left corner of the overlay.
class MotionImageInsertionOffset {
  /// Set the distance, in pixels, between the overlay and the left edge of the
  /// video frame.
  final int? imageX;

  /// Set the distance, in pixels, between the overlay and the top edge of the
  /// video frame.
  final int? imageY;

  MotionImageInsertionOffset({
    this.imageX,
    this.imageY,
  });

  factory MotionImageInsertionOffset.fromJson(Map<String, dynamic> json) {
    return MotionImageInsertionOffset(
      imageX: json['imageX'] as int?,
      imageY: json['imageY'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final imageX = this.imageX;
    final imageY = this.imageY;
    return {
      if (imageX != null) 'imageX': imageX,
      if (imageY != null) 'imageY': imageY,
    };
  }
}

/// Specify whether your motion graphic overlay repeats on a loop or plays only
/// once.
enum MotionImagePlayback {
  once('ONCE'),
  repeat('REPEAT'),
  ;

  final String value;

  const MotionImagePlayback(this.value);

  static MotionImagePlayback fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum MotionImagePlayback'));
}

/// When enabled, include 'clap' atom if appropriate for the video output
/// settings.
enum MovClapAtom {
  include('INCLUDE'),
  exclude('EXCLUDE'),
  ;

  final String value;

  const MovClapAtom(this.value);

  static MovClapAtom fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum MovClapAtom'));
}

/// When enabled, file composition times will start at zero, composition times
/// in the 'ctts' (composition time to sample) box for B-frames will be
/// negative, and a 'cslg' (composition shift least greatest) box will be
/// included per 14496-1 amendment 1. This improves compatibility with Apple
/// players and tools.
enum MovCslgAtom {
  include('INCLUDE'),
  exclude('EXCLUDE'),
  ;

  final String value;

  const MovCslgAtom(this.value);

  static MovCslgAtom fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum MovCslgAtom'));
}

/// When set to XDCAM, writes MPEG2 video streams into the QuickTime file using
/// XDCAM fourcc codes. This increases compatibility with Apple editors and
/// players, but may decrease compatibility with other players. Only applicable
/// when the video codec is MPEG2.
enum MovMpeg2FourCCControl {
  xdcam('XDCAM'),
  mpeg('MPEG'),
  ;

  final String value;

  const MovMpeg2FourCCControl(this.value);

  static MovMpeg2FourCCControl fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum MovMpeg2FourCCControl'));
}

/// Unless you need Omneon compatibility: Keep the default value, None. To make
/// this output compatible with Omneon: Choose Omneon. When you do, MediaConvert
/// increases the length of the 'elst' edit list atom. Note that this might
/// cause file rejections when a recipient of the output file doesn't expect
/// this extra padding.
enum MovPaddingControl {
  omneon('OMNEON'),
  none('NONE'),
  ;

  final String value;

  const MovPaddingControl(this.value);

  static MovPaddingControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum MovPaddingControl'));
}

/// Always keep the default value (SELF_CONTAINED) for this setting.
enum MovReference {
  selfContained('SELF_CONTAINED'),
  external('EXTERNAL'),
  ;

  final String value;

  const MovReference(this.value);

  static MovReference fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum MovReference'));
}

/// These settings relate to your QuickTime MOV output container.
class MovSettings {
  /// When enabled, include 'clap' atom if appropriate for the video output
  /// settings.
  final MovClapAtom? clapAtom;

  /// When enabled, file composition times will start at zero, composition times
  /// in the 'ctts' (composition time to sample) box for B-frames will be
  /// negative, and a 'cslg' (composition shift least greatest) box will be
  /// included per 14496-1 amendment 1. This improves compatibility with Apple
  /// players and tools.
  final MovCslgAtom? cslgAtom;

  /// When set to XDCAM, writes MPEG2 video streams into the QuickTime file using
  /// XDCAM fourcc codes. This increases compatibility with Apple editors and
  /// players, but may decrease compatibility with other players. Only applicable
  /// when the video codec is MPEG2.
  final MovMpeg2FourCCControl? mpeg2FourCCControl;

  /// Unless you need Omneon compatibility: Keep the default value, None. To make
  /// this output compatible with Omneon: Choose Omneon. When you do, MediaConvert
  /// increases the length of the 'elst' edit list atom. Note that this might
  /// cause file rejections when a recipient of the output file doesn't expect
  /// this extra padding.
  final MovPaddingControl? paddingControl;

  /// Always keep the default value (SELF_CONTAINED) for this setting.
  final MovReference? reference;

  MovSettings({
    this.clapAtom,
    this.cslgAtom,
    this.mpeg2FourCCControl,
    this.paddingControl,
    this.reference,
  });

  factory MovSettings.fromJson(Map<String, dynamic> json) {
    return MovSettings(
      clapAtom: (json['clapAtom'] as String?)?.let(MovClapAtom.fromString),
      cslgAtom: (json['cslgAtom'] as String?)?.let(MovCslgAtom.fromString),
      mpeg2FourCCControl: (json['mpeg2FourCCControl'] as String?)
          ?.let(MovMpeg2FourCCControl.fromString),
      paddingControl: (json['paddingControl'] as String?)
          ?.let(MovPaddingControl.fromString),
      reference: (json['reference'] as String?)?.let(MovReference.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final clapAtom = this.clapAtom;
    final cslgAtom = this.cslgAtom;
    final mpeg2FourCCControl = this.mpeg2FourCCControl;
    final paddingControl = this.paddingControl;
    final reference = this.reference;
    return {
      if (clapAtom != null) 'clapAtom': clapAtom.value,
      if (cslgAtom != null) 'cslgAtom': cslgAtom.value,
      if (mpeg2FourCCControl != null)
        'mpeg2FourCCControl': mpeg2FourCCControl.value,
      if (paddingControl != null) 'paddingControl': paddingControl.value,
      if (reference != null) 'reference': reference.value,
    };
  }
}

/// Required when you set Codec to the value MP2.
class Mp2Settings {
  /// Specify the average bitrate in bits per second.
  final int? bitrate;

  /// Set Channels to specify the number of channels in this output audio track.
  /// Choosing Mono in will give you 1 output channel; choosing Stereo will give
  /// you 2. In the API, valid values are 1 and 2.
  final int? channels;

  /// Sample rate in Hz.
  final int? sampleRate;

  Mp2Settings({
    this.bitrate,
    this.channels,
    this.sampleRate,
  });

  factory Mp2Settings.fromJson(Map<String, dynamic> json) {
    return Mp2Settings(
      bitrate: json['bitrate'] as int?,
      channels: json['channels'] as int?,
      sampleRate: json['sampleRate'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final bitrate = this.bitrate;
    final channels = this.channels;
    final sampleRate = this.sampleRate;
    return {
      if (bitrate != null) 'bitrate': bitrate,
      if (channels != null) 'channels': channels,
      if (sampleRate != null) 'sampleRate': sampleRate,
    };
  }
}

/// Specify whether the service encodes this MP3 audio output with a constant
/// bitrate (CBR) or a variable bitrate (VBR).
enum Mp3RateControlMode {
  cbr('CBR'),
  vbr('VBR'),
  ;

  final String value;

  const Mp3RateControlMode(this.value);

  static Mp3RateControlMode fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Mp3RateControlMode'));
}

/// Required when you set Codec, under AudioDescriptions>CodecSettings, to the
/// value MP3.
class Mp3Settings {
  /// Specify the average bitrate in bits per second.
  final int? bitrate;

  /// Specify the number of channels in this output audio track. Choosing Mono
  /// gives you 1 output channel; choosing Stereo gives you 2. In the API, valid
  /// values are 1 and 2.
  final int? channels;

  /// Specify whether the service encodes this MP3 audio output with a constant
  /// bitrate (CBR) or a variable bitrate (VBR).
  final Mp3RateControlMode? rateControlMode;

  /// Sample rate in Hz.
  final int? sampleRate;

  /// Required when you set Bitrate control mode to VBR. Specify the audio quality
  /// of this MP3 output from 0 (highest quality) to 9 (lowest quality).
  final int? vbrQuality;

  Mp3Settings({
    this.bitrate,
    this.channels,
    this.rateControlMode,
    this.sampleRate,
    this.vbrQuality,
  });

  factory Mp3Settings.fromJson(Map<String, dynamic> json) {
    return Mp3Settings(
      bitrate: json['bitrate'] as int?,
      channels: json['channels'] as int?,
      rateControlMode: (json['rateControlMode'] as String?)
          ?.let(Mp3RateControlMode.fromString),
      sampleRate: json['sampleRate'] as int?,
      vbrQuality: json['vbrQuality'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final bitrate = this.bitrate;
    final channels = this.channels;
    final rateControlMode = this.rateControlMode;
    final sampleRate = this.sampleRate;
    final vbrQuality = this.vbrQuality;
    return {
      if (bitrate != null) 'bitrate': bitrate,
      if (channels != null) 'channels': channels,
      if (rateControlMode != null) 'rateControlMode': rateControlMode.value,
      if (sampleRate != null) 'sampleRate': sampleRate,
      if (vbrQuality != null) 'vbrQuality': vbrQuality,
    };
  }
}

/// When enabled, file composition times will start at zero, composition times
/// in the 'ctts' (composition time to sample) box for B-frames will be
/// negative, and a 'cslg' (composition shift least greatest) box will be
/// included per 14496-1 amendment 1. This improves compatibility with Apple
/// players and tools.
enum Mp4CslgAtom {
  include('INCLUDE'),
  exclude('EXCLUDE'),
  ;

  final String value;

  const Mp4CslgAtom(this.value);

  static Mp4CslgAtom fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum Mp4CslgAtom'));
}

/// Inserts a free-space box immediately after the moov box.
enum Mp4FreeSpaceBox {
  include('INCLUDE'),
  exclude('EXCLUDE'),
  ;

  final String value;

  const Mp4FreeSpaceBox(this.value);

  static Mp4FreeSpaceBox fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Mp4FreeSpaceBox'));
}

/// To place the MOOV atom at the beginning of your output, which is useful for
/// progressive downloading: Leave blank or choose Progressive download. To
/// place the MOOV at the end of your output: Choose Normal.
enum Mp4MoovPlacement {
  progressiveDownload('PROGRESSIVE_DOWNLOAD'),
  normal('NORMAL'),
  ;

  final String value;

  const Mp4MoovPlacement(this.value);

  static Mp4MoovPlacement fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Mp4MoovPlacement'));
}

/// These settings relate to your MP4 output container. You can create audio
/// only outputs with this container. For more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/supported-codecs-containers-audio-only.html#output-codecs-and-containers-supported-for-audio-only.
class Mp4Settings {
  /// Specify this setting only when your output will be consumed by a downstream
  /// repackaging workflow that is sensitive to very small duration differences
  /// between video and audio. For this situation, choose Match video duration. In
  /// all other cases, keep the default value, Default codec duration. When you
  /// choose Match video duration, MediaConvert pads the output audio streams with
  /// silence or trims them to ensure that the total duration of each audio stream
  /// is at least as long as the total duration of the video stream. After padding
  /// or trimming, the audio stream duration is no more than one frame longer than
  /// the video stream. MediaConvert applies audio padding or trimming only to the
  /// end of the last segment of the output. For unsegmented outputs, MediaConvert
  /// adds padding only to the end of the file. When you keep the default value,
  /// any minor discrepancies between audio and video duration will depend on your
  /// output audio codec.
  final CmfcAudioDuration? audioDuration;

  /// When enabled, file composition times will start at zero, composition times
  /// in the 'ctts' (composition time to sample) box for B-frames will be
  /// negative, and a 'cslg' (composition shift least greatest) box will be
  /// included per 14496-1 amendment 1. This improves compatibility with Apple
  /// players and tools.
  final Mp4CslgAtom? cslgAtom;

  /// Ignore this setting unless compliance to the CTTS box version specification
  /// matters in your workflow. Specify a value of 1 to set your CTTS box version
  /// to 1 and make your output compliant with the specification. When you specify
  /// a value of 1, you must also set CSLG atom to the value INCLUDE. Keep the
  /// default value 0 to set your CTTS box version to 0. This can provide backward
  /// compatibility for some players and packagers.
  final int? cttsVersion;

  /// Inserts a free-space box immediately after the moov box.
  final Mp4FreeSpaceBox? freeSpaceBox;

  /// To place the MOOV atom at the beginning of your output, which is useful for
  /// progressive downloading: Leave blank or choose Progressive download. To
  /// place the MOOV at the end of your output: Choose Normal.
  final Mp4MoovPlacement? moovPlacement;

  /// Overrides the "Major Brand" field in the output file. Usually not necessary
  /// to specify.
  final String? mp4MajorBrand;

  Mp4Settings({
    this.audioDuration,
    this.cslgAtom,
    this.cttsVersion,
    this.freeSpaceBox,
    this.moovPlacement,
    this.mp4MajorBrand,
  });

  factory Mp4Settings.fromJson(Map<String, dynamic> json) {
    return Mp4Settings(
      audioDuration:
          (json['audioDuration'] as String?)?.let(CmfcAudioDuration.fromString),
      cslgAtom: (json['cslgAtom'] as String?)?.let(Mp4CslgAtom.fromString),
      cttsVersion: json['cttsVersion'] as int?,
      freeSpaceBox:
          (json['freeSpaceBox'] as String?)?.let(Mp4FreeSpaceBox.fromString),
      moovPlacement:
          (json['moovPlacement'] as String?)?.let(Mp4MoovPlacement.fromString),
      mp4MajorBrand: json['mp4MajorBrand'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final audioDuration = this.audioDuration;
    final cslgAtom = this.cslgAtom;
    final cttsVersion = this.cttsVersion;
    final freeSpaceBox = this.freeSpaceBox;
    final moovPlacement = this.moovPlacement;
    final mp4MajorBrand = this.mp4MajorBrand;
    return {
      if (audioDuration != null) 'audioDuration': audioDuration.value,
      if (cslgAtom != null) 'cslgAtom': cslgAtom.value,
      if (cttsVersion != null) 'cttsVersion': cttsVersion,
      if (freeSpaceBox != null) 'freeSpaceBox': freeSpaceBox.value,
      if (moovPlacement != null) 'moovPlacement': moovPlacement.value,
      if (mp4MajorBrand != null) 'mp4MajorBrand': mp4MajorBrand,
    };
  }
}

/// Optional. Choose Include to have MediaConvert mark up your DASH manifest
/// with <Accessibility> elements for embedded 608 captions. This markup isn't
/// generally required, but some video players require it to discover and play
/// embedded 608 captions. Keep the default value, Exclude, to leave these
/// elements out. When you enable this setting, this is the markup that
/// MediaConvert includes in your manifest: <Accessibility
/// schemeIdUri="urn:scte:dash:cc:cea-608:2015" value="CC1=eng"/>
enum MpdAccessibilityCaptionHints {
  include('INCLUDE'),
  exclude('EXCLUDE'),
  ;

  final String value;

  const MpdAccessibilityCaptionHints(this.value);

  static MpdAccessibilityCaptionHints fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum MpdAccessibilityCaptionHints'));
}

/// Specify this setting only when your output will be consumed by a downstream
/// repackaging workflow that is sensitive to very small duration differences
/// between video and audio. For this situation, choose Match video duration. In
/// all other cases, keep the default value, Default codec duration. When you
/// choose Match video duration, MediaConvert pads the output audio streams with
/// silence or trims them to ensure that the total duration of each audio stream
/// is at least as long as the total duration of the video stream. After padding
/// or trimming, the audio stream duration is no more than one frame longer than
/// the video stream. MediaConvert applies audio padding or trimming only to the
/// end of the last segment of the output. For unsegmented outputs, MediaConvert
/// adds padding only to the end of the file. When you keep the default value,
/// any minor discrepancies between audio and video duration will depend on your
/// output audio codec.
enum MpdAudioDuration {
  defaultCodecDuration('DEFAULT_CODEC_DURATION'),
  matchVideoDuration('MATCH_VIDEO_DURATION'),
  ;

  final String value;

  const MpdAudioDuration(this.value);

  static MpdAudioDuration fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum MpdAudioDuration'));
}

/// Use this setting only in DASH output groups that include sidecar TTML or
/// IMSC captions. You specify sidecar captions in a separate output from your
/// audio and video. Choose Raw for captions in a single XML file in a raw
/// container. Choose Fragmented MPEG-4 for captions in XML format contained
/// within fragmented MP4 files. This set of fragmented MP4 files is separate
/// from your video and audio fragmented MP4 files.
enum MpdCaptionContainerType {
  raw('RAW'),
  fragmentedMp4('FRAGMENTED_MP4'),
  ;

  final String value;

  const MpdCaptionContainerType(this.value);

  static MpdCaptionContainerType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum MpdCaptionContainerType'));
}

/// To include key-length-value metadata in this output: Set KLV metadata
/// insertion to Passthrough. MediaConvert reads KLV metadata present in your
/// input and writes each instance to a separate event message box in the
/// output, according to MISB ST1910.1. To exclude this KLV metadata: Set KLV
/// metadata insertion to None or leave blank.
enum MpdKlvMetadata {
  none('NONE'),
  passthrough('PASSTHROUGH'),
  ;

  final String value;

  const MpdKlvMetadata(this.value);

  static MpdKlvMetadata fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum MpdKlvMetadata'));
}

/// To add an InbandEventStream element in your output MPD manifest for each
/// type of event message, set Manifest metadata signaling to Enabled. For ID3
/// event messages, the InbandEventStream element schemeIdUri will be same value
/// that you specify for ID3 metadata scheme ID URI. For SCTE35 event messages,
/// the InbandEventStream element schemeIdUri will be
/// "urn:scte:scte35:2013:bin". To leave these elements out of your output MPD
/// manifest, set Manifest metadata signaling to Disabled. To enable Manifest
/// metadata signaling, you must also set SCTE-35 source to Passthrough, ESAM
/// SCTE-35 to insert, or ID3 metadata to Passthrough.
enum MpdManifestMetadataSignaling {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const MpdManifestMetadataSignaling(this.value);

  static MpdManifestMetadataSignaling fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum MpdManifestMetadataSignaling'));
}

/// Use this setting only when you specify SCTE-35 markers from ESAM. Choose
/// INSERT to put SCTE-35 markers in this output at the insertion points that
/// you specify in an ESAM XML document. Provide the document in the setting SCC
/// XML.
enum MpdScte35Esam {
  insert('INSERT'),
  none('NONE'),
  ;

  final String value;

  const MpdScte35Esam(this.value);

  static MpdScte35Esam fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum MpdScte35Esam'));
}

/// Ignore this setting unless you have SCTE-35 markers in your input video
/// file. Choose Passthrough if you want SCTE-35 markers that appear in your
/// input to also appear in this output. Choose None if you don't want those
/// SCTE-35 markers in this output.
enum MpdScte35Source {
  passthrough('PASSTHROUGH'),
  none('NONE'),
  ;

  final String value;

  const MpdScte35Source(this.value);

  static MpdScte35Source fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum MpdScte35Source'));
}

/// These settings relate to the fragmented MP4 container for the segments in
/// your DASH outputs.
class MpdSettings {
  /// Optional. Choose Include to have MediaConvert mark up your DASH manifest
  /// with <Accessibility> elements for embedded 608 captions. This markup isn't
  /// generally required, but some video players require it to discover and play
  /// embedded 608 captions. Keep the default value, Exclude, to leave these
  /// elements out. When you enable this setting, this is the markup that
  /// MediaConvert includes in your manifest: <Accessibility
  /// schemeIdUri="urn:scte:dash:cc:cea-608:2015" value="CC1=eng"/>
  final MpdAccessibilityCaptionHints? accessibilityCaptionHints;

  /// Specify this setting only when your output will be consumed by a downstream
  /// repackaging workflow that is sensitive to very small duration differences
  /// between video and audio. For this situation, choose Match video duration. In
  /// all other cases, keep the default value, Default codec duration. When you
  /// choose Match video duration, MediaConvert pads the output audio streams with
  /// silence or trims them to ensure that the total duration of each audio stream
  /// is at least as long as the total duration of the video stream. After padding
  /// or trimming, the audio stream duration is no more than one frame longer than
  /// the video stream. MediaConvert applies audio padding or trimming only to the
  /// end of the last segment of the output. For unsegmented outputs, MediaConvert
  /// adds padding only to the end of the file. When you keep the default value,
  /// any minor discrepancies between audio and video duration will depend on your
  /// output audio codec.
  final MpdAudioDuration? audioDuration;

  /// Use this setting only in DASH output groups that include sidecar TTML or
  /// IMSC captions. You specify sidecar captions in a separate output from your
  /// audio and video. Choose Raw for captions in a single XML file in a raw
  /// container. Choose Fragmented MPEG-4 for captions in XML format contained
  /// within fragmented MP4 files. This set of fragmented MP4 files is separate
  /// from your video and audio fragmented MP4 files.
  final MpdCaptionContainerType? captionContainerType;

  /// To include key-length-value metadata in this output: Set KLV metadata
  /// insertion to Passthrough. MediaConvert reads KLV metadata present in your
  /// input and writes each instance to a separate event message box in the
  /// output, according to MISB ST1910.1. To exclude this KLV metadata: Set KLV
  /// metadata insertion to None or leave blank.
  final MpdKlvMetadata? klvMetadata;

  /// To add an InbandEventStream element in your output MPD manifest for each
  /// type of event message, set Manifest metadata signaling to Enabled. For ID3
  /// event messages, the InbandEventStream element schemeIdUri will be same value
  /// that you specify for ID3 metadata scheme ID URI. For SCTE35 event messages,
  /// the InbandEventStream element schemeIdUri will be
  /// "urn:scte:scte35:2013:bin". To leave these elements out of your output MPD
  /// manifest, set Manifest metadata signaling to Disabled. To enable Manifest
  /// metadata signaling, you must also set SCTE-35 source to Passthrough, ESAM
  /// SCTE-35 to insert, or ID3 metadata to Passthrough.
  final MpdManifestMetadataSignaling? manifestMetadataSignaling;

  /// Use this setting only when you specify SCTE-35 markers from ESAM. Choose
  /// INSERT to put SCTE-35 markers in this output at the insertion points that
  /// you specify in an ESAM XML document. Provide the document in the setting SCC
  /// XML.
  final MpdScte35Esam? scte35Esam;

  /// Ignore this setting unless you have SCTE-35 markers in your input video
  /// file. Choose Passthrough if you want SCTE-35 markers that appear in your
  /// input to also appear in this output. Choose None if you don't want those
  /// SCTE-35 markers in this output.
  final MpdScte35Source? scte35Source;

  /// To include ID3 metadata in this output: Set ID3 metadata to Passthrough.
  /// Specify this ID3 metadata in Custom ID3 metadata inserter. MediaConvert
  /// writes each instance of ID3 metadata in a separate Event Message (eMSG) box.
  /// To exclude this ID3 metadata: Set ID3 metadata to None or leave blank.
  final MpdTimedMetadata? timedMetadata;

  /// Specify the event message box (eMSG) version for ID3 timed metadata in your
  /// output.
  /// For more information, see ISO/IEC 23009-1:2022 section 5.10.3.3.3 Syntax.
  /// Leave blank to use the default value Version 0.
  /// When you specify Version 1, you must also set ID3 metadata to Passthrough.
  final MpdTimedMetadataBoxVersion? timedMetadataBoxVersion;

  /// Specify the event message box (eMSG) scheme ID URI for ID3 timed metadata in
  /// your output. For more information, see ISO/IEC 23009-1:2022 section
  /// 5.10.3.3.4 Semantics. Leave blank to use the default value:
  /// https://aomedia.org/emsg/ID3 When you specify a value for ID3 metadata
  /// scheme ID URI, you must also set ID3 metadata to Passthrough.
  final String? timedMetadataSchemeIdUri;

  /// Specify the event message box (eMSG) value for ID3 timed metadata in your
  /// output. For more information, see ISO/IEC 23009-1:2022 section 5.10.3.3.4
  /// Semantics. When you specify a value for ID3 Metadata Value, you must also
  /// set ID3 metadata to Passthrough.
  final String? timedMetadataValue;

  MpdSettings({
    this.accessibilityCaptionHints,
    this.audioDuration,
    this.captionContainerType,
    this.klvMetadata,
    this.manifestMetadataSignaling,
    this.scte35Esam,
    this.scte35Source,
    this.timedMetadata,
    this.timedMetadataBoxVersion,
    this.timedMetadataSchemeIdUri,
    this.timedMetadataValue,
  });

  factory MpdSettings.fromJson(Map<String, dynamic> json) {
    return MpdSettings(
      accessibilityCaptionHints: (json['accessibilityCaptionHints'] as String?)
          ?.let(MpdAccessibilityCaptionHints.fromString),
      audioDuration:
          (json['audioDuration'] as String?)?.let(MpdAudioDuration.fromString),
      captionContainerType: (json['captionContainerType'] as String?)
          ?.let(MpdCaptionContainerType.fromString),
      klvMetadata:
          (json['klvMetadata'] as String?)?.let(MpdKlvMetadata.fromString),
      manifestMetadataSignaling: (json['manifestMetadataSignaling'] as String?)
          ?.let(MpdManifestMetadataSignaling.fromString),
      scte35Esam:
          (json['scte35Esam'] as String?)?.let(MpdScte35Esam.fromString),
      scte35Source:
          (json['scte35Source'] as String?)?.let(MpdScte35Source.fromString),
      timedMetadata:
          (json['timedMetadata'] as String?)?.let(MpdTimedMetadata.fromString),
      timedMetadataBoxVersion: (json['timedMetadataBoxVersion'] as String?)
          ?.let(MpdTimedMetadataBoxVersion.fromString),
      timedMetadataSchemeIdUri: json['timedMetadataSchemeIdUri'] as String?,
      timedMetadataValue: json['timedMetadataValue'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final accessibilityCaptionHints = this.accessibilityCaptionHints;
    final audioDuration = this.audioDuration;
    final captionContainerType = this.captionContainerType;
    final klvMetadata = this.klvMetadata;
    final manifestMetadataSignaling = this.manifestMetadataSignaling;
    final scte35Esam = this.scte35Esam;
    final scte35Source = this.scte35Source;
    final timedMetadata = this.timedMetadata;
    final timedMetadataBoxVersion = this.timedMetadataBoxVersion;
    final timedMetadataSchemeIdUri = this.timedMetadataSchemeIdUri;
    final timedMetadataValue = this.timedMetadataValue;
    return {
      if (accessibilityCaptionHints != null)
        'accessibilityCaptionHints': accessibilityCaptionHints.value,
      if (audioDuration != null) 'audioDuration': audioDuration.value,
      if (captionContainerType != null)
        'captionContainerType': captionContainerType.value,
      if (klvMetadata != null) 'klvMetadata': klvMetadata.value,
      if (manifestMetadataSignaling != null)
        'manifestMetadataSignaling': manifestMetadataSignaling.value,
      if (scte35Esam != null) 'scte35Esam': scte35Esam.value,
      if (scte35Source != null) 'scte35Source': scte35Source.value,
      if (timedMetadata != null) 'timedMetadata': timedMetadata.value,
      if (timedMetadataBoxVersion != null)
        'timedMetadataBoxVersion': timedMetadataBoxVersion.value,
      if (timedMetadataSchemeIdUri != null)
        'timedMetadataSchemeIdUri': timedMetadataSchemeIdUri,
      if (timedMetadataValue != null) 'timedMetadataValue': timedMetadataValue,
    };
  }
}

/// To include ID3 metadata in this output: Set ID3 metadata to Passthrough.
/// Specify this ID3 metadata in Custom ID3 metadata inserter. MediaConvert
/// writes each instance of ID3 metadata in a separate Event Message (eMSG) box.
/// To exclude this ID3 metadata: Set ID3 metadata to None or leave blank.
enum MpdTimedMetadata {
  passthrough('PASSTHROUGH'),
  none('NONE'),
  ;

  final String value;

  const MpdTimedMetadata(this.value);

  static MpdTimedMetadata fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum MpdTimedMetadata'));
}

/// Specify the event message box (eMSG) version for ID3 timed metadata in your
/// output.
/// For more information, see ISO/IEC 23009-1:2022 section 5.10.3.3.3 Syntax.
/// Leave blank to use the default value Version 0.
/// When you specify Version 1, you must also set ID3 metadata to Passthrough.
enum MpdTimedMetadataBoxVersion {
  version_0('VERSION_0'),
  version_1('VERSION_1'),
  ;

  final String value;

  const MpdTimedMetadataBoxVersion(this.value);

  static MpdTimedMetadataBoxVersion fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum MpdTimedMetadataBoxVersion'));
}

/// Specify the strength of any adaptive quantization filters that you enable.
/// The value that you choose here applies to the following settings: Spatial
/// adaptive quantization, and Temporal adaptive quantization.
enum Mpeg2AdaptiveQuantization {
  off('OFF'),
  low('LOW'),
  medium('MEDIUM'),
  high('HIGH'),
  ;

  final String value;

  const Mpeg2AdaptiveQuantization(this.value);

  static Mpeg2AdaptiveQuantization fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Mpeg2AdaptiveQuantization'));
}

/// Use Level to set the MPEG-2 level for the video output.
enum Mpeg2CodecLevel {
  auto('AUTO'),
  low('LOW'),
  main('MAIN'),
  high1440('HIGH1440'),
  high('HIGH'),
  ;

  final String value;

  const Mpeg2CodecLevel(this.value);

  static Mpeg2CodecLevel fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Mpeg2CodecLevel'));
}

/// Use Profile to set the MPEG-2 profile for the video output.
enum Mpeg2CodecProfile {
  main('MAIN'),
  profile_422('PROFILE_422'),
  ;

  final String value;

  const Mpeg2CodecProfile(this.value);

  static Mpeg2CodecProfile fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Mpeg2CodecProfile'));
}

/// Choose Adaptive to improve subjective video quality for high-motion content.
/// This will cause the service to use fewer B-frames (which infer information
/// based on other frames) for high-motion portions of the video and more
/// B-frames for low-motion portions. The maximum number of B-frames is limited
/// by the value you provide for the setting B frames between reference frames.
enum Mpeg2DynamicSubGop {
  adaptive('ADAPTIVE'),
  static('STATIC'),
  ;

  final String value;

  const Mpeg2DynamicSubGop(this.value);

  static Mpeg2DynamicSubGop fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Mpeg2DynamicSubGop'));
}

/// If you are using the console, use the Framerate setting to specify the frame
/// rate for this output. If you want to keep the same frame rate as the input
/// video, choose Follow source. If you want to do frame rate conversion, choose
/// a frame rate from the dropdown list or choose Custom. The framerates shown
/// in the dropdown list are decimal approximations of fractions. If you choose
/// Custom, specify your frame rate as a fraction.
enum Mpeg2FramerateControl {
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  specified('SPECIFIED'),
  ;

  final String value;

  const Mpeg2FramerateControl(this.value);

  static Mpeg2FramerateControl fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Mpeg2FramerateControl'));
}

/// Choose the method that you want MediaConvert to use when increasing or
/// decreasing the frame rate. For numerically simple conversions, such as 60
/// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
/// For numerically complex conversions, to avoid stutter: Choose Interpolate.
/// This results in a smooth picture, but might introduce undesirable video
/// artifacts. For complex frame rate conversions, especially if your source
/// video has already been converted from its original cadence: Choose
/// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
/// best conversion method frame by frame. Note that using FrameFormer increases
/// the transcoding time and incurs a significant add-on cost. When you choose
/// FrameFormer, your input video resolution must be at least 128x96.
enum Mpeg2FramerateConversionAlgorithm {
  duplicateDrop('DUPLICATE_DROP'),
  interpolate('INTERPOLATE'),
  frameformer('FRAMEFORMER'),
  ;

  final String value;

  const Mpeg2FramerateConversionAlgorithm(this.value);

  static Mpeg2FramerateConversionAlgorithm fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Mpeg2FramerateConversionAlgorithm'));
}

/// Specify the units for GOP size. If you don't specify a value here, by
/// default the encoder measures GOP size in frames.
enum Mpeg2GopSizeUnits {
  frames('FRAMES'),
  seconds('SECONDS'),
  ;

  final String value;

  const Mpeg2GopSizeUnits(this.value);

  static Mpeg2GopSizeUnits fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Mpeg2GopSizeUnits'));
}

/// Choose the scan line type for the output. Keep the default value,
/// Progressive to create a progressive output, regardless of the scan type of
/// your input. Use Top field first or Bottom field first to create an output
/// that's interlaced with the same field polarity throughout. Use Follow,
/// default top or Follow, default bottom to produce outputs with the same field
/// polarity as the source. For jobs that have multiple inputs, the output field
/// polarity might change over the course of the output. Follow behavior depends
/// on the input scan type. If the source is interlaced, the output will be
/// interlaced with the same polarity as the source. If the source is
/// progressive, the output will be interlaced with top field bottom field
/// first, depending on which of the Follow options you choose.
enum Mpeg2InterlaceMode {
  progressive('PROGRESSIVE'),
  topField('TOP_FIELD'),
  bottomField('BOTTOM_FIELD'),
  followTopField('FOLLOW_TOP_FIELD'),
  followBottomField('FOLLOW_BOTTOM_FIELD'),
  ;

  final String value;

  const Mpeg2InterlaceMode(this.value);

  static Mpeg2InterlaceMode fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Mpeg2InterlaceMode'));
}

/// Use Intra DC precision to set quantization precision for intra-block DC
/// coefficients. If you choose the value auto, the service will automatically
/// select the precision based on the per-frame compression ratio.
enum Mpeg2IntraDcPrecision {
  auto('AUTO'),
  intraDcPrecision_8('INTRA_DC_PRECISION_8'),
  intraDcPrecision_9('INTRA_DC_PRECISION_9'),
  intraDcPrecision_10('INTRA_DC_PRECISION_10'),
  intraDcPrecision_11('INTRA_DC_PRECISION_11'),
  ;

  final String value;

  const Mpeg2IntraDcPrecision(this.value);

  static Mpeg2IntraDcPrecision fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Mpeg2IntraDcPrecision'));
}

/// Optional. Specify how the service determines the pixel aspect ratio (PAR)
/// for this output. The default behavior, Follow source, uses the PAR from your
/// input video for your output. To specify a different PAR in the console,
/// choose any value other than Follow source. When you choose SPECIFIED for
/// this setting, you must also specify values for the parNumerator and
/// parDenominator settings.
enum Mpeg2ParControl {
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  specified('SPECIFIED'),
  ;

  final String value;

  const Mpeg2ParControl(this.value);

  static Mpeg2ParControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Mpeg2ParControl'));
}

/// Optional. Use Quality tuning level to choose how you want to trade off
/// encoding speed for output video quality. The default behavior is faster,
/// lower quality, single-pass encoding.
enum Mpeg2QualityTuningLevel {
  singlePass('SINGLE_PASS'),
  multiPass('MULTI_PASS'),
  ;

  final String value;

  const Mpeg2QualityTuningLevel(this.value);

  static Mpeg2QualityTuningLevel fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Mpeg2QualityTuningLevel'));
}

/// Use Rate control mode to specify whether the bitrate is variable (vbr) or
/// constant (cbr).
enum Mpeg2RateControlMode {
  vbr('VBR'),
  cbr('CBR'),
  ;

  final String value;

  const Mpeg2RateControlMode(this.value);

  static Mpeg2RateControlMode fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Mpeg2RateControlMode'));
}

/// Use this setting for interlaced outputs, when your output frame rate is half
/// of your input frame rate. In this situation, choose Optimized interlacing to
/// create a better quality interlaced output. In this case, each progressive
/// frame from the input corresponds to an interlaced field in the output. Keep
/// the default value, Basic interlacing, for all other output frame rates. With
/// basic interlacing, MediaConvert performs any frame rate conversion first and
/// then interlaces the frames. When you choose Optimized interlacing and you
/// set your output frame rate to a value that isn't suitable for optimized
/// interlacing, MediaConvert automatically falls back to basic interlacing.
/// Required settings: To use optimized interlacing, you must set Telecine to
/// None or Soft. You can't use optimized interlacing for hard telecine outputs.
/// You must also set Interlace mode to a value other than Progressive.
enum Mpeg2ScanTypeConversionMode {
  interlaced('INTERLACED'),
  interlacedOptimize('INTERLACED_OPTIMIZE'),
  ;

  final String value;

  const Mpeg2ScanTypeConversionMode(this.value);

  static Mpeg2ScanTypeConversionMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Mpeg2ScanTypeConversionMode'));
}

/// Enable this setting to insert I-frames at scene changes that the service
/// automatically detects. This improves video quality and is enabled by
/// default.
enum Mpeg2SceneChangeDetect {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const Mpeg2SceneChangeDetect(this.value);

  static Mpeg2SceneChangeDetect fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Mpeg2SceneChangeDetect'));
}

/// Required when you set Codec to the value MPEG2.
class Mpeg2Settings {
  /// Specify the strength of any adaptive quantization filters that you enable.
  /// The value that you choose here applies to the following settings: Spatial
  /// adaptive quantization, and Temporal adaptive quantization.
  final Mpeg2AdaptiveQuantization? adaptiveQuantization;

  /// Specify the average bitrate in bits per second. Required for VBR and CBR.
  /// For MS Smooth outputs, bitrates must be unique when rounded down to the
  /// nearest multiple of 1000.
  final int? bitrate;

  /// Use Level to set the MPEG-2 level for the video output.
  final Mpeg2CodecLevel? codecLevel;

  /// Use Profile to set the MPEG-2 profile for the video output.
  final Mpeg2CodecProfile? codecProfile;

  /// Choose Adaptive to improve subjective video quality for high-motion content.
  /// This will cause the service to use fewer B-frames (which infer information
  /// based on other frames) for high-motion portions of the video and more
  /// B-frames for low-motion portions. The maximum number of B-frames is limited
  /// by the value you provide for the setting B frames between reference frames.
  final Mpeg2DynamicSubGop? dynamicSubGop;

  /// If you are using the console, use the Framerate setting to specify the frame
  /// rate for this output. If you want to keep the same frame rate as the input
  /// video, choose Follow source. If you want to do frame rate conversion, choose
  /// a frame rate from the dropdown list or choose Custom. The framerates shown
  /// in the dropdown list are decimal approximations of fractions. If you choose
  /// Custom, specify your frame rate as a fraction.
  final Mpeg2FramerateControl? framerateControl;

  /// Choose the method that you want MediaConvert to use when increasing or
  /// decreasing the frame rate. For numerically simple conversions, such as 60
  /// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
  /// For numerically complex conversions, to avoid stutter: Choose Interpolate.
  /// This results in a smooth picture, but might introduce undesirable video
  /// artifacts. For complex frame rate conversions, especially if your source
  /// video has already been converted from its original cadence: Choose
  /// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
  /// best conversion method frame by frame. Note that using FrameFormer increases
  /// the transcoding time and incurs a significant add-on cost. When you choose
  /// FrameFormer, your input video resolution must be at least 128x96.
  final Mpeg2FramerateConversionAlgorithm? framerateConversionAlgorithm;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateDenominator to specify the denominator of this fraction.
  /// In this example, use 1001 for the value of FramerateDenominator. When you
  /// use the console for transcode jobs that use frame rate conversion, provide
  /// the value as a decimal number for Framerate. In this example, specify
  /// 23.976.
  final int? framerateDenominator;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateNumerator to specify the numerator of this fraction. In
  /// this example, use 24000 for the value of FramerateNumerator. When you use
  /// the console for transcode jobs that use frame rate conversion, provide the
  /// value as a decimal number for Framerate. In this example, specify 23.976.
  final int? framerateNumerator;

  /// Specify the relative frequency of open to closed GOPs in this output. For
  /// example, if you want to allow four open GOPs and then require a closed GOP,
  /// set this value to 5. When you create a streaming output, we recommend that
  /// you keep the default value, 1, so that players starting mid-stream receive
  /// an IDR frame as quickly as possible. Don't set this value to 0; that would
  /// break output segmenting.
  final int? gopClosedCadence;

  /// Specify the interval between keyframes, in seconds or frames, for this
  /// output. Default: 12 Related settings: When you specify the GOP size in
  /// seconds, set GOP mode control to Specified, seconds. The default value for
  /// GOP mode control is Frames.
  final double? gopSize;

  /// Specify the units for GOP size. If you don't specify a value here, by
  /// default the encoder measures GOP size in frames.
  final Mpeg2GopSizeUnits? gopSizeUnits;

  /// If your downstream systems have strict buffer requirements: Specify the
  /// minimum percentage of the HRD buffer that's available at the end of each
  /// encoded video segment. For the best video quality: Set to 0 or leave blank
  /// to automatically determine the final buffer fill percentage.
  final int? hrdBufferFinalFillPercentage;

  /// Percentage of the buffer that should initially be filled (HRD buffer model).
  final int? hrdBufferInitialFillPercentage;

  /// Size of buffer (HRD buffer model) in bits. For example, enter five megabits
  /// as 5000000.
  final int? hrdBufferSize;

  /// Choose the scan line type for the output. Keep the default value,
  /// Progressive to create a progressive output, regardless of the scan type of
  /// your input. Use Top field first or Bottom field first to create an output
  /// that's interlaced with the same field polarity throughout. Use Follow,
  /// default top or Follow, default bottom to produce outputs with the same field
  /// polarity as the source. For jobs that have multiple inputs, the output field
  /// polarity might change over the course of the output. Follow behavior depends
  /// on the input scan type. If the source is interlaced, the output will be
  /// interlaced with the same polarity as the source. If the source is
  /// progressive, the output will be interlaced with top field bottom field
  /// first, depending on which of the Follow options you choose.
  final Mpeg2InterlaceMode? interlaceMode;

  /// Use Intra DC precision to set quantization precision for intra-block DC
  /// coefficients. If you choose the value auto, the service will automatically
  /// select the precision based on the per-frame compression ratio.
  final Mpeg2IntraDcPrecision? intraDcPrecision;

  /// Maximum bitrate in bits/second. For example, enter five megabits per second
  /// as 5000000.
  final int? maxBitrate;

  /// Use this setting only when you also enable Scene change detection. This
  /// setting determines how the encoder manages the spacing between I-frames that
  /// it inserts as part of the I-frame cadence and the I-frames that it inserts
  /// for Scene change detection. When you specify a value for this setting, the
  /// encoder determines whether to skip a cadence-driven I-frame by the value you
  /// set. For example, if you set Min I interval to 5 and a cadence-driven
  /// I-frame would fall within 5 frames of a scene-change I-frame, then the
  /// encoder skips the cadence-driven I-frame. In this way, one GOP is shrunk
  /// slightly and one GOP is stretched slightly. When the cadence-driven I-frames
  /// are farther from the scene-change I-frame than the value you set, then the
  /// encoder leaves all I-frames in place and the GOPs surrounding the scene
  /// change are smaller than the usual cadence GOPs.
  final int? minIInterval;

  /// Specify the number of B-frames that MediaConvert puts between reference
  /// frames in this output. Valid values are whole numbers from 0 through 7. When
  /// you don't specify a value, MediaConvert defaults to 2.
  final int? numberBFramesBetweenReferenceFrames;

  /// Optional. Specify how the service determines the pixel aspect ratio (PAR)
  /// for this output. The default behavior, Follow source, uses the PAR from your
  /// input video for your output. To specify a different PAR in the console,
  /// choose any value other than Follow source. When you choose SPECIFIED for
  /// this setting, you must also specify values for the parNumerator and
  /// parDenominator settings.
  final Mpeg2ParControl? parControl;

  /// Required when you set Pixel aspect ratio to SPECIFIED. On the console, this
  /// corresponds to any value other than Follow source. When you specify an
  /// output pixel aspect ratio (PAR) that is different from your input video PAR,
  /// provide your output PAR as a ratio. For example, for D1/DV NTSC widescreen,
  /// you would specify the ratio 40:33. In this example, the value for
  /// parDenominator is 33.
  final int? parDenominator;

  /// Required when you set Pixel aspect ratio to SPECIFIED. On the console, this
  /// corresponds to any value other than Follow source. When you specify an
  /// output pixel aspect ratio (PAR) that is different from your input video PAR,
  /// provide your output PAR as a ratio. For example, for D1/DV NTSC widescreen,
  /// you would specify the ratio 40:33. In this example, the value for
  /// parNumerator is 40.
  final int? parNumerator;

  /// Optional. Use Quality tuning level to choose how you want to trade off
  /// encoding speed for output video quality. The default behavior is faster,
  /// lower quality, single-pass encoding.
  final Mpeg2QualityTuningLevel? qualityTuningLevel;

  /// Use Rate control mode to specify whether the bitrate is variable (vbr) or
  /// constant (cbr).
  final Mpeg2RateControlMode? rateControlMode;

  /// Use this setting for interlaced outputs, when your output frame rate is half
  /// of your input frame rate. In this situation, choose Optimized interlacing to
  /// create a better quality interlaced output. In this case, each progressive
  /// frame from the input corresponds to an interlaced field in the output. Keep
  /// the default value, Basic interlacing, for all other output frame rates. With
  /// basic interlacing, MediaConvert performs any frame rate conversion first and
  /// then interlaces the frames. When you choose Optimized interlacing and you
  /// set your output frame rate to a value that isn't suitable for optimized
  /// interlacing, MediaConvert automatically falls back to basic interlacing.
  /// Required settings: To use optimized interlacing, you must set Telecine to
  /// None or Soft. You can't use optimized interlacing for hard telecine outputs.
  /// You must also set Interlace mode to a value other than Progressive.
  final Mpeg2ScanTypeConversionMode? scanTypeConversionMode;

  /// Enable this setting to insert I-frames at scene changes that the service
  /// automatically detects. This improves video quality and is enabled by
  /// default.
  final Mpeg2SceneChangeDetect? sceneChangeDetect;

  /// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
  /// second (fps). Enable slow PAL to create a 25 fps output. When you enable
  /// slow PAL, MediaConvert relabels the video frames to 25 fps and resamples
  /// your audio to keep it synchronized with the video. Note that enabling this
  /// setting will slightly reduce the duration of your video. Required settings:
  /// You must also set Framerate to 25.
  final Mpeg2SlowPal? slowPal;

  /// Ignore this setting unless you need to comply with a specification that
  /// requires a specific value. If you don't have a specification requirement, we
  /// recommend that you adjust the softness of your output by using a lower value
  /// for the setting Sharpness or by enabling a noise reducer filter. The
  /// Softness setting specifies the quantization matrices that the encoder uses.
  /// Keep the default value, 0, to use the AWS Elemental default matrices. Choose
  /// a value from 17 to 128 to use planar interpolation. Increasing values from
  /// 17 to 128 result in increasing reduction of high-frequency data. The value
  /// 128 results in the softest video.
  final int? softness;

  /// Keep the default value, Enabled, to adjust quantization within each frame
  /// based on spatial variation of content complexity. When you enable this
  /// feature, the encoder uses fewer bits on areas that can sustain more
  /// distortion with no noticeable visual degradation and uses more bits on areas
  /// where any small distortion will be noticeable. For example, complex textured
  /// blocks are encoded with fewer bits and smooth textured blocks are encoded
  /// with more bits. Enabling this feature will almost always improve your video
  /// quality. Note, though, that this feature doesn't take into account where the
  /// viewer's attention is likely to be. If viewers are likely to be focusing
  /// their attention on a part of the screen with a lot of complex texture, you
  /// might choose to disable this feature. Related setting: When you enable
  /// spatial adaptive quantization, set the value for Adaptive quantization
  /// depending on your content. For homogeneous content, such as cartoons and
  /// video games, set it to Low. For content with a wider variety of textures,
  /// set it to High or Higher.
  final Mpeg2SpatialAdaptiveQuantization? spatialAdaptiveQuantization;

  /// Specify whether this output's video uses the D10 syntax. Keep the default
  /// value to not use the syntax. Related settings: When you choose D10 for your
  /// MXF profile, you must also set this value to D10.
  final Mpeg2Syntax? syntax;

  /// When you do frame rate conversion from 23.976 frames per second (fps) to
  /// 29.97 fps, and your output scan type is interlaced, you can optionally
  /// enable hard or soft telecine to create a smoother picture. Hard telecine
  /// produces a 29.97i output. Soft telecine produces an output with a 23.976
  /// output that signals to the video player device to do the conversion during
  /// play back. When you keep the default value, None, MediaConvert does a
  /// standard frame rate conversion to 29.97 without doing anything with the
  /// field polarity to create a smoother picture.
  final Mpeg2Telecine? telecine;

  /// Keep the default value, Enabled, to adjust quantization within each frame
  /// based on temporal variation of content complexity. When you enable this
  /// feature, the encoder uses fewer bits on areas of the frame that aren't
  /// moving and uses more bits on complex objects with sharp edges that move a
  /// lot. For example, this feature improves the readability of text tickers on
  /// newscasts and scoreboards on sports matches. Enabling this feature will
  /// almost always improve your video quality. Note, though, that this feature
  /// doesn't take into account where the viewer's attention is likely to be. If
  /// viewers are likely to be focusing their attention on a part of the screen
  /// that doesn't have moving objects with sharp edges, such as sports athletes'
  /// faces, you might choose to disable this feature. Related setting: When you
  /// enable temporal quantization, adjust the strength of the filter with the
  /// setting Adaptive quantization.
  final Mpeg2TemporalAdaptiveQuantization? temporalAdaptiveQuantization;

  Mpeg2Settings({
    this.adaptiveQuantization,
    this.bitrate,
    this.codecLevel,
    this.codecProfile,
    this.dynamicSubGop,
    this.framerateControl,
    this.framerateConversionAlgorithm,
    this.framerateDenominator,
    this.framerateNumerator,
    this.gopClosedCadence,
    this.gopSize,
    this.gopSizeUnits,
    this.hrdBufferFinalFillPercentage,
    this.hrdBufferInitialFillPercentage,
    this.hrdBufferSize,
    this.interlaceMode,
    this.intraDcPrecision,
    this.maxBitrate,
    this.minIInterval,
    this.numberBFramesBetweenReferenceFrames,
    this.parControl,
    this.parDenominator,
    this.parNumerator,
    this.qualityTuningLevel,
    this.rateControlMode,
    this.scanTypeConversionMode,
    this.sceneChangeDetect,
    this.slowPal,
    this.softness,
    this.spatialAdaptiveQuantization,
    this.syntax,
    this.telecine,
    this.temporalAdaptiveQuantization,
  });

  factory Mpeg2Settings.fromJson(Map<String, dynamic> json) {
    return Mpeg2Settings(
      adaptiveQuantization: (json['adaptiveQuantization'] as String?)
          ?.let(Mpeg2AdaptiveQuantization.fromString),
      bitrate: json['bitrate'] as int?,
      codecLevel:
          (json['codecLevel'] as String?)?.let(Mpeg2CodecLevel.fromString),
      codecProfile:
          (json['codecProfile'] as String?)?.let(Mpeg2CodecProfile.fromString),
      dynamicSubGop: (json['dynamicSubGop'] as String?)
          ?.let(Mpeg2DynamicSubGop.fromString),
      framerateControl: (json['framerateControl'] as String?)
          ?.let(Mpeg2FramerateControl.fromString),
      framerateConversionAlgorithm:
          (json['framerateConversionAlgorithm'] as String?)
              ?.let(Mpeg2FramerateConversionAlgorithm.fromString),
      framerateDenominator: json['framerateDenominator'] as int?,
      framerateNumerator: json['framerateNumerator'] as int?,
      gopClosedCadence: json['gopClosedCadence'] as int?,
      gopSize: json['gopSize'] as double?,
      gopSizeUnits:
          (json['gopSizeUnits'] as String?)?.let(Mpeg2GopSizeUnits.fromString),
      hrdBufferFinalFillPercentage:
          json['hrdBufferFinalFillPercentage'] as int?,
      hrdBufferInitialFillPercentage:
          json['hrdBufferInitialFillPercentage'] as int?,
      hrdBufferSize: json['hrdBufferSize'] as int?,
      interlaceMode: (json['interlaceMode'] as String?)
          ?.let(Mpeg2InterlaceMode.fromString),
      intraDcPrecision: (json['intraDcPrecision'] as String?)
          ?.let(Mpeg2IntraDcPrecision.fromString),
      maxBitrate: json['maxBitrate'] as int?,
      minIInterval: json['minIInterval'] as int?,
      numberBFramesBetweenReferenceFrames:
          json['numberBFramesBetweenReferenceFrames'] as int?,
      parControl:
          (json['parControl'] as String?)?.let(Mpeg2ParControl.fromString),
      parDenominator: json['parDenominator'] as int?,
      parNumerator: json['parNumerator'] as int?,
      qualityTuningLevel: (json['qualityTuningLevel'] as String?)
          ?.let(Mpeg2QualityTuningLevel.fromString),
      rateControlMode: (json['rateControlMode'] as String?)
          ?.let(Mpeg2RateControlMode.fromString),
      scanTypeConversionMode: (json['scanTypeConversionMode'] as String?)
          ?.let(Mpeg2ScanTypeConversionMode.fromString),
      sceneChangeDetect: (json['sceneChangeDetect'] as String?)
          ?.let(Mpeg2SceneChangeDetect.fromString),
      slowPal: (json['slowPal'] as String?)?.let(Mpeg2SlowPal.fromString),
      softness: json['softness'] as int?,
      spatialAdaptiveQuantization:
          (json['spatialAdaptiveQuantization'] as String?)
              ?.let(Mpeg2SpatialAdaptiveQuantization.fromString),
      syntax: (json['syntax'] as String?)?.let(Mpeg2Syntax.fromString),
      telecine: (json['telecine'] as String?)?.let(Mpeg2Telecine.fromString),
      temporalAdaptiveQuantization:
          (json['temporalAdaptiveQuantization'] as String?)
              ?.let(Mpeg2TemporalAdaptiveQuantization.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final adaptiveQuantization = this.adaptiveQuantization;
    final bitrate = this.bitrate;
    final codecLevel = this.codecLevel;
    final codecProfile = this.codecProfile;
    final dynamicSubGop = this.dynamicSubGop;
    final framerateControl = this.framerateControl;
    final framerateConversionAlgorithm = this.framerateConversionAlgorithm;
    final framerateDenominator = this.framerateDenominator;
    final framerateNumerator = this.framerateNumerator;
    final gopClosedCadence = this.gopClosedCadence;
    final gopSize = this.gopSize;
    final gopSizeUnits = this.gopSizeUnits;
    final hrdBufferFinalFillPercentage = this.hrdBufferFinalFillPercentage;
    final hrdBufferInitialFillPercentage = this.hrdBufferInitialFillPercentage;
    final hrdBufferSize = this.hrdBufferSize;
    final interlaceMode = this.interlaceMode;
    final intraDcPrecision = this.intraDcPrecision;
    final maxBitrate = this.maxBitrate;
    final minIInterval = this.minIInterval;
    final numberBFramesBetweenReferenceFrames =
        this.numberBFramesBetweenReferenceFrames;
    final parControl = this.parControl;
    final parDenominator = this.parDenominator;
    final parNumerator = this.parNumerator;
    final qualityTuningLevel = this.qualityTuningLevel;
    final rateControlMode = this.rateControlMode;
    final scanTypeConversionMode = this.scanTypeConversionMode;
    final sceneChangeDetect = this.sceneChangeDetect;
    final slowPal = this.slowPal;
    final softness = this.softness;
    final spatialAdaptiveQuantization = this.spatialAdaptiveQuantization;
    final syntax = this.syntax;
    final telecine = this.telecine;
    final temporalAdaptiveQuantization = this.temporalAdaptiveQuantization;
    return {
      if (adaptiveQuantization != null)
        'adaptiveQuantization': adaptiveQuantization.value,
      if (bitrate != null) 'bitrate': bitrate,
      if (codecLevel != null) 'codecLevel': codecLevel.value,
      if (codecProfile != null) 'codecProfile': codecProfile.value,
      if (dynamicSubGop != null) 'dynamicSubGop': dynamicSubGop.value,
      if (framerateControl != null) 'framerateControl': framerateControl.value,
      if (framerateConversionAlgorithm != null)
        'framerateConversionAlgorithm': framerateConversionAlgorithm.value,
      if (framerateDenominator != null)
        'framerateDenominator': framerateDenominator,
      if (framerateNumerator != null) 'framerateNumerator': framerateNumerator,
      if (gopClosedCadence != null) 'gopClosedCadence': gopClosedCadence,
      if (gopSize != null) 'gopSize': gopSize,
      if (gopSizeUnits != null) 'gopSizeUnits': gopSizeUnits.value,
      if (hrdBufferFinalFillPercentage != null)
        'hrdBufferFinalFillPercentage': hrdBufferFinalFillPercentage,
      if (hrdBufferInitialFillPercentage != null)
        'hrdBufferInitialFillPercentage': hrdBufferInitialFillPercentage,
      if (hrdBufferSize != null) 'hrdBufferSize': hrdBufferSize,
      if (interlaceMode != null) 'interlaceMode': interlaceMode.value,
      if (intraDcPrecision != null) 'intraDcPrecision': intraDcPrecision.value,
      if (maxBitrate != null) 'maxBitrate': maxBitrate,
      if (minIInterval != null) 'minIInterval': minIInterval,
      if (numberBFramesBetweenReferenceFrames != null)
        'numberBFramesBetweenReferenceFrames':
            numberBFramesBetweenReferenceFrames,
      if (parControl != null) 'parControl': parControl.value,
      if (parDenominator != null) 'parDenominator': parDenominator,
      if (parNumerator != null) 'parNumerator': parNumerator,
      if (qualityTuningLevel != null)
        'qualityTuningLevel': qualityTuningLevel.value,
      if (rateControlMode != null) 'rateControlMode': rateControlMode.value,
      if (scanTypeConversionMode != null)
        'scanTypeConversionMode': scanTypeConversionMode.value,
      if (sceneChangeDetect != null)
        'sceneChangeDetect': sceneChangeDetect.value,
      if (slowPal != null) 'slowPal': slowPal.value,
      if (softness != null) 'softness': softness,
      if (spatialAdaptiveQuantization != null)
        'spatialAdaptiveQuantization': spatialAdaptiveQuantization.value,
      if (syntax != null) 'syntax': syntax.value,
      if (telecine != null) 'telecine': telecine.value,
      if (temporalAdaptiveQuantization != null)
        'temporalAdaptiveQuantization': temporalAdaptiveQuantization.value,
    };
  }
}

/// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
/// second (fps). Enable slow PAL to create a 25 fps output. When you enable
/// slow PAL, MediaConvert relabels the video frames to 25 fps and resamples
/// your audio to keep it synchronized with the video. Note that enabling this
/// setting will slightly reduce the duration of your video. Required settings:
/// You must also set Framerate to 25.
enum Mpeg2SlowPal {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const Mpeg2SlowPal(this.value);

  static Mpeg2SlowPal fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Mpeg2SlowPal'));
}

/// Keep the default value, Enabled, to adjust quantization within each frame
/// based on spatial variation of content complexity. When you enable this
/// feature, the encoder uses fewer bits on areas that can sustain more
/// distortion with no noticeable visual degradation and uses more bits on areas
/// where any small distortion will be noticeable. For example, complex textured
/// blocks are encoded with fewer bits and smooth textured blocks are encoded
/// with more bits. Enabling this feature will almost always improve your video
/// quality. Note, though, that this feature doesn't take into account where the
/// viewer's attention is likely to be. If viewers are likely to be focusing
/// their attention on a part of the screen with a lot of complex texture, you
/// might choose to disable this feature. Related setting: When you enable
/// spatial adaptive quantization, set the value for Adaptive quantization
/// depending on your content. For homogeneous content, such as cartoons and
/// video games, set it to Low. For content with a wider variety of textures,
/// set it to High or Higher.
enum Mpeg2SpatialAdaptiveQuantization {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const Mpeg2SpatialAdaptiveQuantization(this.value);

  static Mpeg2SpatialAdaptiveQuantization fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Mpeg2SpatialAdaptiveQuantization'));
}

/// Specify whether this output's video uses the D10 syntax. Keep the default
/// value to not use the syntax. Related settings: When you choose D10 for your
/// MXF profile, you must also set this value to D10.
enum Mpeg2Syntax {
  $default('DEFAULT'),
  d_10('D_10'),
  ;

  final String value;

  const Mpeg2Syntax(this.value);

  static Mpeg2Syntax fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum Mpeg2Syntax'));
}

/// When you do frame rate conversion from 23.976 frames per second (fps) to
/// 29.97 fps, and your output scan type is interlaced, you can optionally
/// enable hard or soft telecine to create a smoother picture. Hard telecine
/// produces a 29.97i output. Soft telecine produces an output with a 23.976
/// output that signals to the video player device to do the conversion during
/// play back. When you keep the default value, None, MediaConvert does a
/// standard frame rate conversion to 29.97 without doing anything with the
/// field polarity to create a smoother picture.
enum Mpeg2Telecine {
  none('NONE'),
  soft('SOFT'),
  hard('HARD'),
  ;

  final String value;

  const Mpeg2Telecine(this.value);

  static Mpeg2Telecine fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Mpeg2Telecine'));
}

/// Keep the default value, Enabled, to adjust quantization within each frame
/// based on temporal variation of content complexity. When you enable this
/// feature, the encoder uses fewer bits on areas of the frame that aren't
/// moving and uses more bits on complex objects with sharp edges that move a
/// lot. For example, this feature improves the readability of text tickers on
/// newscasts and scoreboards on sports matches. Enabling this feature will
/// almost always improve your video quality. Note, though, that this feature
/// doesn't take into account where the viewer's attention is likely to be. If
/// viewers are likely to be focusing their attention on a part of the screen
/// that doesn't have moving objects with sharp edges, such as sports athletes'
/// faces, you might choose to disable this feature. Related setting: When you
/// enable temporal quantization, adjust the strength of the filter with the
/// setting Adaptive quantization.
enum Mpeg2TemporalAdaptiveQuantization {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const Mpeg2TemporalAdaptiveQuantization(this.value);

  static Mpeg2TemporalAdaptiveQuantization fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Mpeg2TemporalAdaptiveQuantization'));
}

/// Specify the details for each additional Microsoft Smooth Streaming manifest
/// that you want the service to generate for this output group. Each manifest
/// can reference a different subset of outputs in the group.
class MsSmoothAdditionalManifest {
  /// Specify a name modifier that the service adds to the name of this manifest
  /// to make it different from the file names of the other main manifests in the
  /// output group. For example, say that the default main manifest for your
  /// Microsoft Smooth group is film-name.ismv. If you enter "-no-premium" for
  /// this setting, then the file name the service generates for this top-level
  /// manifest is film-name-no-premium.ismv.
  final String? manifestNameModifier;

  /// Specify the outputs that you want this additional top-level manifest to
  /// reference.
  final List<String>? selectedOutputs;

  MsSmoothAdditionalManifest({
    this.manifestNameModifier,
    this.selectedOutputs,
  });

  factory MsSmoothAdditionalManifest.fromJson(Map<String, dynamic> json) {
    return MsSmoothAdditionalManifest(
      manifestNameModifier: json['manifestNameModifier'] as String?,
      selectedOutputs: (json['selectedOutputs'] as List?)
          ?.nonNulls
          .map((e) => e as String)
          .toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final manifestNameModifier = this.manifestNameModifier;
    final selectedOutputs = this.selectedOutputs;
    return {
      if (manifestNameModifier != null)
        'manifestNameModifier': manifestNameModifier,
      if (selectedOutputs != null) 'selectedOutputs': selectedOutputs,
    };
  }
}

/// COMBINE_DUPLICATE_STREAMS combines identical audio encoding settings across
/// a Microsoft Smooth output group into a single audio stream.
enum MsSmoothAudioDeduplication {
  combineDuplicateStreams('COMBINE_DUPLICATE_STREAMS'),
  none('NONE'),
  ;

  final String value;

  const MsSmoothAudioDeduplication(this.value);

  static MsSmoothAudioDeduplication fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum MsSmoothAudioDeduplication'));
}

/// If you are using DRM, set DRM System to specify the value SpekeKeyProvider.
class MsSmoothEncryptionSettings {
  /// If your output group type is HLS, DASH, or Microsoft Smooth, use these
  /// settings when doing DRM encryption with a SPEKE-compliant key provider. If
  /// your output group type is CMAF, use the SpekeKeyProviderCmaf settings
  /// instead.
  final SpekeKeyProvider? spekeKeyProvider;

  MsSmoothEncryptionSettings({
    this.spekeKeyProvider,
  });

  factory MsSmoothEncryptionSettings.fromJson(Map<String, dynamic> json) {
    return MsSmoothEncryptionSettings(
      spekeKeyProvider: json['spekeKeyProvider'] != null
          ? SpekeKeyProvider.fromJson(
              json['spekeKeyProvider'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final spekeKeyProvider = this.spekeKeyProvider;
    return {
      if (spekeKeyProvider != null) 'spekeKeyProvider': spekeKeyProvider,
    };
  }
}

/// Specify how you want MediaConvert to determine the fragment length. Choose
/// Exact to have the encoder use the exact length that you specify with the
/// setting Fragment length. This might result in extra I-frames. Choose
/// Multiple of GOP to have the encoder round up the segment lengths to match
/// the next GOP boundary.
enum MsSmoothFragmentLengthControl {
  exact('EXACT'),
  gopMultiple('GOP_MULTIPLE'),
  ;

  final String value;

  const MsSmoothFragmentLengthControl(this.value);

  static MsSmoothFragmentLengthControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum MsSmoothFragmentLengthControl'));
}

/// Settings related to your Microsoft Smooth Streaming output package. For more
/// information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/outputs-file-ABR.html.
class MsSmoothGroupSettings {
  /// By default, the service creates one .ism Microsoft Smooth Streaming manifest
  /// for each Microsoft Smooth Streaming output group in your job. This default
  /// manifest references every output in the output group. To create additional
  /// manifests that reference a subset of the outputs in the output group,
  /// specify a list of them here.
  final List<MsSmoothAdditionalManifest>? additionalManifests;

  /// COMBINE_DUPLICATE_STREAMS combines identical audio encoding settings across
  /// a Microsoft Smooth output group into a single audio stream.
  final MsSmoothAudioDeduplication? audioDeduplication;

  /// Use Destination to specify the S3 output location and the output filename
  /// base. Destination accepts format identifiers. If you do not specify the base
  /// filename in the URI, the service will use the filename of the input file. If
  /// your job has multiple inputs, the service uses the filename of the first
  /// input file.
  final String? destination;

  /// Settings associated with the destination. Will vary based on the type of
  /// destination
  final DestinationSettings? destinationSettings;

  /// If you are using DRM, set DRM System to specify the value SpekeKeyProvider.
  final MsSmoothEncryptionSettings? encryption;

  /// Specify how you want MediaConvert to determine the fragment length. Choose
  /// Exact to have the encoder use the exact length that you specify with the
  /// setting Fragment length. This might result in extra I-frames. Choose
  /// Multiple of GOP to have the encoder round up the segment lengths to match
  /// the next GOP boundary.
  final int? fragmentLength;

  /// Specify how you want MediaConvert to determine the fragment length. Choose
  /// Exact to have the encoder use the exact length that you specify with the
  /// setting Fragment length. This might result in extra I-frames. Choose
  /// Multiple of GOP to have the encoder round up the segment lengths to match
  /// the next GOP boundary.
  final MsSmoothFragmentLengthControl? fragmentLengthControl;

  /// Use Manifest encoding to specify the encoding format for the server and
  /// client manifest. Valid options are utf8 and utf16.
  final MsSmoothManifestEncoding? manifestEncoding;

  MsSmoothGroupSettings({
    this.additionalManifests,
    this.audioDeduplication,
    this.destination,
    this.destinationSettings,
    this.encryption,
    this.fragmentLength,
    this.fragmentLengthControl,
    this.manifestEncoding,
  });

  factory MsSmoothGroupSettings.fromJson(Map<String, dynamic> json) {
    return MsSmoothGroupSettings(
      additionalManifests: (json['additionalManifests'] as List?)
          ?.nonNulls
          .map((e) =>
              MsSmoothAdditionalManifest.fromJson(e as Map<String, dynamic>))
          .toList(),
      audioDeduplication: (json['audioDeduplication'] as String?)
          ?.let(MsSmoothAudioDeduplication.fromString),
      destination: json['destination'] as String?,
      destinationSettings: json['destinationSettings'] != null
          ? DestinationSettings.fromJson(
              json['destinationSettings'] as Map<String, dynamic>)
          : null,
      encryption: json['encryption'] != null
          ? MsSmoothEncryptionSettings.fromJson(
              json['encryption'] as Map<String, dynamic>)
          : null,
      fragmentLength: json['fragmentLength'] as int?,
      fragmentLengthControl: (json['fragmentLengthControl'] as String?)
          ?.let(MsSmoothFragmentLengthControl.fromString),
      manifestEncoding: (json['manifestEncoding'] as String?)
          ?.let(MsSmoothManifestEncoding.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final additionalManifests = this.additionalManifests;
    final audioDeduplication = this.audioDeduplication;
    final destination = this.destination;
    final destinationSettings = this.destinationSettings;
    final encryption = this.encryption;
    final fragmentLength = this.fragmentLength;
    final fragmentLengthControl = this.fragmentLengthControl;
    final manifestEncoding = this.manifestEncoding;
    return {
      if (additionalManifests != null)
        'additionalManifests': additionalManifests,
      if (audioDeduplication != null)
        'audioDeduplication': audioDeduplication.value,
      if (destination != null) 'destination': destination,
      if (destinationSettings != null)
        'destinationSettings': destinationSettings,
      if (encryption != null) 'encryption': encryption,
      if (fragmentLength != null) 'fragmentLength': fragmentLength,
      if (fragmentLengthControl != null)
        'fragmentLengthControl': fragmentLengthControl.value,
      if (manifestEncoding != null) 'manifestEncoding': manifestEncoding.value,
    };
  }
}

/// Use Manifest encoding to specify the encoding format for the server and
/// client manifest. Valid options are utf8 and utf16.
enum MsSmoothManifestEncoding {
  utf8('UTF8'),
  utf16('UTF16'),
  ;

  final String value;

  const MsSmoothManifestEncoding(this.value);

  static MsSmoothManifestEncoding fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum MsSmoothManifestEncoding'));
}

/// Optional. When you have AFD signaling set up in your output video stream,
/// use this setting to choose whether to also include it in the MXF wrapper.
/// Choose Don't copy to exclude AFD signaling from the MXF wrapper. Choose Copy
/// from video stream to copy the AFD values from the video stream for this
/// output to the MXF wrapper. Regardless of which option you choose, the AFD
/// values remain in the video stream. Related settings: To set up your output
/// to include or exclude AFD values, see AfdSignaling, under VideoDescription.
/// On the console, find AFD signaling under the output's video encoding
/// settings.
enum MxfAfdSignaling {
  noCopy('NO_COPY'),
  copyFromVideo('COPY_FROM_VIDEO'),
  ;

  final String value;

  const MxfAfdSignaling(this.value);

  static MxfAfdSignaling fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum MxfAfdSignaling'));
}

/// Specify the MXF profile, also called shim, for this output. To automatically
/// select a profile according to your output video codec and resolution, leave
/// blank. For a list of codecs supported with each MXF profile, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/codecs-supported-with-each-mxf-profile.html.
/// For more information about the automatic selection behavior, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/default-automatic-selection-of-mxf-profiles.html.
enum MxfProfile {
  d_10('D_10'),
  xdcam('XDCAM'),
  op1a('OP1A'),
  xavc('XAVC'),
  xdcamRdd9('XDCAM_RDD9'),
  ;

  final String value;

  const MxfProfile(this.value);

  static MxfProfile fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum MxfProfile'));
}

/// These settings relate to your MXF output container.
class MxfSettings {
  /// Optional. When you have AFD signaling set up in your output video stream,
  /// use this setting to choose whether to also include it in the MXF wrapper.
  /// Choose Don't copy to exclude AFD signaling from the MXF wrapper. Choose Copy
  /// from video stream to copy the AFD values from the video stream for this
  /// output to the MXF wrapper. Regardless of which option you choose, the AFD
  /// values remain in the video stream. Related settings: To set up your output
  /// to include or exclude AFD values, see AfdSignaling, under VideoDescription.
  /// On the console, find AFD signaling under the output's video encoding
  /// settings.
  final MxfAfdSignaling? afdSignaling;

  /// Specify the MXF profile, also called shim, for this output. To automatically
  /// select a profile according to your output video codec and resolution, leave
  /// blank. For a list of codecs supported with each MXF profile, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/codecs-supported-with-each-mxf-profile.html.
  /// For more information about the automatic selection behavior, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/default-automatic-selection-of-mxf-profiles.html.
  final MxfProfile? profile;

  /// Specify the XAVC profile settings for MXF outputs when you set your MXF
  /// profile to XAVC.
  final MxfXavcProfileSettings? xavcProfileSettings;

  MxfSettings({
    this.afdSignaling,
    this.profile,
    this.xavcProfileSettings,
  });

  factory MxfSettings.fromJson(Map<String, dynamic> json) {
    return MxfSettings(
      afdSignaling:
          (json['afdSignaling'] as String?)?.let(MxfAfdSignaling.fromString),
      profile: (json['profile'] as String?)?.let(MxfProfile.fromString),
      xavcProfileSettings: json['xavcProfileSettings'] != null
          ? MxfXavcProfileSettings.fromJson(
              json['xavcProfileSettings'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final afdSignaling = this.afdSignaling;
    final profile = this.profile;
    final xavcProfileSettings = this.xavcProfileSettings;
    return {
      if (afdSignaling != null) 'afdSignaling': afdSignaling.value,
      if (profile != null) 'profile': profile.value,
      if (xavcProfileSettings != null)
        'xavcProfileSettings': xavcProfileSettings,
    };
  }
}

/// To create an output that complies with the XAVC file format guidelines for
/// interoperability, keep the default value, Drop frames for compliance. To
/// include all frames from your input in this output, keep the default setting,
/// Allow any duration. The number of frames that MediaConvert excludes when you
/// set this to Drop frames for compliance depends on the output frame rate and
/// duration.
enum MxfXavcDurationMode {
  allowAnyDuration('ALLOW_ANY_DURATION'),
  dropFramesForCompliance('DROP_FRAMES_FOR_COMPLIANCE'),
  ;

  final String value;

  const MxfXavcDurationMode(this.value);

  static MxfXavcDurationMode fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum MxfXavcDurationMode'));
}

/// Specify the XAVC profile settings for MXF outputs when you set your MXF
/// profile to XAVC.
class MxfXavcProfileSettings {
  /// To create an output that complies with the XAVC file format guidelines for
  /// interoperability, keep the default value, Drop frames for compliance. To
  /// include all frames from your input in this output, keep the default setting,
  /// Allow any duration. The number of frames that MediaConvert excludes when you
  /// set this to Drop frames for compliance depends on the output frame rate and
  /// duration.
  final MxfXavcDurationMode? durationMode;

  /// Specify a value for this setting only for outputs that you set up with one
  /// of these two XAVC profiles: XAVC HD Intra CBG or XAVC 4K Intra CBG. Specify
  /// the amount of space in each frame that the service reserves for ancillary
  /// data, such as teletext captions. The default value for this setting is 1492
  /// bytes per frame. This should be sufficient to prevent overflow unless you
  /// have multiple pages of teletext captions data. If you have a large amount of
  /// teletext data, specify a larger number.
  final int? maxAncDataSize;

  MxfXavcProfileSettings({
    this.durationMode,
    this.maxAncDataSize,
  });

  factory MxfXavcProfileSettings.fromJson(Map<String, dynamic> json) {
    return MxfXavcProfileSettings(
      durationMode: (json['durationMode'] as String?)
          ?.let(MxfXavcDurationMode.fromString),
      maxAncDataSize: json['maxAncDataSize'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final durationMode = this.durationMode;
    final maxAncDataSize = this.maxAncDataSize;
    return {
      if (durationMode != null) 'durationMode': durationMode.value,
      if (maxAncDataSize != null) 'maxAncDataSize': maxAncDataSize,
    };
  }
}

/// For forensic video watermarking, MediaConvert supports Nagra NexGuard File
/// Marker watermarking. MediaConvert supports both PreRelease Content (NGPR/G2)
/// and OTT Streaming workflows.
class NexGuardFileMarkerSettings {
  /// Use the base64 license string that Nagra provides you. Enter it directly in
  /// your JSON job specification or in the console. Required when you include
  /// Nagra NexGuard File Marker watermarking in your job.
  final String? license;

  /// Specify the payload ID that you want associated with this output. Valid
  /// values vary depending on your Nagra NexGuard forensic watermarking workflow.
  /// Required when you include Nagra NexGuard File Marker watermarking in your
  /// job. For PreRelease Content (NGPR/G2), specify an integer from 1 through
  /// 4,194,303. You must generate a unique ID for each asset you watermark, and
  /// keep a record of which ID you have assigned to each asset. Neither Nagra nor
  /// MediaConvert keep track of the relationship between output files and your
  /// IDs. For OTT Streaming, create two adaptive bitrate (ABR) stacks for each
  /// asset. Do this by setting up two output groups. For one output group, set
  /// the value of Payload ID to 0 in every output. For the other output group,
  /// set Payload ID to 1 in every output.
  final int? payload;

  /// Enter one of the watermarking preset strings that Nagra provides you.
  /// Required when you include Nagra NexGuard File Marker watermarking in your
  /// job.
  final String? preset;

  /// Optional. Ignore this setting unless Nagra support directs you to specify a
  /// value. When you don't specify a value here, the Nagra NexGuard library uses
  /// its default value.
  final WatermarkingStrength? strength;

  NexGuardFileMarkerSettings({
    this.license,
    this.payload,
    this.preset,
    this.strength,
  });

  factory NexGuardFileMarkerSettings.fromJson(Map<String, dynamic> json) {
    return NexGuardFileMarkerSettings(
      license: json['license'] as String?,
      payload: json['payload'] as int?,
      preset: json['preset'] as String?,
      strength:
          (json['strength'] as String?)?.let(WatermarkingStrength.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final license = this.license;
    final payload = this.payload;
    final preset = this.preset;
    final strength = this.strength;
    return {
      if (license != null) 'license': license,
      if (payload != null) 'payload': payload,
      if (preset != null) 'preset': preset,
      if (strength != null) 'strength': strength.value,
    };
  }
}

/// Choose the type of Nielsen watermarks that you want in your outputs. When
/// you choose NAES 2 and NW, you must provide a value for the setting SID. When
/// you choose CBET, you must provide a value for the setting CSID. When you
/// choose NAES 2, NW, and CBET, you must provide values for both of these
/// settings.
enum NielsenActiveWatermarkProcessType {
  naes2AndNw('NAES2_AND_NW'),
  cbet('CBET'),
  naes2AndNwAndCbet('NAES2_AND_NW_AND_CBET'),
  ;

  final String value;

  const NielsenActiveWatermarkProcessType(this.value);

  static NielsenActiveWatermarkProcessType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum NielsenActiveWatermarkProcessType'));
}

/// Settings for your Nielsen configuration. If you don't do Nielsen measurement
/// and analytics, ignore these settings. When you enable Nielsen configuration,
/// MediaConvert enables PCM to ID3 tagging for all outputs in the job.
class NielsenConfiguration {
  /// Nielsen has discontinued the use of breakout code functionality. If you must
  /// include this property, set the value to zero.
  final int? breakoutCode;

  /// Use Distributor ID to specify the distributor ID that is assigned to your
  /// organization by Nielsen.
  final String? distributorId;

  NielsenConfiguration({
    this.breakoutCode,
    this.distributorId,
  });

  factory NielsenConfiguration.fromJson(Map<String, dynamic> json) {
    return NielsenConfiguration(
      breakoutCode: json['breakoutCode'] as int?,
      distributorId: json['distributorId'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final breakoutCode = this.breakoutCode;
    final distributorId = this.distributorId;
    return {
      if (breakoutCode != null) 'breakoutCode': breakoutCode,
      if (distributorId != null) 'distributorId': distributorId,
    };
  }
}

/// Ignore these settings unless you are using Nielsen non-linear watermarking.
/// Specify the values that MediaConvert uses to generate and place Nielsen
/// watermarks in your output audio. In addition to specifying these values, you
/// also need to set up your cloud TIC server. These settings apply to every
/// output in your job. The MediaConvert implementation is currently with the
/// following Nielsen versions: Nielsen Watermark SDK Version 5.2.1 Nielsen NLM
/// Watermark Engine Version 1.2.7 Nielsen Watermark Authenticator [SID_TIC]
/// Version [5.0.0]
class NielsenNonLinearWatermarkSettings {
  /// Choose the type of Nielsen watermarks that you want in your outputs. When
  /// you choose NAES 2 and NW, you must provide a value for the setting SID. When
  /// you choose CBET, you must provide a value for the setting CSID. When you
  /// choose NAES 2, NW, and CBET, you must provide values for both of these
  /// settings.
  final NielsenActiveWatermarkProcessType? activeWatermarkProcess;

  /// Optional. Use this setting when you want the service to include an ADI file
  /// in the Nielsen metadata .zip file. To provide an ADI file, store it in
  /// Amazon S3 and provide a URL to it here. The URL should be in the following
  /// format: S3://bucket/path/ADI-file. For more information about the metadata
  /// .zip file, see the setting Metadata destination.
  final String? adiFilename;

  /// Use the asset ID that you provide to Nielsen to uniquely identify this
  /// asset. Required for all Nielsen non-linear watermarking.
  final String? assetId;

  /// Use the asset name that you provide to Nielsen for this asset. Required for
  /// all Nielsen non-linear watermarking.
  final String? assetName;

  /// Use the CSID that Nielsen provides to you. This CBET source ID should be
  /// unique to your Nielsen account but common to all of your output assets that
  /// have CBET watermarking. Required when you choose a value for the setting
  /// Watermark types that includes CBET.
  final String? cbetSourceId;

  /// Optional. If this asset uses an episode ID with Nielsen, provide it here.
  final String? episodeId;

  /// Specify the Amazon S3 location where you want MediaConvert to save your
  /// Nielsen non-linear metadata .zip file. This Amazon S3 bucket must be in the
  /// same Region as the one where you do your MediaConvert transcoding. If you
  /// want to include an ADI file in this .zip file, use the setting ADI file to
  /// specify it. MediaConvert delivers the Nielsen metadata .zip files only to
  /// your metadata destination Amazon S3 bucket. It doesn't deliver the .zip
  /// files to Nielsen. You are responsible for delivering the metadata .zip files
  /// to Nielsen.
  final String? metadataDestination;

  /// Use the SID that Nielsen provides to you. This source ID should be unique to
  /// your Nielsen account but common to all of your output assets. Required for
  /// all Nielsen non-linear watermarking. This ID should be unique to your
  /// Nielsen account but common to all of your output assets. Required for all
  /// Nielsen non-linear watermarking.
  final int? sourceId;

  /// Required. Specify whether your source content already contains Nielsen
  /// non-linear watermarks. When you set this value to Watermarked, the service
  /// fails the job. Nielsen requires that you add non-linear watermarking to only
  /// clean content that doesn't already have non-linear Nielsen watermarks.
  final NielsenSourceWatermarkStatusType? sourceWatermarkStatus;

  /// Specify the endpoint for the TIC server that you have deployed and
  /// configured in the AWS Cloud. Required for all Nielsen non-linear
  /// watermarking. MediaConvert can't connect directly to a TIC server. Instead,
  /// you must use API Gateway to provide a RESTful interface between MediaConvert
  /// and a TIC server that you deploy in your AWS account. For more information
  /// on deploying a TIC server in your AWS account and the required API Gateway,
  /// contact Nielsen support.
  final String? ticServerUrl;

  /// To create assets that have the same TIC values in each audio track, keep the
  /// default value Share TICs. To create assets that have unique TIC values for
  /// each audio track, choose Use unique TICs.
  final NielsenUniqueTicPerAudioTrackType? uniqueTicPerAudioTrack;

  NielsenNonLinearWatermarkSettings({
    this.activeWatermarkProcess,
    this.adiFilename,
    this.assetId,
    this.assetName,
    this.cbetSourceId,
    this.episodeId,
    this.metadataDestination,
    this.sourceId,
    this.sourceWatermarkStatus,
    this.ticServerUrl,
    this.uniqueTicPerAudioTrack,
  });

  factory NielsenNonLinearWatermarkSettings.fromJson(
      Map<String, dynamic> json) {
    return NielsenNonLinearWatermarkSettings(
      activeWatermarkProcess: (json['activeWatermarkProcess'] as String?)
          ?.let(NielsenActiveWatermarkProcessType.fromString),
      adiFilename: json['adiFilename'] as String?,
      assetId: json['assetId'] as String?,
      assetName: json['assetName'] as String?,
      cbetSourceId: json['cbetSourceId'] as String?,
      episodeId: json['episodeId'] as String?,
      metadataDestination: json['metadataDestination'] as String?,
      sourceId: json['sourceId'] as int?,
      sourceWatermarkStatus: (json['sourceWatermarkStatus'] as String?)
          ?.let(NielsenSourceWatermarkStatusType.fromString),
      ticServerUrl: json['ticServerUrl'] as String?,
      uniqueTicPerAudioTrack: (json['uniqueTicPerAudioTrack'] as String?)
          ?.let(NielsenUniqueTicPerAudioTrackType.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final activeWatermarkProcess = this.activeWatermarkProcess;
    final adiFilename = this.adiFilename;
    final assetId = this.assetId;
    final assetName = this.assetName;
    final cbetSourceId = this.cbetSourceId;
    final episodeId = this.episodeId;
    final metadataDestination = this.metadataDestination;
    final sourceId = this.sourceId;
    final sourceWatermarkStatus = this.sourceWatermarkStatus;
    final ticServerUrl = this.ticServerUrl;
    final uniqueTicPerAudioTrack = this.uniqueTicPerAudioTrack;
    return {
      if (activeWatermarkProcess != null)
        'activeWatermarkProcess': activeWatermarkProcess.value,
      if (adiFilename != null) 'adiFilename': adiFilename,
      if (assetId != null) 'assetId': assetId,
      if (assetName != null) 'assetName': assetName,
      if (cbetSourceId != null) 'cbetSourceId': cbetSourceId,
      if (episodeId != null) 'episodeId': episodeId,
      if (metadataDestination != null)
        'metadataDestination': metadataDestination,
      if (sourceId != null) 'sourceId': sourceId,
      if (sourceWatermarkStatus != null)
        'sourceWatermarkStatus': sourceWatermarkStatus.value,
      if (ticServerUrl != null) 'ticServerUrl': ticServerUrl,
      if (uniqueTicPerAudioTrack != null)
        'uniqueTicPerAudioTrack': uniqueTicPerAudioTrack.value,
    };
  }
}

/// Required. Specify whether your source content already contains Nielsen
/// non-linear watermarks. When you set this value to Watermarked, the service
/// fails the job. Nielsen requires that you add non-linear watermarking to only
/// clean content that doesn't already have non-linear Nielsen watermarks.
enum NielsenSourceWatermarkStatusType {
  clean('CLEAN'),
  watermarked('WATERMARKED'),
  ;

  final String value;

  const NielsenSourceWatermarkStatusType(this.value);

  static NielsenSourceWatermarkStatusType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum NielsenSourceWatermarkStatusType'));
}

/// To create assets that have the same TIC values in each audio track, keep the
/// default value Share TICs. To create assets that have unique TIC values for
/// each audio track, choose Use unique TICs.
enum NielsenUniqueTicPerAudioTrackType {
  reserveUniqueTicsPerTrack('RESERVE_UNIQUE_TICS_PER_TRACK'),
  sameTicsPerTrack('SAME_TICS_PER_TRACK'),
  ;

  final String value;

  const NielsenUniqueTicPerAudioTrackType(this.value);

  static NielsenUniqueTicPerAudioTrackType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum NielsenUniqueTicPerAudioTrackType'));
}

/// When you set Noise reducer to Temporal, the bandwidth and sharpness of your
/// output is reduced. You can optionally use Post temporal sharpening to apply
/// sharpening to the edges of your output. Note that Post temporal sharpening
/// will also make the bandwidth reduction from the Noise reducer smaller. The
/// default behavior, Auto, allows the transcoder to determine whether to apply
/// sharpening, depending on your input type and quality. When you set Post
/// temporal sharpening to Enabled, specify how much sharpening is applied using
/// Post temporal sharpening strength. Set Post temporal sharpening to Disabled
/// to not apply sharpening.
enum NoiseFilterPostTemporalSharpening {
  disabled('DISABLED'),
  enabled('ENABLED'),
  auto('AUTO'),
  ;

  final String value;

  const NoiseFilterPostTemporalSharpening(this.value);

  static NoiseFilterPostTemporalSharpening fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum NoiseFilterPostTemporalSharpening'));
}

/// Use Post temporal sharpening strength to define the amount of sharpening the
/// transcoder applies to your output. Set Post temporal sharpening strength to
/// Low, Medium, or High to indicate the amount of sharpening.
enum NoiseFilterPostTemporalSharpeningStrength {
  low('LOW'),
  medium('MEDIUM'),
  high('HIGH'),
  ;

  final String value;

  const NoiseFilterPostTemporalSharpeningStrength(this.value);

  static NoiseFilterPostTemporalSharpeningStrength fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum NoiseFilterPostTemporalSharpeningStrength'));
}

/// Enable the Noise reducer feature to remove noise from your video output if
/// necessary. Enable or disable this feature for each output individually. This
/// setting is disabled by default. When you enable Noise reducer, you must also
/// select a value for Noise reducer filter. For AVC outputs, when you include
/// Noise reducer, you cannot include the Bandwidth reduction filter.
class NoiseReducer {
  /// Use Noise reducer filter to select one of the following spatial image
  /// filtering functions. To use this setting, you must also enable Noise
  /// reducer. * Bilateral preserves edges while reducing noise. * Mean (softest),
  /// Gaussian, Lanczos, and Sharpen (sharpest) do convolution filtering. *
  /// Conserve does min/max noise reduction. * Spatial does frequency-domain
  /// filtering based on JND principles. * Temporal optimizes video quality for
  /// complex motion.
  final NoiseReducerFilter? filter;

  /// Settings for a noise reducer filter
  final NoiseReducerFilterSettings? filterSettings;

  /// Noise reducer filter settings for spatial filter.
  final NoiseReducerSpatialFilterSettings? spatialFilterSettings;

  /// Noise reducer filter settings for temporal filter.
  final NoiseReducerTemporalFilterSettings? temporalFilterSettings;

  NoiseReducer({
    this.filter,
    this.filterSettings,
    this.spatialFilterSettings,
    this.temporalFilterSettings,
  });

  factory NoiseReducer.fromJson(Map<String, dynamic> json) {
    return NoiseReducer(
      filter: (json['filter'] as String?)?.let(NoiseReducerFilter.fromString),
      filterSettings: json['filterSettings'] != null
          ? NoiseReducerFilterSettings.fromJson(
              json['filterSettings'] as Map<String, dynamic>)
          : null,
      spatialFilterSettings: json['spatialFilterSettings'] != null
          ? NoiseReducerSpatialFilterSettings.fromJson(
              json['spatialFilterSettings'] as Map<String, dynamic>)
          : null,
      temporalFilterSettings: json['temporalFilterSettings'] != null
          ? NoiseReducerTemporalFilterSettings.fromJson(
              json['temporalFilterSettings'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final filter = this.filter;
    final filterSettings = this.filterSettings;
    final spatialFilterSettings = this.spatialFilterSettings;
    final temporalFilterSettings = this.temporalFilterSettings;
    return {
      if (filter != null) 'filter': filter.value,
      if (filterSettings != null) 'filterSettings': filterSettings,
      if (spatialFilterSettings != null)
        'spatialFilterSettings': spatialFilterSettings,
      if (temporalFilterSettings != null)
        'temporalFilterSettings': temporalFilterSettings,
    };
  }
}

/// Use Noise reducer filter to select one of the following spatial image
/// filtering functions. To use this setting, you must also enable Noise
/// reducer. * Bilateral preserves edges while reducing noise. * Mean (softest),
/// Gaussian, Lanczos, and Sharpen (sharpest) do convolution filtering. *
/// Conserve does min/max noise reduction. * Spatial does frequency-domain
/// filtering based on JND principles. * Temporal optimizes video quality for
/// complex motion.
enum NoiseReducerFilter {
  bilateral('BILATERAL'),
  mean('MEAN'),
  gaussian('GAUSSIAN'),
  lanczos('LANCZOS'),
  sharpen('SHARPEN'),
  conserve('CONSERVE'),
  spatial('SPATIAL'),
  temporal('TEMPORAL'),
  ;

  final String value;

  const NoiseReducerFilter(this.value);

  static NoiseReducerFilter fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum NoiseReducerFilter'));
}

/// Settings for a noise reducer filter
class NoiseReducerFilterSettings {
  /// Relative strength of noise reducing filter. Higher values produce stronger
  /// filtering.
  final int? strength;

  NoiseReducerFilterSettings({
    this.strength,
  });

  factory NoiseReducerFilterSettings.fromJson(Map<String, dynamic> json) {
    return NoiseReducerFilterSettings(
      strength: json['strength'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final strength = this.strength;
    return {
      if (strength != null) 'strength': strength,
    };
  }
}

/// Noise reducer filter settings for spatial filter.
class NoiseReducerSpatialFilterSettings {
  /// Specify strength of post noise reduction sharpening filter, with 0 disabling
  /// the filter and 3 enabling it at maximum strength.
  final int? postFilterSharpenStrength;

  /// The speed of the filter, from -2 (lower speed) to 3 (higher speed), with 0
  /// being the nominal value.
  final int? speed;

  /// Relative strength of noise reducing filter. Higher values produce stronger
  /// filtering.
  final int? strength;

  NoiseReducerSpatialFilterSettings({
    this.postFilterSharpenStrength,
    this.speed,
    this.strength,
  });

  factory NoiseReducerSpatialFilterSettings.fromJson(
      Map<String, dynamic> json) {
    return NoiseReducerSpatialFilterSettings(
      postFilterSharpenStrength: json['postFilterSharpenStrength'] as int?,
      speed: json['speed'] as int?,
      strength: json['strength'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final postFilterSharpenStrength = this.postFilterSharpenStrength;
    final speed = this.speed;
    final strength = this.strength;
    return {
      if (postFilterSharpenStrength != null)
        'postFilterSharpenStrength': postFilterSharpenStrength,
      if (speed != null) 'speed': speed,
      if (strength != null) 'strength': strength,
    };
  }
}

/// Noise reducer filter settings for temporal filter.
class NoiseReducerTemporalFilterSettings {
  /// Use Aggressive mode for content that has complex motion. Higher values
  /// produce stronger temporal filtering. This filters highly complex scenes more
  /// aggressively and creates better VQ for low bitrate outputs.
  final int? aggressiveMode;

  /// When you set Noise reducer to Temporal, the bandwidth and sharpness of your
  /// output is reduced. You can optionally use Post temporal sharpening to apply
  /// sharpening to the edges of your output. Note that Post temporal sharpening
  /// will also make the bandwidth reduction from the Noise reducer smaller. The
  /// default behavior, Auto, allows the transcoder to determine whether to apply
  /// sharpening, depending on your input type and quality. When you set Post
  /// temporal sharpening to Enabled, specify how much sharpening is applied using
  /// Post temporal sharpening strength. Set Post temporal sharpening to Disabled
  /// to not apply sharpening.
  final NoiseFilterPostTemporalSharpening? postTemporalSharpening;

  /// Use Post temporal sharpening strength to define the amount of sharpening the
  /// transcoder applies to your output. Set Post temporal sharpening strength to
  /// Low, Medium, or High to indicate the amount of sharpening.
  final NoiseFilterPostTemporalSharpeningStrength?
      postTemporalSharpeningStrength;

  /// The speed of the filter (higher number is faster). Low setting reduces bit
  /// rate at the cost of transcode time, high setting improves transcode time at
  /// the cost of bit rate.
  final int? speed;

  /// Specify the strength of the noise reducing filter on this output. Higher
  /// values produce stronger filtering. We recommend the following value ranges,
  /// depending on the result that you want: * 0-2 for complexity reduction with
  /// minimal sharpness loss * 2-8 for complexity reduction with image
  /// preservation * 8-16 for a high level of complexity reduction
  final int? strength;

  NoiseReducerTemporalFilterSettings({
    this.aggressiveMode,
    this.postTemporalSharpening,
    this.postTemporalSharpeningStrength,
    this.speed,
    this.strength,
  });

  factory NoiseReducerTemporalFilterSettings.fromJson(
      Map<String, dynamic> json) {
    return NoiseReducerTemporalFilterSettings(
      aggressiveMode: json['aggressiveMode'] as int?,
      postTemporalSharpening: (json['postTemporalSharpening'] as String?)
          ?.let(NoiseFilterPostTemporalSharpening.fromString),
      postTemporalSharpeningStrength:
          (json['postTemporalSharpeningStrength'] as String?)
              ?.let(NoiseFilterPostTemporalSharpeningStrength.fromString),
      speed: json['speed'] as int?,
      strength: json['strength'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final aggressiveMode = this.aggressiveMode;
    final postTemporalSharpening = this.postTemporalSharpening;
    final postTemporalSharpeningStrength = this.postTemporalSharpeningStrength;
    final speed = this.speed;
    final strength = this.strength;
    return {
      if (aggressiveMode != null) 'aggressiveMode': aggressiveMode,
      if (postTemporalSharpening != null)
        'postTemporalSharpening': postTemporalSharpening.value,
      if (postTemporalSharpeningStrength != null)
        'postTemporalSharpeningStrength': postTemporalSharpeningStrength.value,
      if (speed != null) 'speed': speed,
      if (strength != null) 'strength': strength,
    };
  }
}

/// Required when you set Codec, under AudioDescriptions>CodecSettings, to the
/// value OPUS.
class OpusSettings {
  /// Optional. Specify the average bitrate in bits per second. Valid values are
  /// multiples of 8000, from 32000 through 192000. The default value is 96000,
  /// which we recommend for quality and bandwidth.
  final int? bitrate;

  /// Specify the number of channels in this output audio track. Choosing Mono on
  /// gives you 1 output channel; choosing Stereo gives you 2. In the API, valid
  /// values are 1 and 2.
  final int? channels;

  /// Optional. Sample rate in Hz. Valid values are 16000, 24000, and 48000. The
  /// default value is 48000.
  final int? sampleRate;

  OpusSettings({
    this.bitrate,
    this.channels,
    this.sampleRate,
  });

  factory OpusSettings.fromJson(Map<String, dynamic> json) {
    return OpusSettings(
      bitrate: json['bitrate'] as int?,
      channels: json['channels'] as int?,
      sampleRate: json['sampleRate'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final bitrate = this.bitrate;
    final channels = this.channels;
    final sampleRate = this.sampleRate;
    return {
      if (bitrate != null) 'bitrate': bitrate,
      if (channels != null) 'channels': channels,
      if (sampleRate != null) 'sampleRate': sampleRate,
    };
  }
}

/// Optional. When you request lists of resources, you can specify whether they
/// are sorted in ASCENDING or DESCENDING order. Default varies by resource.
enum Order {
  ascending('ASCENDING'),
  descending('DESCENDING'),
  ;

  final String value;

  const Order(this.value);

  static Order fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception('$value is not known in enum Order'));
}

/// Each output in your job is a collection of settings that describes how you
/// want MediaConvert to encode a single output file or stream. For more
/// information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/create-outputs.html.
class Output {
  /// Contains groups of audio encoding settings organized by audio codec. Include
  /// one instance of per output. Can contain multiple groups of encoding
  /// settings.
  final List<AudioDescription>? audioDescriptions;

  /// Contains groups of captions settings. For each output that has captions,
  /// include one instance of CaptionDescriptions. Can contain multiple groups of
  /// captions settings.
  final List<CaptionDescription>? captionDescriptions;

  /// Container specific settings.
  final ContainerSettings? containerSettings;

  /// Use Extension to specify the file extension for outputs in File output
  /// groups. If you do not specify a value, the service will use default
  /// extensions by container type as follows * MPEG-2 transport stream, m2ts *
  /// Quicktime, mov * MXF container, mxf * MPEG-4 container, mp4 * WebM
  /// container, webm * No Container, the service will use codec extensions (e.g.
  /// AAC, H265, H265, AC3)
  final String? extension;

  /// Use Name modifier to have the service add a string to the end of each output
  /// filename. You specify the base filename as part of your destination URI.
  /// When you create multiple outputs in the same output group, Name modifier is
  /// required. Name modifier also accepts format identifiers. For DASH ISO
  /// outputs, if you use the format identifiers $Number$ or $Time$ in one output,
  /// you must use them in the same way in all outputs of the output group.
  final String? nameModifier;

  /// Specific settings for this type of output.
  final OutputSettings? outputSettings;

  /// Use Preset to specify a preset for your transcoding settings. Provide the
  /// system or custom preset name. You can specify either Preset or Container
  /// settings, but not both.
  final String? preset;

  /// VideoDescription contains a group of video encoding settings. The specific
  /// video settings depend on the video codec that you choose for the property
  /// codec. Include one instance of VideoDescription per output.
  final VideoDescription? videoDescription;

  Output({
    this.audioDescriptions,
    this.captionDescriptions,
    this.containerSettings,
    this.extension,
    this.nameModifier,
    this.outputSettings,
    this.preset,
    this.videoDescription,
  });

  factory Output.fromJson(Map<String, dynamic> json) {
    return Output(
      audioDescriptions: (json['audioDescriptions'] as List?)
          ?.nonNulls
          .map((e) => AudioDescription.fromJson(e as Map<String, dynamic>))
          .toList(),
      captionDescriptions: (json['captionDescriptions'] as List?)
          ?.nonNulls
          .map((e) => CaptionDescription.fromJson(e as Map<String, dynamic>))
          .toList(),
      containerSettings: json['containerSettings'] != null
          ? ContainerSettings.fromJson(
              json['containerSettings'] as Map<String, dynamic>)
          : null,
      extension: json['extension'] as String?,
      nameModifier: json['nameModifier'] as String?,
      outputSettings: json['outputSettings'] != null
          ? OutputSettings.fromJson(
              json['outputSettings'] as Map<String, dynamic>)
          : null,
      preset: json['preset'] as String?,
      videoDescription: json['videoDescription'] != null
          ? VideoDescription.fromJson(
              json['videoDescription'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final audioDescriptions = this.audioDescriptions;
    final captionDescriptions = this.captionDescriptions;
    final containerSettings = this.containerSettings;
    final extension = this.extension;
    final nameModifier = this.nameModifier;
    final outputSettings = this.outputSettings;
    final preset = this.preset;
    final videoDescription = this.videoDescription;
    return {
      if (audioDescriptions != null) 'audioDescriptions': audioDescriptions,
      if (captionDescriptions != null)
        'captionDescriptions': captionDescriptions,
      if (containerSettings != null) 'containerSettings': containerSettings,
      if (extension != null) 'extension': extension,
      if (nameModifier != null) 'nameModifier': nameModifier,
      if (outputSettings != null) 'outputSettings': outputSettings,
      if (preset != null) 'preset': preset,
      if (videoDescription != null) 'videoDescription': videoDescription,
    };
  }
}

/// OutputChannel mapping settings.
class OutputChannelMapping {
  /// Use this setting to specify your remix values when they are integers, such
  /// as -10, 0, or 4.
  final List<int>? inputChannels;

  /// Use this setting to specify your remix values when they have a decimal
  /// component, such as -10.312, 0.08, or 4.9. MediaConvert rounds your remixing
  /// values to the nearest thousandth.
  final List<double>? inputChannelsFineTune;

  OutputChannelMapping({
    this.inputChannels,
    this.inputChannelsFineTune,
  });

  factory OutputChannelMapping.fromJson(Map<String, dynamic> json) {
    return OutputChannelMapping(
      inputChannels: (json['inputChannels'] as List?)
          ?.nonNulls
          .map((e) => e as int)
          .toList(),
      inputChannelsFineTune: (json['inputChannelsFineTune'] as List?)
          ?.nonNulls
          .map((e) => e as double)
          .toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final inputChannels = this.inputChannels;
    final inputChannelsFineTune = this.inputChannelsFineTune;
    return {
      if (inputChannels != null) 'inputChannels': inputChannels,
      if (inputChannelsFineTune != null)
        'inputChannelsFineTune': inputChannelsFineTune,
    };
  }
}

/// Details regarding output
class OutputDetail {
  /// Duration in milliseconds
  final int? durationInMs;

  /// Contains details about the output's video stream
  final VideoDetail? videoDetails;

  OutputDetail({
    this.durationInMs,
    this.videoDetails,
  });

  factory OutputDetail.fromJson(Map<String, dynamic> json) {
    return OutputDetail(
      durationInMs: json['durationInMs'] as int?,
      videoDetails: json['videoDetails'] != null
          ? VideoDetail.fromJson(json['videoDetails'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final durationInMs = this.durationInMs;
    final videoDetails = this.videoDetails;
    return {
      if (durationInMs != null) 'durationInMs': durationInMs,
      if (videoDetails != null) 'videoDetails': videoDetails,
    };
  }
}

/// Group of outputs
class OutputGroup {
  /// Use automated encoding to have MediaConvert choose your encoding settings
  /// for you, based on characteristics of your input video.
  final AutomatedEncodingSettings? automatedEncodingSettings;

  /// Use Custom Group Name to specify a name for the output group. This value is
  /// displayed on the console and can make your job settings JSON more
  /// human-readable. It does not affect your outputs. Use up to twelve characters
  /// that are either letters, numbers, spaces, or underscores.
  final String? customName;

  /// Name of the output group
  final String? name;

  /// Output Group settings, including type
  final OutputGroupSettings? outputGroupSettings;

  /// This object holds groups of encoding settings, one group of settings per
  /// output.
  final List<Output>? outputs;

  OutputGroup({
    this.automatedEncodingSettings,
    this.customName,
    this.name,
    this.outputGroupSettings,
    this.outputs,
  });

  factory OutputGroup.fromJson(Map<String, dynamic> json) {
    return OutputGroup(
      automatedEncodingSettings: json['automatedEncodingSettings'] != null
          ? AutomatedEncodingSettings.fromJson(
              json['automatedEncodingSettings'] as Map<String, dynamic>)
          : null,
      customName: json['customName'] as String?,
      name: json['name'] as String?,
      outputGroupSettings: json['outputGroupSettings'] != null
          ? OutputGroupSettings.fromJson(
              json['outputGroupSettings'] as Map<String, dynamic>)
          : null,
      outputs: (json['outputs'] as List?)
          ?.nonNulls
          .map((e) => Output.fromJson(e as Map<String, dynamic>))
          .toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final automatedEncodingSettings = this.automatedEncodingSettings;
    final customName = this.customName;
    final name = this.name;
    final outputGroupSettings = this.outputGroupSettings;
    final outputs = this.outputs;
    return {
      if (automatedEncodingSettings != null)
        'automatedEncodingSettings': automatedEncodingSettings,
      if (customName != null) 'customName': customName,
      if (name != null) 'name': name,
      if (outputGroupSettings != null)
        'outputGroupSettings': outputGroupSettings,
      if (outputs != null) 'outputs': outputs,
    };
  }
}

/// Contains details about the output groups specified in the job settings.
class OutputGroupDetail {
  /// Details about the output
  final List<OutputDetail>? outputDetails;

  OutputGroupDetail({
    this.outputDetails,
  });

  factory OutputGroupDetail.fromJson(Map<String, dynamic> json) {
    return OutputGroupDetail(
      outputDetails: (json['outputDetails'] as List?)
          ?.nonNulls
          .map((e) => OutputDetail.fromJson(e as Map<String, dynamic>))
          .toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final outputDetails = this.outputDetails;
    return {
      if (outputDetails != null) 'outputDetails': outputDetails,
    };
  }
}

/// Output Group settings, including type
class OutputGroupSettings {
  /// Settings related to your CMAF output package. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/outputs-file-ABR.html.
  final CmafGroupSettings? cmafGroupSettings;

  /// Settings related to your DASH output package. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/outputs-file-ABR.html.
  final DashIsoGroupSettings? dashIsoGroupSettings;

  /// Settings related to your File output group. MediaConvert uses this group of
  /// settings to generate a single standalone file, rather than a streaming
  /// package.
  final FileGroupSettings? fileGroupSettings;

  /// Settings related to your HLS output package. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/outputs-file-ABR.html.
  final HlsGroupSettings? hlsGroupSettings;

  /// Settings related to your Microsoft Smooth Streaming output package. For more
  /// information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/outputs-file-ABR.html.
  final MsSmoothGroupSettings? msSmoothGroupSettings;

  /// Type of output group (File group, Apple HLS, DASH ISO, Microsoft Smooth
  /// Streaming, CMAF)
  final OutputGroupType? type;

  OutputGroupSettings({
    this.cmafGroupSettings,
    this.dashIsoGroupSettings,
    this.fileGroupSettings,
    this.hlsGroupSettings,
    this.msSmoothGroupSettings,
    this.type,
  });

  factory OutputGroupSettings.fromJson(Map<String, dynamic> json) {
    return OutputGroupSettings(
      cmafGroupSettings: json['cmafGroupSettings'] != null
          ? CmafGroupSettings.fromJson(
              json['cmafGroupSettings'] as Map<String, dynamic>)
          : null,
      dashIsoGroupSettings: json['dashIsoGroupSettings'] != null
          ? DashIsoGroupSettings.fromJson(
              json['dashIsoGroupSettings'] as Map<String, dynamic>)
          : null,
      fileGroupSettings: json['fileGroupSettings'] != null
          ? FileGroupSettings.fromJson(
              json['fileGroupSettings'] as Map<String, dynamic>)
          : null,
      hlsGroupSettings: json['hlsGroupSettings'] != null
          ? HlsGroupSettings.fromJson(
              json['hlsGroupSettings'] as Map<String, dynamic>)
          : null,
      msSmoothGroupSettings: json['msSmoothGroupSettings'] != null
          ? MsSmoothGroupSettings.fromJson(
              json['msSmoothGroupSettings'] as Map<String, dynamic>)
          : null,
      type: (json['type'] as String?)?.let(OutputGroupType.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final cmafGroupSettings = this.cmafGroupSettings;
    final dashIsoGroupSettings = this.dashIsoGroupSettings;
    final fileGroupSettings = this.fileGroupSettings;
    final hlsGroupSettings = this.hlsGroupSettings;
    final msSmoothGroupSettings = this.msSmoothGroupSettings;
    final type = this.type;
    return {
      if (cmafGroupSettings != null) 'cmafGroupSettings': cmafGroupSettings,
      if (dashIsoGroupSettings != null)
        'dashIsoGroupSettings': dashIsoGroupSettings,
      if (fileGroupSettings != null) 'fileGroupSettings': fileGroupSettings,
      if (hlsGroupSettings != null) 'hlsGroupSettings': hlsGroupSettings,
      if (msSmoothGroupSettings != null)
        'msSmoothGroupSettings': msSmoothGroupSettings,
      if (type != null) 'type': type.value,
    };
  }
}

/// Type of output group (File group, Apple HLS, DASH ISO, Microsoft Smooth
/// Streaming, CMAF)
enum OutputGroupType {
  hlsGroupSettings('HLS_GROUP_SETTINGS'),
  dashIsoGroupSettings('DASH_ISO_GROUP_SETTINGS'),
  fileGroupSettings('FILE_GROUP_SETTINGS'),
  msSmoothGroupSettings('MS_SMOOTH_GROUP_SETTINGS'),
  cmafGroupSettings('CMAF_GROUP_SETTINGS'),
  ;

  final String value;

  const OutputGroupType(this.value);

  static OutputGroupType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum OutputGroupType'));
}

/// Selects method of inserting SDT information into output stream. "Follow
/// input SDT" copies SDT information from input stream to output stream.
/// "Follow input SDT if present" copies SDT information from input stream to
/// output stream if SDT information is present in the input, otherwise it will
/// fall back on the user-defined values. Enter "SDT Manually" means user will
/// enter the SDT information. "No SDT" means output stream will not contain SDT
/// information.
enum OutputSdt {
  sdtFollow('SDT_FOLLOW'),
  sdtFollowIfPresent('SDT_FOLLOW_IF_PRESENT'),
  sdtManual('SDT_MANUAL'),
  sdtNone('SDT_NONE'),
  ;

  final String value;

  const OutputSdt(this.value);

  static OutputSdt fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum OutputSdt'));
}

/// Specific settings for this type of output.
class OutputSettings {
  /// Settings for HLS output groups
  final HlsSettings? hlsSettings;

  OutputSettings({
    this.hlsSettings,
  });

  factory OutputSettings.fromJson(Map<String, dynamic> json) {
    return OutputSettings(
      hlsSettings: json['hlsSettings'] != null
          ? HlsSettings.fromJson(json['hlsSettings'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final hlsSettings = this.hlsSettings;
    return {
      if (hlsSettings != null) 'hlsSettings': hlsSettings,
    };
  }
}

/// Use this setting if your input has video and audio durations that don't
/// align, and your output or player has strict alignment requirements.
/// Examples: Input audio track has a delayed start. Input video track ends
/// before audio ends. When you set Pad video to Black, MediaConvert generates
/// black video frames so that output video and audio durations match. Black
/// video frames are added at the beginning or end, depending on your input. To
/// keep the default behavior and not generate black video, set Pad video to
/// Disabled or leave blank.
enum PadVideo {
  disabled('DISABLED'),
  black('BLACK'),
  ;

  final String value;

  const PadVideo(this.value);

  static PadVideo fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum PadVideo'));
}

/// If you work with a third party video watermarking partner, use the group of
/// settings that correspond with your watermarking partner to include
/// watermarks in your output.
class PartnerWatermarking {
  /// For forensic video watermarking, MediaConvert supports Nagra NexGuard File
  /// Marker watermarking. MediaConvert supports both PreRelease Content (NGPR/G2)
  /// and OTT Streaming workflows.
  final NexGuardFileMarkerSettings? nexguardFileMarkerSettings;

  PartnerWatermarking({
    this.nexguardFileMarkerSettings,
  });

  factory PartnerWatermarking.fromJson(Map<String, dynamic> json) {
    return PartnerWatermarking(
      nexguardFileMarkerSettings: json['nexguardFileMarkerSettings'] != null
          ? NexGuardFileMarkerSettings.fromJson(
              json['nexguardFileMarkerSettings'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final nexguardFileMarkerSettings = this.nexguardFileMarkerSettings;
    return {
      if (nexguardFileMarkerSettings != null)
        'nexguardFileMarkerSettings': nexguardFileMarkerSettings,
    };
  }
}

/// A policy configures behavior that you allow or disallow for your account.
/// For information about MediaConvert policies, see the user guide at
/// http://docs.aws.amazon.com/mediaconvert/latest/ug/what-is.html
class Policy {
  /// Allow or disallow jobs that specify HTTP inputs.
  final InputPolicy? httpInputs;

  /// Allow or disallow jobs that specify HTTPS inputs.
  final InputPolicy? httpsInputs;

  /// Allow or disallow jobs that specify Amazon S3 inputs.
  final InputPolicy? s3Inputs;

  Policy({
    this.httpInputs,
    this.httpsInputs,
    this.s3Inputs,
  });

  factory Policy.fromJson(Map<String, dynamic> json) {
    return Policy(
      httpInputs: (json['httpInputs'] as String?)?.let(InputPolicy.fromString),
      httpsInputs:
          (json['httpsInputs'] as String?)?.let(InputPolicy.fromString),
      s3Inputs: (json['s3Inputs'] as String?)?.let(InputPolicy.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final httpInputs = this.httpInputs;
    final httpsInputs = this.httpsInputs;
    final s3Inputs = this.s3Inputs;
    return {
      if (httpInputs != null) 'httpInputs': httpInputs.value,
      if (httpsInputs != null) 'httpsInputs': httpsInputs.value,
      if (s3Inputs != null) 's3Inputs': s3Inputs.value,
    };
  }
}

/// A preset is a collection of preconfigured media conversion settings that you
/// want MediaConvert to apply to the output during the conversion process.
class Preset {
  /// A name you create for each preset. Each name must be unique within your
  /// account.
  final String name;

  /// Settings for preset
  final PresetSettings settings;

  /// An identifier for this resource that is unique within all of AWS.
  final String? arn;

  /// An optional category you create to organize your presets.
  final String? category;

  /// The timestamp in epoch seconds for preset creation.
  final DateTime? createdAt;

  /// An optional description you create for each preset.
  final String? description;

  /// The timestamp in epoch seconds when the preset was last updated.
  final DateTime? lastUpdated;

  /// A preset can be of two types: system or custom. System or built-in preset
  /// can't be modified or deleted by the user.
  final Type? type;

  Preset({
    required this.name,
    required this.settings,
    this.arn,
    this.category,
    this.createdAt,
    this.description,
    this.lastUpdated,
    this.type,
  });

  factory Preset.fromJson(Map<String, dynamic> json) {
    return Preset(
      name: json['name'] as String,
      settings:
          PresetSettings.fromJson(json['settings'] as Map<String, dynamic>),
      arn: json['arn'] as String?,
      category: json['category'] as String?,
      createdAt: timeStampFromJson(json['createdAt']),
      description: json['description'] as String?,
      lastUpdated: timeStampFromJson(json['lastUpdated']),
      type: (json['type'] as String?)?.let(Type.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final name = this.name;
    final settings = this.settings;
    final arn = this.arn;
    final category = this.category;
    final createdAt = this.createdAt;
    final description = this.description;
    final lastUpdated = this.lastUpdated;
    final type = this.type;
    return {
      'name': name,
      'settings': settings,
      if (arn != null) 'arn': arn,
      if (category != null) 'category': category,
      if (createdAt != null) 'createdAt': unixTimestampToJson(createdAt),
      if (description != null) 'description': description,
      if (lastUpdated != null) 'lastUpdated': unixTimestampToJson(lastUpdated),
      if (type != null) 'type': type.value,
    };
  }
}

/// Optional. When you request a list of presets, you can choose to list them
/// alphabetically by NAME or chronologically by CREATION_DATE. If you don't
/// specify, the service will list them by name.
enum PresetListBy {
  name('NAME'),
  creationDate('CREATION_DATE'),
  system('SYSTEM'),
  ;

  final String value;

  const PresetListBy(this.value);

  static PresetListBy fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum PresetListBy'));
}

/// Settings for preset
class PresetSettings {
  /// Contains groups of audio encoding settings organized by audio codec. Include
  /// one instance of per output. Can contain multiple groups of encoding
  /// settings.
  final List<AudioDescription>? audioDescriptions;

  /// This object holds groups of settings related to captions for one output. For
  /// each output that has captions, include one instance of CaptionDescriptions.
  final List<CaptionDescriptionPreset>? captionDescriptions;

  /// Container specific settings.
  final ContainerSettings? containerSettings;

  /// VideoDescription contains a group of video encoding settings. The specific
  /// video settings depend on the video codec that you choose for the property
  /// codec. Include one instance of VideoDescription per output.
  final VideoDescription? videoDescription;

  PresetSettings({
    this.audioDescriptions,
    this.captionDescriptions,
    this.containerSettings,
    this.videoDescription,
  });

  factory PresetSettings.fromJson(Map<String, dynamic> json) {
    return PresetSettings(
      audioDescriptions: (json['audioDescriptions'] as List?)
          ?.nonNulls
          .map((e) => AudioDescription.fromJson(e as Map<String, dynamic>))
          .toList(),
      captionDescriptions: (json['captionDescriptions'] as List?)
          ?.nonNulls
          .map((e) =>
              CaptionDescriptionPreset.fromJson(e as Map<String, dynamic>))
          .toList(),
      containerSettings: json['containerSettings'] != null
          ? ContainerSettings.fromJson(
              json['containerSettings'] as Map<String, dynamic>)
          : null,
      videoDescription: json['videoDescription'] != null
          ? VideoDescription.fromJson(
              json['videoDescription'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final audioDescriptions = this.audioDescriptions;
    final captionDescriptions = this.captionDescriptions;
    final containerSettings = this.containerSettings;
    final videoDescription = this.videoDescription;
    return {
      if (audioDescriptions != null) 'audioDescriptions': audioDescriptions,
      if (captionDescriptions != null)
        'captionDescriptions': captionDescriptions,
      if (containerSettings != null) 'containerSettings': containerSettings,
      if (videoDescription != null) 'videoDescription': videoDescription,
    };
  }
}

/// Specifies whether the pricing plan for the queue is on-demand or reserved.
/// For on-demand, you pay per minute, billed in increments of .01 minute. For
/// reserved, you pay for the transcoding capacity of the entire queue,
/// regardless of how much or how little you use it. Reserved pricing requires a
/// 12-month commitment.
enum PricingPlan {
  onDemand('ON_DEMAND'),
  reserved('RESERVED'),
  ;

  final String value;

  const PricingPlan(this.value);

  static PricingPlan fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum PricingPlan'));
}

/// This setting applies only to ProRes 4444 and ProRes 4444 XQ outputs that you
/// create from inputs that use 4:4:4 chroma sampling. Set Preserve 4:4:4
/// sampling to allow outputs to also use 4:4:4 chroma sampling. You must
/// specify a value for this setting when your output codec profile supports
/// 4:4:4 chroma sampling. Related Settings: For Apple ProRes outputs with 4:4:4
/// chroma sampling: Choose Preserve 4:4:4 sampling. Use when your input has
/// 4:4:4 chroma sampling and your output codec Profile is Apple ProRes 4444 or
/// 4444 XQ. Note that when you choose Preserve 4:4:4 sampling, you cannot
/// include any of the following Preprocessors: Dolby Vision, HDR10+, or Noise
/// reducer.
enum ProresChromaSampling {
  preserve_444Sampling('PRESERVE_444_SAMPLING'),
  subsampleTo_422('SUBSAMPLE_TO_422'),
  ;

  final String value;

  const ProresChromaSampling(this.value);

  static ProresChromaSampling fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum ProresChromaSampling'));
}

/// Use Profile to specify the type of Apple ProRes codec to use for this
/// output.
enum ProresCodecProfile {
  appleProres_422('APPLE_PRORES_422'),
  appleProres_422Hq('APPLE_PRORES_422_HQ'),
  appleProres_422Lt('APPLE_PRORES_422_LT'),
  appleProres_422Proxy('APPLE_PRORES_422_PROXY'),
  appleProres_4444('APPLE_PRORES_4444'),
  appleProres_4444Xq('APPLE_PRORES_4444_XQ'),
  ;

  final String value;

  const ProresCodecProfile(this.value);

  static ProresCodecProfile fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum ProresCodecProfile'));
}

/// If you are using the console, use the Framerate setting to specify the frame
/// rate for this output. If you want to keep the same frame rate as the input
/// video, choose Follow source. If you want to do frame rate conversion, choose
/// a frame rate from the dropdown list or choose Custom. The framerates shown
/// in the dropdown list are decimal approximations of fractions. If you choose
/// Custom, specify your frame rate as a fraction.
enum ProresFramerateControl {
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  specified('SPECIFIED'),
  ;

  final String value;

  const ProresFramerateControl(this.value);

  static ProresFramerateControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum ProresFramerateControl'));
}

/// Choose the method that you want MediaConvert to use when increasing or
/// decreasing the frame rate. For numerically simple conversions, such as 60
/// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
/// For numerically complex conversions, to avoid stutter: Choose Interpolate.
/// This results in a smooth picture, but might introduce undesirable video
/// artifacts. For complex frame rate conversions, especially if your source
/// video has already been converted from its original cadence: Choose
/// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
/// best conversion method frame by frame. Note that using FrameFormer increases
/// the transcoding time and incurs a significant add-on cost. When you choose
/// FrameFormer, your input video resolution must be at least 128x96.
enum ProresFramerateConversionAlgorithm {
  duplicateDrop('DUPLICATE_DROP'),
  interpolate('INTERPOLATE'),
  frameformer('FRAMEFORMER'),
  ;

  final String value;

  const ProresFramerateConversionAlgorithm(this.value);

  static ProresFramerateConversionAlgorithm fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum ProresFramerateConversionAlgorithm'));
}

/// Choose the scan line type for the output. Keep the default value,
/// Progressive to create a progressive output, regardless of the scan type of
/// your input. Use Top field first or Bottom field first to create an output
/// that's interlaced with the same field polarity throughout. Use Follow,
/// default top or Follow, default bottom to produce outputs with the same field
/// polarity as the source. For jobs that have multiple inputs, the output field
/// polarity might change over the course of the output. Follow behavior depends
/// on the input scan type. If the source is interlaced, the output will be
/// interlaced with the same polarity as the source. If the source is
/// progressive, the output will be interlaced with top field bottom field
/// first, depending on which of the Follow options you choose.
enum ProresInterlaceMode {
  progressive('PROGRESSIVE'),
  topField('TOP_FIELD'),
  bottomField('BOTTOM_FIELD'),
  followTopField('FOLLOW_TOP_FIELD'),
  followBottomField('FOLLOW_BOTTOM_FIELD'),
  ;

  final String value;

  const ProresInterlaceMode(this.value);

  static ProresInterlaceMode fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum ProresInterlaceMode'));
}

/// Optional. Specify how the service determines the pixel aspect ratio (PAR)
/// for this output. The default behavior, Follow source, uses the PAR from your
/// input video for your output. To specify a different PAR, choose any value
/// other than Follow source. When you choose SPECIFIED for this setting, you
/// must also specify values for the parNumerator and parDenominator settings.
enum ProresParControl {
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  specified('SPECIFIED'),
  ;

  final String value;

  const ProresParControl(this.value);

  static ProresParControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum ProresParControl'));
}

/// Use this setting for interlaced outputs, when your output frame rate is half
/// of your input frame rate. In this situation, choose Optimized interlacing to
/// create a better quality interlaced output. In this case, each progressive
/// frame from the input corresponds to an interlaced field in the output. Keep
/// the default value, Basic interlacing, for all other output frame rates. With
/// basic interlacing, MediaConvert performs any frame rate conversion first and
/// then interlaces the frames. When you choose Optimized interlacing and you
/// set your output frame rate to a value that isn't suitable for optimized
/// interlacing, MediaConvert automatically falls back to basic interlacing.
/// Required settings: To use optimized interlacing, you must set Telecine to
/// None or Soft. You can't use optimized interlacing for hard telecine outputs.
/// You must also set Interlace mode to a value other than Progressive.
enum ProresScanTypeConversionMode {
  interlaced('INTERLACED'),
  interlacedOptimize('INTERLACED_OPTIMIZE'),
  ;

  final String value;

  const ProresScanTypeConversionMode(this.value);

  static ProresScanTypeConversionMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum ProresScanTypeConversionMode'));
}

/// Required when you set Codec to the value PRORES.
class ProresSettings {
  /// This setting applies only to ProRes 4444 and ProRes 4444 XQ outputs that you
  /// create from inputs that use 4:4:4 chroma sampling. Set Preserve 4:4:4
  /// sampling to allow outputs to also use 4:4:4 chroma sampling. You must
  /// specify a value for this setting when your output codec profile supports
  /// 4:4:4 chroma sampling. Related Settings: For Apple ProRes outputs with 4:4:4
  /// chroma sampling: Choose Preserve 4:4:4 sampling. Use when your input has
  /// 4:4:4 chroma sampling and your output codec Profile is Apple ProRes 4444 or
  /// 4444 XQ. Note that when you choose Preserve 4:4:4 sampling, you cannot
  /// include any of the following Preprocessors: Dolby Vision, HDR10+, or Noise
  /// reducer.
  final ProresChromaSampling? chromaSampling;

  /// Use Profile to specify the type of Apple ProRes codec to use for this
  /// output.
  final ProresCodecProfile? codecProfile;

  /// If you are using the console, use the Framerate setting to specify the frame
  /// rate for this output. If you want to keep the same frame rate as the input
  /// video, choose Follow source. If you want to do frame rate conversion, choose
  /// a frame rate from the dropdown list or choose Custom. The framerates shown
  /// in the dropdown list are decimal approximations of fractions. If you choose
  /// Custom, specify your frame rate as a fraction.
  final ProresFramerateControl? framerateControl;

  /// Choose the method that you want MediaConvert to use when increasing or
  /// decreasing the frame rate. For numerically simple conversions, such as 60
  /// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
  /// For numerically complex conversions, to avoid stutter: Choose Interpolate.
  /// This results in a smooth picture, but might introduce undesirable video
  /// artifacts. For complex frame rate conversions, especially if your source
  /// video has already been converted from its original cadence: Choose
  /// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
  /// best conversion method frame by frame. Note that using FrameFormer increases
  /// the transcoding time and incurs a significant add-on cost. When you choose
  /// FrameFormer, your input video resolution must be at least 128x96.
  final ProresFramerateConversionAlgorithm? framerateConversionAlgorithm;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateDenominator to specify the denominator of this fraction.
  /// In this example, use 1001 for the value of FramerateDenominator. When you
  /// use the console for transcode jobs that use frame rate conversion, provide
  /// the value as a decimal number for Framerate. In this example, specify
  /// 23.976.
  final int? framerateDenominator;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateNumerator to specify the numerator of this fraction. In
  /// this example, use 24000 for the value of FramerateNumerator. When you use
  /// the console for transcode jobs that use frame rate conversion, provide the
  /// value as a decimal number for Framerate. In this example, specify 23.976.
  final int? framerateNumerator;

  /// Choose the scan line type for the output. Keep the default value,
  /// Progressive to create a progressive output, regardless of the scan type of
  /// your input. Use Top field first or Bottom field first to create an output
  /// that's interlaced with the same field polarity throughout. Use Follow,
  /// default top or Follow, default bottom to produce outputs with the same field
  /// polarity as the source. For jobs that have multiple inputs, the output field
  /// polarity might change over the course of the output. Follow behavior depends
  /// on the input scan type. If the source is interlaced, the output will be
  /// interlaced with the same polarity as the source. If the source is
  /// progressive, the output will be interlaced with top field bottom field
  /// first, depending on which of the Follow options you choose.
  final ProresInterlaceMode? interlaceMode;

  /// Optional. Specify how the service determines the pixel aspect ratio (PAR)
  /// for this output. The default behavior, Follow source, uses the PAR from your
  /// input video for your output. To specify a different PAR, choose any value
  /// other than Follow source. When you choose SPECIFIED for this setting, you
  /// must also specify values for the parNumerator and parDenominator settings.
  final ProresParControl? parControl;

  /// Required when you set Pixel aspect ratio to SPECIFIED. On the console, this
  /// corresponds to any value other than Follow source. When you specify an
  /// output pixel aspect ratio (PAR) that is different from your input video PAR,
  /// provide your output PAR as a ratio. For example, for D1/DV NTSC widescreen,
  /// you would specify the ratio 40:33. In this example, the value for
  /// parDenominator is 33.
  final int? parDenominator;

  /// Required when you set Pixel aspect ratio to SPECIFIED. On the console, this
  /// corresponds to any value other than Follow source. When you specify an
  /// output pixel aspect ratio (PAR) that is different from your input video PAR,
  /// provide your output PAR as a ratio. For example, for D1/DV NTSC widescreen,
  /// you would specify the ratio 40:33. In this example, the value for
  /// parNumerator is 40.
  final int? parNumerator;

  /// Use this setting for interlaced outputs, when your output frame rate is half
  /// of your input frame rate. In this situation, choose Optimized interlacing to
  /// create a better quality interlaced output. In this case, each progressive
  /// frame from the input corresponds to an interlaced field in the output. Keep
  /// the default value, Basic interlacing, for all other output frame rates. With
  /// basic interlacing, MediaConvert performs any frame rate conversion first and
  /// then interlaces the frames. When you choose Optimized interlacing and you
  /// set your output frame rate to a value that isn't suitable for optimized
  /// interlacing, MediaConvert automatically falls back to basic interlacing.
  /// Required settings: To use optimized interlacing, you must set Telecine to
  /// None or Soft. You can't use optimized interlacing for hard telecine outputs.
  /// You must also set Interlace mode to a value other than Progressive.
  final ProresScanTypeConversionMode? scanTypeConversionMode;

  /// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
  /// second (fps). Enable slow PAL to create a 25 fps output. When you enable
  /// slow PAL, MediaConvert relabels the video frames to 25 fps and resamples
  /// your audio to keep it synchronized with the video. Note that enabling this
  /// setting will slightly reduce the duration of your video. Required settings:
  /// You must also set Framerate to 25.
  final ProresSlowPal? slowPal;

  /// When you do frame rate conversion from 23.976 frames per second (fps) to
  /// 29.97 fps, and your output scan type is interlaced, you can optionally
  /// enable hard telecine to create a smoother picture. When you keep the default
  /// value, None, MediaConvert does a standard frame rate conversion to 29.97
  /// without doing anything with the field polarity to create a smoother picture.
  final ProresTelecine? telecine;

  ProresSettings({
    this.chromaSampling,
    this.codecProfile,
    this.framerateControl,
    this.framerateConversionAlgorithm,
    this.framerateDenominator,
    this.framerateNumerator,
    this.interlaceMode,
    this.parControl,
    this.parDenominator,
    this.parNumerator,
    this.scanTypeConversionMode,
    this.slowPal,
    this.telecine,
  });

  factory ProresSettings.fromJson(Map<String, dynamic> json) {
    return ProresSettings(
      chromaSampling: (json['chromaSampling'] as String?)
          ?.let(ProresChromaSampling.fromString),
      codecProfile:
          (json['codecProfile'] as String?)?.let(ProresCodecProfile.fromString),
      framerateControl: (json['framerateControl'] as String?)
          ?.let(ProresFramerateControl.fromString),
      framerateConversionAlgorithm:
          (json['framerateConversionAlgorithm'] as String?)
              ?.let(ProresFramerateConversionAlgorithm.fromString),
      framerateDenominator: json['framerateDenominator'] as int?,
      framerateNumerator: json['framerateNumerator'] as int?,
      interlaceMode: (json['interlaceMode'] as String?)
          ?.let(ProresInterlaceMode.fromString),
      parControl:
          (json['parControl'] as String?)?.let(ProresParControl.fromString),
      parDenominator: json['parDenominator'] as int?,
      parNumerator: json['parNumerator'] as int?,
      scanTypeConversionMode: (json['scanTypeConversionMode'] as String?)
          ?.let(ProresScanTypeConversionMode.fromString),
      slowPal: (json['slowPal'] as String?)?.let(ProresSlowPal.fromString),
      telecine: (json['telecine'] as String?)?.let(ProresTelecine.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final chromaSampling = this.chromaSampling;
    final codecProfile = this.codecProfile;
    final framerateControl = this.framerateControl;
    final framerateConversionAlgorithm = this.framerateConversionAlgorithm;
    final framerateDenominator = this.framerateDenominator;
    final framerateNumerator = this.framerateNumerator;
    final interlaceMode = this.interlaceMode;
    final parControl = this.parControl;
    final parDenominator = this.parDenominator;
    final parNumerator = this.parNumerator;
    final scanTypeConversionMode = this.scanTypeConversionMode;
    final slowPal = this.slowPal;
    final telecine = this.telecine;
    return {
      if (chromaSampling != null) 'chromaSampling': chromaSampling.value,
      if (codecProfile != null) 'codecProfile': codecProfile.value,
      if (framerateControl != null) 'framerateControl': framerateControl.value,
      if (framerateConversionAlgorithm != null)
        'framerateConversionAlgorithm': framerateConversionAlgorithm.value,
      if (framerateDenominator != null)
        'framerateDenominator': framerateDenominator,
      if (framerateNumerator != null) 'framerateNumerator': framerateNumerator,
      if (interlaceMode != null) 'interlaceMode': interlaceMode.value,
      if (parControl != null) 'parControl': parControl.value,
      if (parDenominator != null) 'parDenominator': parDenominator,
      if (parNumerator != null) 'parNumerator': parNumerator,
      if (scanTypeConversionMode != null)
        'scanTypeConversionMode': scanTypeConversionMode.value,
      if (slowPal != null) 'slowPal': slowPal.value,
      if (telecine != null) 'telecine': telecine.value,
    };
  }
}

/// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
/// second (fps). Enable slow PAL to create a 25 fps output. When you enable
/// slow PAL, MediaConvert relabels the video frames to 25 fps and resamples
/// your audio to keep it synchronized with the video. Note that enabling this
/// setting will slightly reduce the duration of your video. Required settings:
/// You must also set Framerate to 25.
enum ProresSlowPal {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const ProresSlowPal(this.value);

  static ProresSlowPal fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum ProresSlowPal'));
}

/// When you do frame rate conversion from 23.976 frames per second (fps) to
/// 29.97 fps, and your output scan type is interlaced, you can optionally
/// enable hard telecine to create a smoother picture. When you keep the default
/// value, None, MediaConvert does a standard frame rate conversion to 29.97
/// without doing anything with the field polarity to create a smoother picture.
enum ProresTelecine {
  none('NONE'),
  hard('HARD'),
  ;

  final String value;

  const ProresTelecine(this.value);

  static ProresTelecine fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum ProresTelecine'));
}

class PutPolicyResponse {
  /// A policy configures behavior that you allow or disallow for your account.
  /// For information about MediaConvert policies, see the user guide at
  /// http://docs.aws.amazon.com/mediaconvert/latest/ug/what-is.html
  final Policy? policy;

  PutPolicyResponse({
    this.policy,
  });

  factory PutPolicyResponse.fromJson(Map<String, dynamic> json) {
    return PutPolicyResponse(
      policy: json['policy'] != null
          ? Policy.fromJson(json['policy'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final policy = this.policy;
    return {
      if (policy != null) 'policy': policy,
    };
  }
}

/// You can use queues to manage the resources that are available to your AWS
/// account for running multiple transcoding jobs at the same time. If you don't
/// specify a queue, the service sends all jobs through the default queue. For
/// more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/working-with-queues.html.
class Queue {
  /// A name that you create for each queue. Each name must be unique within your
  /// account.
  final String name;

  /// An identifier for this resource that is unique within all of AWS.
  final String? arn;

  /// The timestamp in epoch seconds for when you created the queue.
  final DateTime? createdAt;

  /// An optional description that you create for each queue.
  final String? description;

  /// The timestamp in epoch seconds for when you most recently updated the queue.
  final DateTime? lastUpdated;

  /// Specifies whether the pricing plan for the queue is on-demand or reserved.
  /// For on-demand, you pay per minute, billed in increments of .01 minute. For
  /// reserved, you pay for the transcoding capacity of the entire queue,
  /// regardless of how much or how little you use it. Reserved pricing requires a
  /// 12-month commitment.
  final PricingPlan? pricingPlan;

  /// The estimated number of jobs with a PROGRESSING status.
  final int? progressingJobsCount;

  /// Details about the pricing plan for your reserved queue. Required for
  /// reserved queues and not applicable to on-demand queues.
  final ReservationPlan? reservationPlan;

  /// Queues can be ACTIVE or PAUSED. If you pause a queue, the service won't
  /// begin processing jobs in that queue. Jobs that are running when you pause
  /// the queue continue to run until they finish or result in an error.
  final QueueStatus? status;

  /// The estimated number of jobs with a SUBMITTED status.
  final int? submittedJobsCount;

  /// Specifies whether this on-demand queue is system or custom. System queues
  /// are built in. You can't modify or delete system queues. You can create and
  /// modify custom queues.
  final Type? type;

  Queue({
    required this.name,
    this.arn,
    this.createdAt,
    this.description,
    this.lastUpdated,
    this.pricingPlan,
    this.progressingJobsCount,
    this.reservationPlan,
    this.status,
    this.submittedJobsCount,
    this.type,
  });

  factory Queue.fromJson(Map<String, dynamic> json) {
    return Queue(
      name: json['name'] as String,
      arn: json['arn'] as String?,
      createdAt: timeStampFromJson(json['createdAt']),
      description: json['description'] as String?,
      lastUpdated: timeStampFromJson(json['lastUpdated']),
      pricingPlan:
          (json['pricingPlan'] as String?)?.let(PricingPlan.fromString),
      progressingJobsCount: json['progressingJobsCount'] as int?,
      reservationPlan: json['reservationPlan'] != null
          ? ReservationPlan.fromJson(
              json['reservationPlan'] as Map<String, dynamic>)
          : null,
      status: (json['status'] as String?)?.let(QueueStatus.fromString),
      submittedJobsCount: json['submittedJobsCount'] as int?,
      type: (json['type'] as String?)?.let(Type.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final name = this.name;
    final arn = this.arn;
    final createdAt = this.createdAt;
    final description = this.description;
    final lastUpdated = this.lastUpdated;
    final pricingPlan = this.pricingPlan;
    final progressingJobsCount = this.progressingJobsCount;
    final reservationPlan = this.reservationPlan;
    final status = this.status;
    final submittedJobsCount = this.submittedJobsCount;
    final type = this.type;
    return {
      'name': name,
      if (arn != null) 'arn': arn,
      if (createdAt != null) 'createdAt': unixTimestampToJson(createdAt),
      if (description != null) 'description': description,
      if (lastUpdated != null) 'lastUpdated': unixTimestampToJson(lastUpdated),
      if (pricingPlan != null) 'pricingPlan': pricingPlan.value,
      if (progressingJobsCount != null)
        'progressingJobsCount': progressingJobsCount,
      if (reservationPlan != null) 'reservationPlan': reservationPlan,
      if (status != null) 'status': status.value,
      if (submittedJobsCount != null) 'submittedJobsCount': submittedJobsCount,
      if (type != null) 'type': type.value,
    };
  }
}

/// Optional. When you request a list of queues, you can choose to list them
/// alphabetically by NAME or chronologically by CREATION_DATE. If you don't
/// specify, the service will list them by creation date.
enum QueueListBy {
  name('NAME'),
  creationDate('CREATION_DATE'),
  ;

  final String value;

  const QueueListBy(this.value);

  static QueueListBy fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum QueueListBy'));
}

/// Queues can be ACTIVE or PAUSED. If you pause a queue, jobs in that queue
/// won't begin. Jobs that are running when you pause a queue continue to run
/// until they finish or result in an error.
enum QueueStatus {
  active('ACTIVE'),
  paused('PAUSED'),
  ;

  final String value;

  const QueueStatus(this.value);

  static QueueStatus fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum QueueStatus'));
}

/// Description of the source and destination queues between which the job has
/// moved, along with the timestamp of the move
class QueueTransition {
  /// The queue that the job was on after the transition.
  final String? destinationQueue;

  /// The queue that the job was on before the transition.
  final String? sourceQueue;

  /// The time, in Unix epoch format, that the job moved from the source queue to
  /// the destination queue.
  final DateTime? timestamp;

  QueueTransition({
    this.destinationQueue,
    this.sourceQueue,
    this.timestamp,
  });

  factory QueueTransition.fromJson(Map<String, dynamic> json) {
    return QueueTransition(
      destinationQueue: json['destinationQueue'] as String?,
      sourceQueue: json['sourceQueue'] as String?,
      timestamp: timeStampFromJson(json['timestamp']),
    );
  }

  Map<String, dynamic> toJson() {
    final destinationQueue = this.destinationQueue;
    final sourceQueue = this.sourceQueue;
    final timestamp = this.timestamp;
    return {
      if (destinationQueue != null) 'destinationQueue': destinationQueue,
      if (sourceQueue != null) 'sourceQueue': sourceQueue,
      if (timestamp != null) 'timestamp': unixTimestampToJson(timestamp),
    };
  }
}

/// Use Rectangle to identify a specific area of the video frame.
class Rectangle {
  /// Height of rectangle in pixels. Specify only even numbers.
  final int? height;

  /// Width of rectangle in pixels. Specify only even numbers.
  final int? width;

  /// The distance, in pixels, between the rectangle and the left edge of the
  /// video frame. Specify only even numbers.
  final int? x;

  /// The distance, in pixels, between the rectangle and the top edge of the video
  /// frame. Specify only even numbers.
  final int? y;

  Rectangle({
    this.height,
    this.width,
    this.x,
    this.y,
  });

  factory Rectangle.fromJson(Map<String, dynamic> json) {
    return Rectangle(
      height: json['height'] as int?,
      width: json['width'] as int?,
      x: json['x'] as int?,
      y: json['y'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final height = this.height;
    final width = this.width;
    final x = this.x;
    final y = this.y;
    return {
      if (height != null) 'height': height,
      if (width != null) 'width': width,
      if (x != null) 'x': x,
      if (y != null) 'y': y,
    };
  }
}

/// Use Manual audio remixing to adjust audio levels for each audio channel in
/// each output of your job. With audio remixing, you can output more or fewer
/// audio channels than your input audio source provides.
class RemixSettings {
  /// Optionally specify the channel in your input that contains your audio
  /// description audio signal. MediaConvert mixes your audio signal across all
  /// output channels, while reducing their volume according to your data stream.
  /// When you specify an audio description audio channel, you must also specify
  /// an audio description data channel. For more information about audio
  /// description signals, see the BBC WHP 198 and 051 white papers.
  final int? audioDescriptionAudioChannel;

  /// Optionally specify the channel in your input that contains your audio
  /// description data stream. MediaConvert mixes your audio signal across all
  /// output channels, while reducing their volume according to your data stream.
  /// When you specify an audio description data channel, you must also specify an
  /// audio description audio channel. For more information about audio
  /// description signals, see the BBC WHP 198 and 051 white papers.
  final int? audioDescriptionDataChannel;

  /// Channel mapping contains the group of fields that hold the remixing value
  /// for each channel, in dB. Specify remix values to indicate how much of the
  /// content from your input audio channel you want in your output audio
  /// channels. Each instance of the InputChannels or InputChannelsFineTune array
  /// specifies these values for one output channel. Use one instance of this
  /// array for each output channel. In the console, each array corresponds to a
  /// column in the graphical depiction of the mapping matrix. The rows of the
  /// graphical matrix correspond to input channels. Valid values are within the
  /// range from -60 (mute) through 6. A setting of 0 passes the input channel
  /// unchanged to the output channel (no attenuation or amplification). Use
  /// InputChannels or InputChannelsFineTune to specify your remix values. Don't
  /// use both.
  final ChannelMapping? channelMapping;

  /// Specify the number of audio channels from your input that you want to use in
  /// your output. With remixing, you might combine or split the data in these
  /// channels, so the number of channels in your final output might be different.
  /// If you are doing both input channel mapping and output channel mapping, the
  /// number of output channels in your input mapping must be the same as the
  /// number of input channels in your output mapping.
  final int? channelsIn;

  /// Specify the number of channels in this output after remixing. Valid values:
  /// 1, 2, 4, 6, 8... 64. (1 and even numbers to 64.) If you are doing both input
  /// channel mapping and output channel mapping, the number of output channels in
  /// your input mapping must be the same as the number of input channels in your
  /// output mapping.
  final int? channelsOut;

  RemixSettings({
    this.audioDescriptionAudioChannel,
    this.audioDescriptionDataChannel,
    this.channelMapping,
    this.channelsIn,
    this.channelsOut,
  });

  factory RemixSettings.fromJson(Map<String, dynamic> json) {
    return RemixSettings(
      audioDescriptionAudioChannel:
          json['audioDescriptionAudioChannel'] as int?,
      audioDescriptionDataChannel: json['audioDescriptionDataChannel'] as int?,
      channelMapping: json['channelMapping'] != null
          ? ChannelMapping.fromJson(
              json['channelMapping'] as Map<String, dynamic>)
          : null,
      channelsIn: json['channelsIn'] as int?,
      channelsOut: json['channelsOut'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final audioDescriptionAudioChannel = this.audioDescriptionAudioChannel;
    final audioDescriptionDataChannel = this.audioDescriptionDataChannel;
    final channelMapping = this.channelMapping;
    final channelsIn = this.channelsIn;
    final channelsOut = this.channelsOut;
    return {
      if (audioDescriptionAudioChannel != null)
        'audioDescriptionAudioChannel': audioDescriptionAudioChannel,
      if (audioDescriptionDataChannel != null)
        'audioDescriptionDataChannel': audioDescriptionDataChannel,
      if (channelMapping != null) 'channelMapping': channelMapping,
      if (channelsIn != null) 'channelsIn': channelsIn,
      if (channelsOut != null) 'channelsOut': channelsOut,
    };
  }
}

/// Specifies whether the term of your reserved queue pricing plan is
/// automatically extended (AUTO_RENEW) or expires (EXPIRE) at the end of the
/// term.
enum RenewalType {
  autoRenew('AUTO_RENEW'),
  expire('EXPIRE'),
  ;

  final String value;

  const RenewalType(this.value);

  static RenewalType fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum RenewalType'));
}

/// Set to ENABLED to force a rendition to be included.
enum RequiredFlag {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const RequiredFlag(this.value);

  static RequiredFlag fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum RequiredFlag'));
}

/// Details about the pricing plan for your reserved queue. Required for
/// reserved queues and not applicable to on-demand queues.
class ReservationPlan {
  /// The length of the term of your reserved queue pricing plan commitment.
  final Commitment? commitment;

  /// The timestamp in epoch seconds for when the current pricing plan term for
  /// this reserved queue expires.
  final DateTime? expiresAt;

  /// The timestamp in epoch seconds for when you set up the current pricing plan
  /// for this reserved queue.
  final DateTime? purchasedAt;

  /// Specifies whether the term of your reserved queue pricing plan is
  /// automatically extended (AUTO_RENEW) or expires (EXPIRE) at the end of the
  /// term.
  final RenewalType? renewalType;

  /// Specifies the number of reserved transcode slots (RTS) for this queue. The
  /// number of RTS determines how many jobs the queue can process in parallel;
  /// each RTS can process one job at a time. When you increase this number, you
  /// extend your existing commitment with a new 12-month commitment for a larger
  /// number of RTS. The new commitment begins when you purchase the additional
  /// capacity. You can't decrease the number of RTS in your reserved queue.
  final int? reservedSlots;

  /// Specifies whether the pricing plan for your reserved queue is ACTIVE or
  /// EXPIRED.
  final ReservationPlanStatus? status;

  ReservationPlan({
    this.commitment,
    this.expiresAt,
    this.purchasedAt,
    this.renewalType,
    this.reservedSlots,
    this.status,
  });

  factory ReservationPlan.fromJson(Map<String, dynamic> json) {
    return ReservationPlan(
      commitment: (json['commitment'] as String?)?.let(Commitment.fromString),
      expiresAt: timeStampFromJson(json['expiresAt']),
      purchasedAt: timeStampFromJson(json['purchasedAt']),
      renewalType:
          (json['renewalType'] as String?)?.let(RenewalType.fromString),
      reservedSlots: json['reservedSlots'] as int?,
      status:
          (json['status'] as String?)?.let(ReservationPlanStatus.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final commitment = this.commitment;
    final expiresAt = this.expiresAt;
    final purchasedAt = this.purchasedAt;
    final renewalType = this.renewalType;
    final reservedSlots = this.reservedSlots;
    final status = this.status;
    return {
      if (commitment != null) 'commitment': commitment.value,
      if (expiresAt != null) 'expiresAt': unixTimestampToJson(expiresAt),
      if (purchasedAt != null) 'purchasedAt': unixTimestampToJson(purchasedAt),
      if (renewalType != null) 'renewalType': renewalType.value,
      if (reservedSlots != null) 'reservedSlots': reservedSlots,
      if (status != null) 'status': status.value,
    };
  }
}

/// Details about the pricing plan for your reserved queue. Required for
/// reserved queues and not applicable to on-demand queues.
class ReservationPlanSettings {
  /// The length of the term of your reserved queue pricing plan commitment.
  final Commitment commitment;

  /// Specifies whether the term of your reserved queue pricing plan is
  /// automatically extended (AUTO_RENEW) or expires (EXPIRE) at the end of the
  /// term. When your term is auto renewed, you extend your commitment by 12
  /// months from the auto renew date. You can cancel this commitment.
  final RenewalType renewalType;

  /// Specifies the number of reserved transcode slots (RTS) for this queue. The
  /// number of RTS determines how many jobs the queue can process in parallel;
  /// each RTS can process one job at a time. You can't decrease the number of RTS
  /// in your reserved queue. You can increase the number of RTS by extending your
  /// existing commitment with a new 12-month commitment for the larger number.
  /// The new commitment begins when you purchase the additional capacity. You
  /// can't cancel your commitment or revert to your original commitment after you
  /// increase the capacity.
  final int reservedSlots;

  ReservationPlanSettings({
    required this.commitment,
    required this.renewalType,
    required this.reservedSlots,
  });

  Map<String, dynamic> toJson() {
    final commitment = this.commitment;
    final renewalType = this.renewalType;
    final reservedSlots = this.reservedSlots;
    return {
      'commitment': commitment.value,
      'renewalType': renewalType.value,
      'reservedSlots': reservedSlots,
    };
  }
}

/// Specifies whether the pricing plan for your reserved queue is ACTIVE or
/// EXPIRED.
enum ReservationPlanStatus {
  active('ACTIVE'),
  expired('EXPIRED'),
  ;

  final String value;

  const ReservationPlanStatus(this.value);

  static ReservationPlanStatus fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum ReservationPlanStatus'));
}

/// The Amazon Resource Name (ARN) and tags for an AWS Elemental MediaConvert
/// resource.
class ResourceTags {
  /// The Amazon Resource Name (ARN) of the resource.
  final String? arn;

  /// The tags for the resource.
  final Map<String, String>? tags;

  ResourceTags({
    this.arn,
    this.tags,
  });

  factory ResourceTags.fromJson(Map<String, dynamic> json) {
    return ResourceTags(
      arn: json['arn'] as String?,
      tags: (json['tags'] as Map<String, dynamic>?)
          ?.map((k, e) => MapEntry(k, e as String)),
    );
  }

  Map<String, dynamic> toJson() {
    final arn = this.arn;
    final tags = this.tags;
    return {
      if (arn != null) 'arn': arn,
      if (tags != null) 'tags': tags,
    };
  }
}

/// Use Respond to AFD to specify how the service changes the video itself in
/// response to AFD values in the input. * Choose Respond to clip the input
/// video frame according to the AFD value, input display aspect ratio, and
/// output display aspect ratio. * Choose Passthrough to include the input AFD
/// values. Do not choose this when AfdSignaling is set to NONE. A preferred
/// implementation of this workflow is to set RespondToAfd to and set
/// AfdSignaling to AUTO. * Choose None to remove all input AFD values from this
/// output.
enum RespondToAfd {
  none('NONE'),
  respond('RESPOND'),
  passthrough('PASSTHROUGH'),
  ;

  final String value;

  const RespondToAfd(this.value);

  static RespondToAfd fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum RespondToAfd'));
}

/// Use Min top rendition size to specify a minimum size for the highest
/// resolution in your ABR stack. * The highest resolution in your ABR stack
/// will be equal to or greater than the value that you enter. For example: If
/// you specify 1280x720 the highest resolution in your ABR stack will be equal
/// to or greater than 1280x720. * If you specify a value for Max resolution,
/// the value that you specify for Min top rendition size must be less than, or
/// equal to, Max resolution. Use Min bottom rendition size to specify a minimum
/// size for the lowest resolution in your ABR stack. * The lowest resolution in
/// your ABR stack will be equal to or greater than the value that you enter.
/// For example: If you specify 640x360 the lowest resolution in your ABR stack
/// will be equal to or greater than to 640x360. * If you specify a Min top
/// rendition size rule, the value that you specify for Min bottom rendition
/// size must be less than, or equal to, Min top rendition size. Use Force
/// include renditions to specify one or more resolutions to include your ABR
/// stack. * (Recommended) To optimize automated ABR, specify as few resolutions
/// as possible. * (Required) The number of resolutions that you specify must be
/// equal to, or less than, the Max renditions setting. * If you specify a Min
/// top rendition size rule, specify at least one resolution that is equal to,
/// or greater than, Min top rendition size. * If you specify a Min bottom
/// rendition size rule, only specify resolutions that are equal to, or greater
/// than, Min bottom rendition size. * If you specify a Force include renditions
/// rule, do not specify a separate rule for Allowed renditions. * Note: The ABR
/// stack may include other resolutions that you do not specify here, depending
/// on the Max renditions setting. Use Allowed renditions to specify a list of
/// possible resolutions in your ABR stack. * (Required) The number of
/// resolutions that you specify must be equal to, or greater than, the Max
/// renditions setting. * MediaConvert will create an ABR stack exclusively from
/// the list of resolutions that you specify. * Some resolutions in the Allowed
/// renditions list may not be included, however you can force a resolution to
/// be included by setting Required to ENABLED. * You must specify at least one
/// resolution that is greater than or equal to any resolutions that you specify
/// in Min top rendition size or Min bottom rendition size. * If you specify
/// Allowed renditions, you must not specify a separate rule for Force include
/// renditions.
enum RuleType {
  minTopRenditionSize('MIN_TOP_RENDITION_SIZE'),
  minBottomRenditionSize('MIN_BOTTOM_RENDITION_SIZE'),
  forceIncludeRenditions('FORCE_INCLUDE_RENDITIONS'),
  allowedRenditions('ALLOWED_RENDITIONS'),
  ;

  final String value;

  const RuleType(this.value);

  static RuleType fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum RuleType'));
}

/// Optional. Have MediaConvert automatically apply Amazon S3 access control for
/// the outputs in this output group. When you don't use this setting, S3
/// automatically applies the default access control list PRIVATE.
class S3DestinationAccessControl {
  /// Choose an Amazon S3 canned ACL for MediaConvert to apply to this output.
  final S3ObjectCannedAcl? cannedAcl;

  S3DestinationAccessControl({
    this.cannedAcl,
  });

  factory S3DestinationAccessControl.fromJson(Map<String, dynamic> json) {
    return S3DestinationAccessControl(
      cannedAcl:
          (json['cannedAcl'] as String?)?.let(S3ObjectCannedAcl.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final cannedAcl = this.cannedAcl;
    return {
      if (cannedAcl != null) 'cannedAcl': cannedAcl.value,
    };
  }
}

/// Settings associated with S3 destination
class S3DestinationSettings {
  /// Optional. Have MediaConvert automatically apply Amazon S3 access control for
  /// the outputs in this output group. When you don't use this setting, S3
  /// automatically applies the default access control list PRIVATE.
  final S3DestinationAccessControl? accessControl;

  /// Settings for how your job outputs are encrypted as they are uploaded to
  /// Amazon S3.
  final S3EncryptionSettings? encryption;

  /// Specify the S3 storage class to use for this output. To use your
  /// destination's default storage class: Keep the default value, Not set. For
  /// more information about S3 storage classes, see
  /// https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html
  final S3StorageClass? storageClass;

  S3DestinationSettings({
    this.accessControl,
    this.encryption,
    this.storageClass,
  });

  factory S3DestinationSettings.fromJson(Map<String, dynamic> json) {
    return S3DestinationSettings(
      accessControl: json['accessControl'] != null
          ? S3DestinationAccessControl.fromJson(
              json['accessControl'] as Map<String, dynamic>)
          : null,
      encryption: json['encryption'] != null
          ? S3EncryptionSettings.fromJson(
              json['encryption'] as Map<String, dynamic>)
          : null,
      storageClass:
          (json['storageClass'] as String?)?.let(S3StorageClass.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final accessControl = this.accessControl;
    final encryption = this.encryption;
    final storageClass = this.storageClass;
    return {
      if (accessControl != null) 'accessControl': accessControl,
      if (encryption != null) 'encryption': encryption,
      if (storageClass != null) 'storageClass': storageClass.value,
    };
  }
}

/// Settings for how your job outputs are encrypted as they are uploaded to
/// Amazon S3.
class S3EncryptionSettings {
  /// Specify how you want your data keys managed. AWS uses data keys to encrypt
  /// your content. AWS also encrypts the data keys themselves, using a customer
  /// master key (CMK), and then stores the encrypted data keys alongside your
  /// encrypted content. Use this setting to specify which AWS service manages the
  /// CMK. For simplest set up, choose Amazon S3. If you want your master key to
  /// be managed by AWS Key Management Service (KMS), choose AWS KMS. By default,
  /// when you choose AWS KMS, KMS uses the AWS managed customer master key (CMK)
  /// associated with Amazon S3 to encrypt your data keys. You can optionally
  /// choose to specify a different, customer managed CMK. Do so by specifying the
  /// Amazon Resource Name (ARN) of the key for the setting KMS ARN.
  final S3ServerSideEncryptionType? encryptionType;

  /// Optionally, specify the encryption context that you want to use alongside
  /// your KMS key. AWS KMS uses this encryption context as additional
  /// authenticated data (AAD) to support authenticated encryption. This value
  /// must be a base64-encoded UTF-8 string holding JSON which represents a
  /// string-string map. To use this setting, you must also set Server-side
  /// encryption to AWS KMS. For more information about encryption context, see:
  /// https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#encrypt_context.
  final String? kmsEncryptionContext;

  /// Optionally, specify the customer master key (CMK) that you want to use to
  /// encrypt the data key that AWS uses to encrypt your output content. Enter the
  /// Amazon Resource Name (ARN) of the CMK. To use this setting, you must also
  /// set Server-side encryption to AWS KMS. If you set Server-side encryption to
  /// AWS KMS but don't specify a CMK here, AWS uses the AWS managed CMK
  /// associated with Amazon S3.
  final String? kmsKeyArn;

  S3EncryptionSettings({
    this.encryptionType,
    this.kmsEncryptionContext,
    this.kmsKeyArn,
  });

  factory S3EncryptionSettings.fromJson(Map<String, dynamic> json) {
    return S3EncryptionSettings(
      encryptionType: (json['encryptionType'] as String?)
          ?.let(S3ServerSideEncryptionType.fromString),
      kmsEncryptionContext: json['kmsEncryptionContext'] as String?,
      kmsKeyArn: json['kmsKeyArn'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final encryptionType = this.encryptionType;
    final kmsEncryptionContext = this.kmsEncryptionContext;
    final kmsKeyArn = this.kmsKeyArn;
    return {
      if (encryptionType != null) 'encryptionType': encryptionType.value,
      if (kmsEncryptionContext != null)
        'kmsEncryptionContext': kmsEncryptionContext,
      if (kmsKeyArn != null) 'kmsKeyArn': kmsKeyArn,
    };
  }
}

/// Choose an Amazon S3 canned ACL for MediaConvert to apply to this output.
enum S3ObjectCannedAcl {
  publicRead('PUBLIC_READ'),
  authenticatedRead('AUTHENTICATED_READ'),
  bucketOwnerRead('BUCKET_OWNER_READ'),
  bucketOwnerFullControl('BUCKET_OWNER_FULL_CONTROL'),
  ;

  final String value;

  const S3ObjectCannedAcl(this.value);

  static S3ObjectCannedAcl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum S3ObjectCannedAcl'));
}

/// Specify how you want your data keys managed. AWS uses data keys to encrypt
/// your content. AWS also encrypts the data keys themselves, using a customer
/// master key (CMK), and then stores the encrypted data keys alongside your
/// encrypted content. Use this setting to specify which AWS service manages the
/// CMK. For simplest set up, choose Amazon S3. If you want your master key to
/// be managed by AWS Key Management Service (KMS), choose AWS KMS. By default,
/// when you choose AWS KMS, KMS uses the AWS managed customer master key (CMK)
/// associated with Amazon S3 to encrypt your data keys. You can optionally
/// choose to specify a different, customer managed CMK. Do so by specifying the
/// Amazon Resource Name (ARN) of the key for the setting KMS ARN.
enum S3ServerSideEncryptionType {
  serverSideEncryptionS3('SERVER_SIDE_ENCRYPTION_S3'),
  serverSideEncryptionKms('SERVER_SIDE_ENCRYPTION_KMS'),
  ;

  final String value;

  const S3ServerSideEncryptionType(this.value);

  static S3ServerSideEncryptionType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum S3ServerSideEncryptionType'));
}

/// Specify the S3 storage class to use for this output. To use your
/// destination's default storage class: Keep the default value, Not set. For
/// more information about S3 storage classes, see
/// https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html
enum S3StorageClass {
  standard('STANDARD'),
  reducedRedundancy('REDUCED_REDUNDANCY'),
  standardIa('STANDARD_IA'),
  onezoneIa('ONEZONE_IA'),
  intelligentTiering('INTELLIGENT_TIERING'),
  glacier('GLACIER'),
  deepArchive('DEEP_ARCHIVE'),
  ;

  final String value;

  const S3StorageClass(this.value);

  static S3StorageClass fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum S3StorageClass'));
}

/// Specify how MediaConvert limits the color sample range for this output. To
/// create a limited range output from a full range input: Choose Limited range
/// squeeze. For full range inputs, MediaConvert performs a linear offset to
/// color samples equally across all pixels and frames. Color samples in 10-bit
/// outputs are limited to 64 through 940, and 8-bit outputs are limited to 16
/// through 235. Note: For limited range inputs, values for color samples are
/// passed through to your output unchanged. MediaConvert does not limit the
/// sample range. To correct pixels in your input that are out of range or out
/// of gamut: Choose Limited range clip. Use for broadcast applications.
/// MediaConvert conforms any pixels outside of the values that you specify
/// under Minimum YUV and Maximum YUV to limited range bounds. MediaConvert also
/// corrects any YUV values that, when converted to RGB, would be outside the
/// bounds you specify under Minimum RGB tolerance and Maximum RGB tolerance.
/// With either limited range conversion, MediaConvert writes the sample range
/// metadata in the output.
enum SampleRangeConversion {
  limitedRangeSqueeze('LIMITED_RANGE_SQUEEZE'),
  none('NONE'),
  limitedRangeClip('LIMITED_RANGE_CLIP'),
  ;

  final String value;

  const SampleRangeConversion(this.value);

  static SampleRangeConversion fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum SampleRangeConversion'));
}

/// Specify the video Scaling behavior when your output has a different
/// resolution than your input. For more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/video-scaling.html
enum ScalingBehavior {
  $default('DEFAULT'),
  stretchToOutput('STRETCH_TO_OUTPUT'),
  fit('FIT'),
  fitNoUpscale('FIT_NO_UPSCALE'),
  fill('FILL'),
  ;

  final String value;

  const ScalingBehavior(this.value);

  static ScalingBehavior fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum ScalingBehavior'));
}

/// Set Framerate to make sure that the captions and the video are synchronized
/// in the output. Specify a frame rate that matches the frame rate of the
/// associated video. If the video frame rate is 29.97, choose 29.97 dropframe
/// only if the video has video_insertion=true and drop_frame_timecode=true;
/// otherwise, choose 29.97 non-dropframe.
enum SccDestinationFramerate {
  framerate_23_97('FRAMERATE_23_97'),
  framerate_24('FRAMERATE_24'),
  framerate_25('FRAMERATE_25'),
  framerate_29_97Dropframe('FRAMERATE_29_97_DROPFRAME'),
  framerate_29_97NonDropframe('FRAMERATE_29_97_NON_DROPFRAME'),
  ;

  final String value;

  const SccDestinationFramerate(this.value);

  static SccDestinationFramerate fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum SccDestinationFramerate'));
}

/// Settings related to SCC captions. SCC is a sidecar format that holds
/// captions in a file that is separate from the video container. Set up sidecar
/// captions in the same output group, but different output from your video. For
/// more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/scc-srt-output-captions.html.
class SccDestinationSettings {
  /// Set Framerate to make sure that the captions and the video are synchronized
  /// in the output. Specify a frame rate that matches the frame rate of the
  /// associated video. If the video frame rate is 29.97, choose 29.97 dropframe
  /// only if the video has video_insertion=true and drop_frame_timecode=true;
  /// otherwise, choose 29.97 non-dropframe.
  final SccDestinationFramerate? framerate;

  SccDestinationSettings({
    this.framerate,
  });

  factory SccDestinationSettings.fromJson(Map<String, dynamic> json) {
    return SccDestinationSettings(
      framerate: (json['framerate'] as String?)
          ?.let(SccDestinationFramerate.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final framerate = this.framerate;
    return {
      if (framerate != null) 'framerate': framerate.value,
    };
  }
}

class SearchJobsResponse {
  /// List of jobs.
  final List<Job>? jobs;

  /// Use this string to request the next batch of jobs.
  final String? nextToken;

  SearchJobsResponse({
    this.jobs,
    this.nextToken,
  });

  factory SearchJobsResponse.fromJson(Map<String, dynamic> json) {
    return SearchJobsResponse(
      jobs: (json['jobs'] as List?)
          ?.nonNulls
          .map((e) => Job.fromJson(e as Map<String, dynamic>))
          .toList(),
      nextToken: json['nextToken'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final jobs = this.jobs;
    final nextToken = this.nextToken;
    return {
      if (jobs != null) 'jobs': jobs,
      if (nextToken != null) 'nextToken': nextToken,
    };
  }
}

/// Enable this setting when you run a test job to estimate how many reserved
/// transcoding slots (RTS) you need. When this is enabled, MediaConvert runs
/// your job from an on-demand queue with similar performance to what you will
/// see with one RTS in a reserved queue. This setting is disabled by default.
enum SimulateReservedQueue {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const SimulateReservedQueue(this.value);

  static SimulateReservedQueue fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum SimulateReservedQueue'));
}

/// If your output group type is HLS, DASH, or Microsoft Smooth, use these
/// settings when doing DRM encryption with a SPEKE-compliant key provider. If
/// your output group type is CMAF, use the SpekeKeyProviderCmaf settings
/// instead.
class SpekeKeyProvider {
  /// If you want your key provider to encrypt the content keys that it provides
  /// to MediaConvert, set up a certificate with a master key using AWS
  /// Certificate Manager. Specify the certificate's Amazon Resource Name (ARN)
  /// here.
  final String? certificateArn;

  /// Specify the resource ID that your SPEKE-compliant key provider uses to
  /// identify this content.
  final String? resourceId;

  /// Relates to SPEKE implementation. DRM system identifiers. DASH output groups
  /// support a max of two system ids. Other group types support one system id.
  /// See
  /// https://dashif.org/identifiers/content_protection/ for more details.
  final List<String>? systemIds;

  /// Specify the URL to the key server that your SPEKE-compliant DRM key provider
  /// uses to provide keys for encrypting your content.
  final String? url;

  SpekeKeyProvider({
    this.certificateArn,
    this.resourceId,
    this.systemIds,
    this.url,
  });

  factory SpekeKeyProvider.fromJson(Map<String, dynamic> json) {
    return SpekeKeyProvider(
      certificateArn: json['certificateArn'] as String?,
      resourceId: json['resourceId'] as String?,
      systemIds: (json['systemIds'] as List?)
          ?.nonNulls
          .map((e) => e as String)
          .toList(),
      url: json['url'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final certificateArn = this.certificateArn;
    final resourceId = this.resourceId;
    final systemIds = this.systemIds;
    final url = this.url;
    return {
      if (certificateArn != null) 'certificateArn': certificateArn,
      if (resourceId != null) 'resourceId': resourceId,
      if (systemIds != null) 'systemIds': systemIds,
      if (url != null) 'url': url,
    };
  }
}

/// If your output group type is CMAF, use these settings when doing DRM
/// encryption with a SPEKE-compliant key provider. If your output group type is
/// HLS, DASH, or Microsoft Smooth, use the SpekeKeyProvider settings instead.
class SpekeKeyProviderCmaf {
  /// If you want your key provider to encrypt the content keys that it provides
  /// to MediaConvert, set up a certificate with a master key using AWS
  /// Certificate Manager. Specify the certificate's Amazon Resource Name (ARN)
  /// here.
  final String? certificateArn;

  /// Specify the DRM system IDs that you want signaled in the DASH manifest that
  /// MediaConvert creates as part of this CMAF package. The DASH manifest can
  /// currently signal up to three system IDs. For more information, see
  /// https://dashif.org/identifiers/content_protection/.
  final List<String>? dashSignaledSystemIds;

  /// Specify the DRM system ID that you want signaled in the HLS manifest that
  /// MediaConvert creates as part of this CMAF package. The HLS manifest can
  /// currently signal only one system ID. For more information, see
  /// https://dashif.org/identifiers/content_protection/.
  final List<String>? hlsSignaledSystemIds;

  /// Specify the resource ID that your SPEKE-compliant key provider uses to
  /// identify this content.
  final String? resourceId;

  /// Specify the URL to the key server that your SPEKE-compliant DRM key provider
  /// uses to provide keys for encrypting your content.
  final String? url;

  SpekeKeyProviderCmaf({
    this.certificateArn,
    this.dashSignaledSystemIds,
    this.hlsSignaledSystemIds,
    this.resourceId,
    this.url,
  });

  factory SpekeKeyProviderCmaf.fromJson(Map<String, dynamic> json) {
    return SpekeKeyProviderCmaf(
      certificateArn: json['certificateArn'] as String?,
      dashSignaledSystemIds: (json['dashSignaledSystemIds'] as List?)
          ?.nonNulls
          .map((e) => e as String)
          .toList(),
      hlsSignaledSystemIds: (json['hlsSignaledSystemIds'] as List?)
          ?.nonNulls
          .map((e) => e as String)
          .toList(),
      resourceId: json['resourceId'] as String?,
      url: json['url'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final certificateArn = this.certificateArn;
    final dashSignaledSystemIds = this.dashSignaledSystemIds;
    final hlsSignaledSystemIds = this.hlsSignaledSystemIds;
    final resourceId = this.resourceId;
    final url = this.url;
    return {
      if (certificateArn != null) 'certificateArn': certificateArn,
      if (dashSignaledSystemIds != null)
        'dashSignaledSystemIds': dashSignaledSystemIds,
      if (hlsSignaledSystemIds != null)
        'hlsSignaledSystemIds': hlsSignaledSystemIds,
      if (resourceId != null) 'resourceId': resourceId,
      if (url != null) 'url': url,
    };
  }
}

/// Settings related to SRT captions. SRT is a sidecar format that holds
/// captions in a file that is separate from the video container. Set up sidecar
/// captions in the same output group, but different output from your video.
class SrtDestinationSettings {
  /// Set Style passthrough to ENABLED to use the available style, color, and
  /// position information from your input captions. MediaConvert uses default
  /// settings for any missing style and position information in your input
  /// captions. Set Style passthrough to DISABLED, or leave blank, to ignore the
  /// style and position information from your input captions and use simplified
  /// output captions.
  final SrtStylePassthrough? stylePassthrough;

  SrtDestinationSettings({
    this.stylePassthrough,
  });

  factory SrtDestinationSettings.fromJson(Map<String, dynamic> json) {
    return SrtDestinationSettings(
      stylePassthrough: (json['stylePassthrough'] as String?)
          ?.let(SrtStylePassthrough.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final stylePassthrough = this.stylePassthrough;
    return {
      if (stylePassthrough != null) 'stylePassthrough': stylePassthrough.value,
    };
  }
}

/// Set Style passthrough to ENABLED to use the available style, color, and
/// position information from your input captions. MediaConvert uses default
/// settings for any missing style and position information in your input
/// captions. Set Style passthrough to DISABLED, or leave blank, to ignore the
/// style and position information from your input captions and use simplified
/// output captions.
enum SrtStylePassthrough {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const SrtStylePassthrough(this.value);

  static SrtStylePassthrough fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum SrtStylePassthrough'));
}

/// Use these settings to set up encryption with a static key provider.
class StaticKeyProvider {
  /// Relates to DRM implementation. Sets the value of the KEYFORMAT attribute.
  /// Must be 'identity' or a reverse DNS string. May be omitted to indicate an
  /// implicit value of 'identity'.
  final String? keyFormat;

  /// Relates to DRM implementation. Either a single positive integer version
  /// value or a slash delimited list of version values (1/2/3).
  final String? keyFormatVersions;

  /// Relates to DRM implementation. Use a 32-character hexidecimal string to
  /// specify Key Value.
  final String? staticKeyValue;

  /// Relates to DRM implementation. The location of the license server used for
  /// protecting content.
  final String? url;

  StaticKeyProvider({
    this.keyFormat,
    this.keyFormatVersions,
    this.staticKeyValue,
    this.url,
  });

  factory StaticKeyProvider.fromJson(Map<String, dynamic> json) {
    return StaticKeyProvider(
      keyFormat: json['keyFormat'] as String?,
      keyFormatVersions: json['keyFormatVersions'] as String?,
      staticKeyValue: json['staticKeyValue'] as String?,
      url: json['url'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final keyFormat = this.keyFormat;
    final keyFormatVersions = this.keyFormatVersions;
    final staticKeyValue = this.staticKeyValue;
    final url = this.url;
    return {
      if (keyFormat != null) 'keyFormat': keyFormat,
      if (keyFormatVersions != null) 'keyFormatVersions': keyFormatVersions,
      if (staticKeyValue != null) 'staticKeyValue': staticKeyValue,
      if (url != null) 'url': url,
    };
  }
}

/// Specify how often MediaConvert sends STATUS_UPDATE events to Amazon
/// CloudWatch Events. Set the interval, in seconds, between status updates.
/// MediaConvert sends an update at this interval from the time the service
/// begins processing your job to the time it completes the transcode or
/// encounters an error.
enum StatusUpdateInterval {
  seconds_10('SECONDS_10'),
  seconds_12('SECONDS_12'),
  seconds_15('SECONDS_15'),
  seconds_20('SECONDS_20'),
  seconds_30('SECONDS_30'),
  seconds_60('SECONDS_60'),
  seconds_120('SECONDS_120'),
  seconds_180('SECONDS_180'),
  seconds_240('SECONDS_240'),
  seconds_300('SECONDS_300'),
  seconds_360('SECONDS_360'),
  seconds_420('SECONDS_420'),
  seconds_480('SECONDS_480'),
  seconds_540('SECONDS_540'),
  seconds_600('SECONDS_600'),
  ;

  final String value;

  const StatusUpdateInterval(this.value);

  static StatusUpdateInterval fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum StatusUpdateInterval'));
}

class TagResourceResponse {
  TagResourceResponse();

  factory TagResourceResponse.fromJson(Map<String, dynamic> _) {
    return TagResourceResponse();
  }

  Map<String, dynamic> toJson() {
    return {};
  }
}

/// Settings related to teletext captions. Set up teletext captions in the same
/// output as your video. For more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/teletext-output-captions.html.
class TeletextDestinationSettings {
  /// Set pageNumber to the Teletext page number for the destination captions for
  /// this output. This value must be a three-digit hexadecimal string; strings
  /// ending in -FF are invalid. If you are passing through the entire set of
  /// Teletext data, do not use this field.
  final String? pageNumber;

  /// Specify the page types for this Teletext page. If you don't specify a value
  /// here, the service sets the page type to the default value Subtitle. If you
  /// pass through the entire set of Teletext data, don't use this field. When you
  /// pass through a set of Teletext pages, your output has the same page types as
  /// your input.
  final List<TeletextPageType>? pageTypes;

  TeletextDestinationSettings({
    this.pageNumber,
    this.pageTypes,
  });

  factory TeletextDestinationSettings.fromJson(Map<String, dynamic> json) {
    return TeletextDestinationSettings(
      pageNumber: json['pageNumber'] as String?,
      pageTypes: (json['pageTypes'] as List?)
          ?.nonNulls
          .map((e) => TeletextPageType.fromString((e as String)))
          .toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final pageNumber = this.pageNumber;
    final pageTypes = this.pageTypes;
    return {
      if (pageNumber != null) 'pageNumber': pageNumber,
      if (pageTypes != null)
        'pageTypes': pageTypes.map((e) => e.value).toList(),
    };
  }
}

/// A page type as defined in the standard ETSI EN 300 468, Table 94
enum TeletextPageType {
  pageTypeInitial('PAGE_TYPE_INITIAL'),
  pageTypeSubtitle('PAGE_TYPE_SUBTITLE'),
  pageTypeAddlInfo('PAGE_TYPE_ADDL_INFO'),
  pageTypeProgramSchedule('PAGE_TYPE_PROGRAM_SCHEDULE'),
  pageTypeHearingImpairedSubtitle('PAGE_TYPE_HEARING_IMPAIRED_SUBTITLE'),
  ;

  final String value;

  const TeletextPageType(this.value);

  static TeletextPageType fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum TeletextPageType'));
}

/// Settings specific to Teletext caption sources, including Page number.
class TeletextSourceSettings {
  /// Use Page Number to specify the three-digit hexadecimal page number that will
  /// be used for Teletext captions. Do not use this setting if you are passing
  /// through teletext from the input source to output.
  final String? pageNumber;

  TeletextSourceSettings({
    this.pageNumber,
  });

  factory TeletextSourceSettings.fromJson(Map<String, dynamic> json) {
    return TeletextSourceSettings(
      pageNumber: json['pageNumber'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final pageNumber = this.pageNumber;
    return {
      if (pageNumber != null) 'pageNumber': pageNumber,
    };
  }
}

/// Settings for burning the output timecode and specified prefix into the
/// output.
class TimecodeBurnin {
  /// Use Font size to set the font size of any burned-in timecode. Valid values
  /// are 10, 16, 32, 48.
  final int? fontSize;

  /// Use Position under Timecode burn-in to specify the location the burned-in
  /// timecode on output video.
  final TimecodeBurninPosition? position;

  /// Use Prefix to place ASCII characters before any burned-in timecode. For
  /// example, a prefix of "EZ-" will result in the timecode "EZ-00:00:00:00".
  /// Provide either the characters themselves or the ASCII code equivalents. The
  /// supported range of characters is 0x20 through 0x7e. This includes letters,
  /// numbers, and all special characters represented on a standard English
  /// keyboard.
  final String? prefix;

  TimecodeBurnin({
    this.fontSize,
    this.position,
    this.prefix,
  });

  factory TimecodeBurnin.fromJson(Map<String, dynamic> json) {
    return TimecodeBurnin(
      fontSize: json['fontSize'] as int?,
      position:
          (json['position'] as String?)?.let(TimecodeBurninPosition.fromString),
      prefix: json['prefix'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final fontSize = this.fontSize;
    final position = this.position;
    final prefix = this.prefix;
    return {
      if (fontSize != null) 'fontSize': fontSize,
      if (position != null) 'position': position.value,
      if (prefix != null) 'prefix': prefix,
    };
  }
}

/// Use Position under Timecode burn-in to specify the location the burned-in
/// timecode on output video.
enum TimecodeBurninPosition {
  topCenter('TOP_CENTER'),
  topLeft('TOP_LEFT'),
  topRight('TOP_RIGHT'),
  middleLeft('MIDDLE_LEFT'),
  middleCenter('MIDDLE_CENTER'),
  middleRight('MIDDLE_RIGHT'),
  bottomLeft('BOTTOM_LEFT'),
  bottomCenter('BOTTOM_CENTER'),
  bottomRight('BOTTOM_RIGHT'),
  ;

  final String value;

  const TimecodeBurninPosition(this.value);

  static TimecodeBurninPosition fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum TimecodeBurninPosition'));
}

/// These settings control how the service handles timecodes throughout the job.
/// These settings don't affect input clipping.
class TimecodeConfig {
  /// If you use an editing platform that relies on an anchor timecode, use Anchor
  /// Timecode to specify a timecode that will match the input video frame to the
  /// output video frame. Use 24-hour format with frame number, (HH:MM:SS:FF) or
  /// (HH:MM:SS;FF). This setting ignores frame rate conversion. System behavior
  /// for Anchor Timecode varies depending on your setting for Source. * If Source
  /// is set to Specified Start, the first input frame is the specified value in
  /// Start Timecode. Anchor Timecode and Start Timecode are used calculate output
  /// timecode. * If Source is set to Start at 0 the first frame is 00:00:00:00. *
  /// If Source is set to Embedded, the first frame is the timecode value on the
  /// first input frame of the input.
  final String? anchor;

  /// Use Source to set how timecodes are handled within this job. To make sure
  /// that your video, audio, captions, and markers are synchronized and that
  /// time-based features, such as image inserter, work correctly, choose the
  /// Timecode source option that matches your assets. All timecodes are in a
  /// 24-hour format with frame number (HH:MM:SS:FF). * Embedded - Use the
  /// timecode that is in the input video. If no embedded timecode is in the
  /// source, the service will use Start at 0 instead. * Start at 0 - Set the
  /// timecode of the initial frame to 00:00:00:00. * Specified Start - Set the
  /// timecode of the initial frame to a value other than zero. You use Start
  /// timecode to provide this value.
  final TimecodeSource? source;

  /// Only use when you set Source to Specified start. Use Start timecode to
  /// specify the timecode for the initial frame. Use 24-hour format with frame
  /// number, (HH:MM:SS:FF) or (HH:MM:SS;FF).
  final String? start;

  /// Only applies to outputs that support program-date-time stamp. Use Timestamp
  /// offset to overwrite the timecode date without affecting the time and frame
  /// number. Provide the new date as a string in the format "yyyy-mm-dd". To use
  /// Timestamp offset, you must also enable Insert program-date-time in the
  /// output settings. For example, if the date part of your timecodes is
  /// 2002-1-25 and you want to change it to one year later, set Timestamp offset
  /// to 2003-1-25.
  final String? timestampOffset;

  TimecodeConfig({
    this.anchor,
    this.source,
    this.start,
    this.timestampOffset,
  });

  factory TimecodeConfig.fromJson(Map<String, dynamic> json) {
    return TimecodeConfig(
      anchor: json['anchor'] as String?,
      source: (json['source'] as String?)?.let(TimecodeSource.fromString),
      start: json['start'] as String?,
      timestampOffset: json['timestampOffset'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final anchor = this.anchor;
    final source = this.source;
    final start = this.start;
    final timestampOffset = this.timestampOffset;
    return {
      if (anchor != null) 'anchor': anchor,
      if (source != null) 'source': source.value,
      if (start != null) 'start': start,
      if (timestampOffset != null) 'timestampOffset': timestampOffset,
    };
  }
}

/// Use Source to set how timecodes are handled within this job. To make sure
/// that your video, audio, captions, and markers are synchronized and that
/// time-based features, such as image inserter, work correctly, choose the
/// Timecode source option that matches your assets. All timecodes are in a
/// 24-hour format with frame number (HH:MM:SS:FF). * Embedded - Use the
/// timecode that is in the input video. If no embedded timecode is in the
/// source, the service will use Start at 0 instead. * Start at 0 - Set the
/// timecode of the initial frame to 00:00:00:00. * Specified Start - Set the
/// timecode of the initial frame to a value other than zero. You use Start
/// timecode to provide this value.
enum TimecodeSource {
  embedded('EMBEDDED'),
  zerobased('ZEROBASED'),
  specifiedstart('SPECIFIEDSTART'),
  ;

  final String value;

  const TimecodeSource(this.value);

  static TimecodeSource fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum TimecodeSource'));
}

/// Set ID3 metadata to Passthrough to include ID3 metadata in this output. This
/// includes ID3 metadata from the following features: ID3 timestamp period, and
/// Custom ID3 metadata inserter. To exclude this ID3 metadata in this output:
/// set ID3 metadata to None or leave blank.
enum TimedMetadata {
  passthrough('PASSTHROUGH'),
  none('NONE'),
  ;

  final String value;

  const TimedMetadata(this.value);

  static TimedMetadata fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum TimedMetadata'));
}

/// Insert user-defined custom ID3 metadata at timecodes that you specify. In
/// each output that you want to include this metadata, you must set ID3
/// metadata to Passthrough.
class TimedMetadataInsertion {
  /// Id3Insertions contains the array of Id3Insertion instances.
  final List<Id3Insertion>? id3Insertions;

  TimedMetadataInsertion({
    this.id3Insertions,
  });

  factory TimedMetadataInsertion.fromJson(Map<String, dynamic> json) {
    return TimedMetadataInsertion(
      id3Insertions: (json['id3Insertions'] as List?)
          ?.nonNulls
          .map((e) => Id3Insertion.fromJson(e as Map<String, dynamic>))
          .toList(),
    );
  }

  Map<String, dynamic> toJson() {
    final id3Insertions = this.id3Insertions;
    return {
      if (id3Insertions != null) 'id3Insertions': id3Insertions,
    };
  }
}

/// Information about when jobs are submitted, started, and finished is
/// specified in Unix epoch format in seconds.
class Timing {
  /// The time, in Unix epoch format, that the transcoding job finished
  final DateTime? finishTime;

  /// The time, in Unix epoch format, that transcoding for the job began.
  final DateTime? startTime;

  /// The time, in Unix epoch format, that you submitted the job.
  final DateTime? submitTime;

  Timing({
    this.finishTime,
    this.startTime,
    this.submitTime,
  });

  factory Timing.fromJson(Map<String, dynamic> json) {
    return Timing(
      finishTime: timeStampFromJson(json['finishTime']),
      startTime: timeStampFromJson(json['startTime']),
      submitTime: timeStampFromJson(json['submitTime']),
    );
  }

  Map<String, dynamic> toJson() {
    final finishTime = this.finishTime;
    final startTime = this.startTime;
    final submitTime = this.submitTime;
    return {
      if (finishTime != null) 'finishTime': unixTimestampToJson(finishTime),
      if (startTime != null) 'startTime': unixTimestampToJson(startTime),
      if (submitTime != null) 'submitTime': unixTimestampToJson(submitTime),
    };
  }
}

/// Settings specific to caption sources that are specified by track number.
/// Currently, this is only IMSC captions in an IMF package. If your caption
/// source is IMSC 1.1 in a separate xml file, use FileSourceSettings instead of
/// TrackSourceSettings.
class TrackSourceSettings {
  /// Use this setting to select a single captions track from a source. Track
  /// numbers correspond to the order in the captions source file. For IMF
  /// sources, track numbering is based on the order that the captions appear in
  /// the CPL. For example, use 1 to select the captions asset that is listed
  /// first in the CPL. To include more than one captions track in your job
  /// outputs, create multiple input captions selectors. Specify one track per
  /// selector.
  final int? trackNumber;

  TrackSourceSettings({
    this.trackNumber,
  });

  factory TrackSourceSettings.fromJson(Map<String, dynamic> json) {
    return TrackSourceSettings(
      trackNumber: json['trackNumber'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final trackNumber = this.trackNumber;
    return {
      if (trackNumber != null) 'trackNumber': trackNumber,
    };
  }
}

/// Specify the initial presentation timestamp (PTS) offset for your transport
/// stream output. To let MediaConvert automatically determine the initial PTS
/// offset: Keep the default value, Auto. We recommend that you choose Auto for
/// the widest player compatibility. The initial PTS will be at least two
/// seconds and vary depending on your output's bitrate, HRD buffer size and HRD
/// buffer initial fill percentage. To manually specify an initial PTS offset:
/// Choose Seconds. Then specify the number of seconds with PTS offset.
enum TsPtsOffset {
  auto('AUTO'),
  seconds('SECONDS'),
  ;

  final String value;

  const TsPtsOffset(this.value);

  static TsPtsOffset fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum TsPtsOffset'));
}

/// Settings related to TTML captions. TTML is a sidecar format that holds
/// captions in a file that is separate from the video container. Set up sidecar
/// captions in the same output group, but different output from your video. For
/// more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/ttml-and-webvtt-output-captions.html.
class TtmlDestinationSettings {
  /// Pass through style and position information from a TTML-like input source
  /// (TTML, IMSC, SMPTE-TT) to the TTML output.
  final TtmlStylePassthrough? stylePassthrough;

  TtmlDestinationSettings({
    this.stylePassthrough,
  });

  factory TtmlDestinationSettings.fromJson(Map<String, dynamic> json) {
    return TtmlDestinationSettings(
      stylePassthrough: (json['stylePassthrough'] as String?)
          ?.let(TtmlStylePassthrough.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final stylePassthrough = this.stylePassthrough;
    return {
      if (stylePassthrough != null) 'stylePassthrough': stylePassthrough.value,
    };
  }
}

/// Pass through style and position information from a TTML-like input source
/// (TTML, IMSC, SMPTE-TT) to the TTML output.
enum TtmlStylePassthrough {
  enabled('ENABLED'),
  disabled('DISABLED'),
  ;

  final String value;

  const TtmlStylePassthrough(this.value);

  static TtmlStylePassthrough fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum TtmlStylePassthrough'));
}

enum Type {
  system('SYSTEM'),
  custom('CUSTOM'),
  ;

  final String value;

  const Type(this.value);

  static Type fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception('$value is not known in enum Type'));
}

/// The four character code for the uncompressed video.
enum UncompressedFourcc {
  i420('I420'),
  i422('I422'),
  i444('I444'),
  ;

  final String value;

  const UncompressedFourcc(this.value);

  static UncompressedFourcc fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum UncompressedFourcc'));
}

/// Use the Framerate setting to specify the frame rate for this output. If you
/// want to keep the same frame rate as the input video, choose Follow source.
/// If you want to do frame rate conversion, choose a frame rate from the
/// dropdown list or choose Custom. The framerates shown in the dropdown list
/// are decimal approximations of fractions. If you choose Custom, specify your
/// frame rate as a fraction.
enum UncompressedFramerateControl {
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  specified('SPECIFIED'),
  ;

  final String value;

  const UncompressedFramerateControl(this.value);

  static UncompressedFramerateControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum UncompressedFramerateControl'));
}

/// Choose the method that you want MediaConvert to use when increasing or
/// decreasing the frame rate. For numerically simple conversions, such as 60
/// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
/// For numerically complex conversions, to avoid stutter: Choose Interpolate.
/// This results in a smooth picture, but might introduce undesirable video
/// artifacts. For complex frame rate conversions, especially if your source
/// video has already been converted from its original cadence: Choose
/// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
/// best conversion method frame by frame. Note that using FrameFormer increases
/// the transcoding time and incurs a significant add-on cost. When you choose
/// FrameFormer, your input video resolution must be at least 128x96.
enum UncompressedFramerateConversionAlgorithm {
  duplicateDrop('DUPLICATE_DROP'),
  interpolate('INTERPOLATE'),
  frameformer('FRAMEFORMER'),
  ;

  final String value;

  const UncompressedFramerateConversionAlgorithm(this.value);

  static UncompressedFramerateConversionAlgorithm fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum UncompressedFramerateConversionAlgorithm'));
}

/// Optional. Choose the scan line type for this output. If you don't specify a
/// value, MediaConvert will create a progressive output.
enum UncompressedInterlaceMode {
  interlaced('INTERLACED'),
  progressive('PROGRESSIVE'),
  ;

  final String value;

  const UncompressedInterlaceMode(this.value);

  static UncompressedInterlaceMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum UncompressedInterlaceMode'));
}

/// Use this setting for interlaced outputs, when your output frame rate is half
/// of your input frame rate. In this situation, choose Optimized interlacing to
/// create a better quality interlaced output. In this case, each progressive
/// frame from the input corresponds to an interlaced field in the output. Keep
/// the default value, Basic interlacing, for all other output frame rates. With
/// basic interlacing, MediaConvert performs any frame rate conversion first and
/// then interlaces the frames. When you choose Optimized interlacing and you
/// set your output frame rate to a value that isn't suitable for optimized
/// interlacing, MediaConvert automatically falls back to basic interlacing.
/// Required settings: To use optimized interlacing, you must set Telecine to
/// None or Soft. You can't use optimized interlacing for hard telecine outputs.
/// You must also set Interlace mode to a value other than Progressive.
enum UncompressedScanTypeConversionMode {
  interlaced('INTERLACED'),
  interlacedOptimize('INTERLACED_OPTIMIZE'),
  ;

  final String value;

  const UncompressedScanTypeConversionMode(this.value);

  static UncompressedScanTypeConversionMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum UncompressedScanTypeConversionMode'));
}

/// Required when you set Codec, under VideoDescription>CodecSettings to the
/// value UNCOMPRESSED.
class UncompressedSettings {
  /// The four character code for the uncompressed video.
  final UncompressedFourcc? fourcc;

  /// Use the Framerate setting to specify the frame rate for this output. If you
  /// want to keep the same frame rate as the input video, choose Follow source.
  /// If you want to do frame rate conversion, choose a frame rate from the
  /// dropdown list or choose Custom. The framerates shown in the dropdown list
  /// are decimal approximations of fractions. If you choose Custom, specify your
  /// frame rate as a fraction.
  final UncompressedFramerateControl? framerateControl;

  /// Choose the method that you want MediaConvert to use when increasing or
  /// decreasing the frame rate. For numerically simple conversions, such as 60
  /// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
  /// For numerically complex conversions, to avoid stutter: Choose Interpolate.
  /// This results in a smooth picture, but might introduce undesirable video
  /// artifacts. For complex frame rate conversions, especially if your source
  /// video has already been converted from its original cadence: Choose
  /// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
  /// best conversion method frame by frame. Note that using FrameFormer increases
  /// the transcoding time and incurs a significant add-on cost. When you choose
  /// FrameFormer, your input video resolution must be at least 128x96.
  final UncompressedFramerateConversionAlgorithm? framerateConversionAlgorithm;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateDenominator to specify the denominator of this fraction.
  /// In this example, use 1001 for the value of FramerateDenominator. When you
  /// use the console for transcode jobs that use frame rate conversion, provide
  /// the value as a decimal number for Framerate. In this example, specify
  /// 23.976.
  final int? framerateDenominator;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateNumerator to specify the numerator of this fraction. In
  /// this example, use 24000 for the value of FramerateNumerator. When you use
  /// the console for transcode jobs that use frame rate conversion, provide the
  /// value as a decimal number for Framerate. In this example, specify 23.976.
  final int? framerateNumerator;

  /// Optional. Choose the scan line type for this output. If you don't specify a
  /// value, MediaConvert will create a progressive output.
  final UncompressedInterlaceMode? interlaceMode;

  /// Use this setting for interlaced outputs, when your output frame rate is half
  /// of your input frame rate. In this situation, choose Optimized interlacing to
  /// create a better quality interlaced output. In this case, each progressive
  /// frame from the input corresponds to an interlaced field in the output. Keep
  /// the default value, Basic interlacing, for all other output frame rates. With
  /// basic interlacing, MediaConvert performs any frame rate conversion first and
  /// then interlaces the frames. When you choose Optimized interlacing and you
  /// set your output frame rate to a value that isn't suitable for optimized
  /// interlacing, MediaConvert automatically falls back to basic interlacing.
  /// Required settings: To use optimized interlacing, you must set Telecine to
  /// None or Soft. You can't use optimized interlacing for hard telecine outputs.
  /// You must also set Interlace mode to a value other than Progressive.
  final UncompressedScanTypeConversionMode? scanTypeConversionMode;

  /// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
  /// second (fps). Enable slow PAL to create a 25 fps output by relabeling the
  /// video frames and resampling your audio. Note that enabling this setting will
  /// slightly reduce the duration of your video. Related settings: You must also
  /// set Framerate to 25.
  final UncompressedSlowPal? slowPal;

  /// When you do frame rate conversion from 23.976 frames per second (fps) to
  /// 29.97 fps, and your output scan type is interlaced, you can optionally
  /// enable hard telecine to create a smoother picture. When you keep the default
  /// value, None, MediaConvert does a standard frame rate conversion to 29.97
  /// without doing anything with the field polarity to create a smoother picture.
  final UncompressedTelecine? telecine;

  UncompressedSettings({
    this.fourcc,
    this.framerateControl,
    this.framerateConversionAlgorithm,
    this.framerateDenominator,
    this.framerateNumerator,
    this.interlaceMode,
    this.scanTypeConversionMode,
    this.slowPal,
    this.telecine,
  });

  factory UncompressedSettings.fromJson(Map<String, dynamic> json) {
    return UncompressedSettings(
      fourcc: (json['fourcc'] as String?)?.let(UncompressedFourcc.fromString),
      framerateControl: (json['framerateControl'] as String?)
          ?.let(UncompressedFramerateControl.fromString),
      framerateConversionAlgorithm:
          (json['framerateConversionAlgorithm'] as String?)
              ?.let(UncompressedFramerateConversionAlgorithm.fromString),
      framerateDenominator: json['framerateDenominator'] as int?,
      framerateNumerator: json['framerateNumerator'] as int?,
      interlaceMode: (json['interlaceMode'] as String?)
          ?.let(UncompressedInterlaceMode.fromString),
      scanTypeConversionMode: (json['scanTypeConversionMode'] as String?)
          ?.let(UncompressedScanTypeConversionMode.fromString),
      slowPal:
          (json['slowPal'] as String?)?.let(UncompressedSlowPal.fromString),
      telecine:
          (json['telecine'] as String?)?.let(UncompressedTelecine.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final fourcc = this.fourcc;
    final framerateControl = this.framerateControl;
    final framerateConversionAlgorithm = this.framerateConversionAlgorithm;
    final framerateDenominator = this.framerateDenominator;
    final framerateNumerator = this.framerateNumerator;
    final interlaceMode = this.interlaceMode;
    final scanTypeConversionMode = this.scanTypeConversionMode;
    final slowPal = this.slowPal;
    final telecine = this.telecine;
    return {
      if (fourcc != null) 'fourcc': fourcc.value,
      if (framerateControl != null) 'framerateControl': framerateControl.value,
      if (framerateConversionAlgorithm != null)
        'framerateConversionAlgorithm': framerateConversionAlgorithm.value,
      if (framerateDenominator != null)
        'framerateDenominator': framerateDenominator,
      if (framerateNumerator != null) 'framerateNumerator': framerateNumerator,
      if (interlaceMode != null) 'interlaceMode': interlaceMode.value,
      if (scanTypeConversionMode != null)
        'scanTypeConversionMode': scanTypeConversionMode.value,
      if (slowPal != null) 'slowPal': slowPal.value,
      if (telecine != null) 'telecine': telecine.value,
    };
  }
}

/// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
/// second (fps). Enable slow PAL to create a 25 fps output by relabeling the
/// video frames and resampling your audio. Note that enabling this setting will
/// slightly reduce the duration of your video. Related settings: You must also
/// set Framerate to 25.
enum UncompressedSlowPal {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const UncompressedSlowPal(this.value);

  static UncompressedSlowPal fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum UncompressedSlowPal'));
}

/// When you do frame rate conversion from 23.976 frames per second (fps) to
/// 29.97 fps, and your output scan type is interlaced, you can optionally
/// enable hard telecine to create a smoother picture. When you keep the default
/// value, None, MediaConvert does a standard frame rate conversion to 29.97
/// without doing anything with the field polarity to create a smoother picture.
enum UncompressedTelecine {
  none('NONE'),
  hard('HARD'),
  ;

  final String value;

  const UncompressedTelecine(this.value);

  static UncompressedTelecine fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum UncompressedTelecine'));
}

class UntagResourceResponse {
  UntagResourceResponse();

  factory UntagResourceResponse.fromJson(Map<String, dynamic> _) {
    return UntagResourceResponse();
  }

  Map<String, dynamic> toJson() {
    return {};
  }
}

class UpdateJobTemplateResponse {
  /// A job template is a pre-made set of encoding instructions that you can use
  /// to quickly create a job.
  final JobTemplate? jobTemplate;

  UpdateJobTemplateResponse({
    this.jobTemplate,
  });

  factory UpdateJobTemplateResponse.fromJson(Map<String, dynamic> json) {
    return UpdateJobTemplateResponse(
      jobTemplate: json['jobTemplate'] != null
          ? JobTemplate.fromJson(json['jobTemplate'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final jobTemplate = this.jobTemplate;
    return {
      if (jobTemplate != null) 'jobTemplate': jobTemplate,
    };
  }
}

class UpdatePresetResponse {
  /// A preset is a collection of preconfigured media conversion settings that you
  /// want MediaConvert to apply to the output during the conversion process.
  final Preset? preset;

  UpdatePresetResponse({
    this.preset,
  });

  factory UpdatePresetResponse.fromJson(Map<String, dynamic> json) {
    return UpdatePresetResponse(
      preset: json['preset'] != null
          ? Preset.fromJson(json['preset'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final preset = this.preset;
    return {
      if (preset != null) 'preset': preset,
    };
  }
}

class UpdateQueueResponse {
  /// You can use queues to manage the resources that are available to your AWS
  /// account for running multiple transcoding jobs at the same time. If you don't
  /// specify a queue, the service sends all jobs through the default queue. For
  /// more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/working-with-queues.html.
  final Queue? queue;

  UpdateQueueResponse({
    this.queue,
  });

  factory UpdateQueueResponse.fromJson(Map<String, dynamic> json) {
    return UpdateQueueResponse(
      queue: json['queue'] != null
          ? Queue.fromJson(json['queue'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final queue = this.queue;
    return {
      if (queue != null) 'queue': queue,
    };
  }
}

/// Specify the VC3 class to choose the quality characteristics for this output.
/// VC3 class, together with the settings Framerate (framerateNumerator and
/// framerateDenominator) and Resolution (height and width), determine your
/// output bitrate. For example, say that your video resolution is 1920x1080 and
/// your framerate is 29.97. Then Class 145 gives you an output with a bitrate
/// of approximately 145 Mbps and Class 220 gives you and output with a bitrate
/// of approximately 220 Mbps. VC3 class also specifies the color bit depth of
/// your output.
enum Vc3Class {
  class_145_8bit('CLASS_145_8BIT'),
  class_220_8bit('CLASS_220_8BIT'),
  class_220_10bit('CLASS_220_10BIT'),
  ;

  final String value;

  const Vc3Class(this.value);

  static Vc3Class fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum Vc3Class'));
}

/// If you are using the console, use the Framerate setting to specify the frame
/// rate for this output. If you want to keep the same frame rate as the input
/// video, choose Follow source. If you want to do frame rate conversion, choose
/// a frame rate from the dropdown list or choose Custom. The framerates shown
/// in the dropdown list are decimal approximations of fractions. If you choose
/// Custom, specify your frame rate as a fraction.
enum Vc3FramerateControl {
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  specified('SPECIFIED'),
  ;

  final String value;

  const Vc3FramerateControl(this.value);

  static Vc3FramerateControl fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Vc3FramerateControl'));
}

/// Choose the method that you want MediaConvert to use when increasing or
/// decreasing the frame rate. For numerically simple conversions, such as 60
/// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
/// For numerically complex conversions, to avoid stutter: Choose Interpolate.
/// This results in a smooth picture, but might introduce undesirable video
/// artifacts. For complex frame rate conversions, especially if your source
/// video has already been converted from its original cadence: Choose
/// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
/// best conversion method frame by frame. Note that using FrameFormer increases
/// the transcoding time and incurs a significant add-on cost. When you choose
/// FrameFormer, your input video resolution must be at least 128x96.
enum Vc3FramerateConversionAlgorithm {
  duplicateDrop('DUPLICATE_DROP'),
  interpolate('INTERPOLATE'),
  frameformer('FRAMEFORMER'),
  ;

  final String value;

  const Vc3FramerateConversionAlgorithm(this.value);

  static Vc3FramerateConversionAlgorithm fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Vc3FramerateConversionAlgorithm'));
}

/// Optional. Choose the scan line type for this output. If you don't specify a
/// value, MediaConvert will create a progressive output.
enum Vc3InterlaceMode {
  interlaced('INTERLACED'),
  progressive('PROGRESSIVE'),
  ;

  final String value;

  const Vc3InterlaceMode(this.value);

  static Vc3InterlaceMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Vc3InterlaceMode'));
}

/// Use this setting for interlaced outputs, when your output frame rate is half
/// of your input frame rate. In this situation, choose Optimized interlacing to
/// create a better quality interlaced output. In this case, each progressive
/// frame from the input corresponds to an interlaced field in the output. Keep
/// the default value, Basic interlacing, for all other output frame rates. With
/// basic interlacing, MediaConvert performs any frame rate conversion first and
/// then interlaces the frames. When you choose Optimized interlacing and you
/// set your output frame rate to a value that isn't suitable for optimized
/// interlacing, MediaConvert automatically falls back to basic interlacing.
/// Required settings: To use optimized interlacing, you must set Telecine to
/// None or Soft. You can't use optimized interlacing for hard telecine outputs.
/// You must also set Interlace mode to a value other than Progressive.
enum Vc3ScanTypeConversionMode {
  interlaced('INTERLACED'),
  interlacedOptimize('INTERLACED_OPTIMIZE'),
  ;

  final String value;

  const Vc3ScanTypeConversionMode(this.value);

  static Vc3ScanTypeConversionMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Vc3ScanTypeConversionMode'));
}

/// Required when you set Codec to the value VC3
class Vc3Settings {
  /// If you are using the console, use the Framerate setting to specify the frame
  /// rate for this output. If you want to keep the same frame rate as the input
  /// video, choose Follow source. If you want to do frame rate conversion, choose
  /// a frame rate from the dropdown list or choose Custom. The framerates shown
  /// in the dropdown list are decimal approximations of fractions. If you choose
  /// Custom, specify your frame rate as a fraction.
  final Vc3FramerateControl? framerateControl;

  /// Choose the method that you want MediaConvert to use when increasing or
  /// decreasing the frame rate. For numerically simple conversions, such as 60
  /// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
  /// For numerically complex conversions, to avoid stutter: Choose Interpolate.
  /// This results in a smooth picture, but might introduce undesirable video
  /// artifacts. For complex frame rate conversions, especially if your source
  /// video has already been converted from its original cadence: Choose
  /// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
  /// best conversion method frame by frame. Note that using FrameFormer increases
  /// the transcoding time and incurs a significant add-on cost. When you choose
  /// FrameFormer, your input video resolution must be at least 128x96.
  final Vc3FramerateConversionAlgorithm? framerateConversionAlgorithm;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateDenominator to specify the denominator of this fraction.
  /// In this example, use 1001 for the value of FramerateDenominator. When you
  /// use the console for transcode jobs that use frame rate conversion, provide
  /// the value as a decimal number for Framerate. In this example, specify
  /// 23.976.
  final int? framerateDenominator;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateNumerator to specify the numerator of this fraction. In
  /// this example, use 24000 for the value of FramerateNumerator. When you use
  /// the console for transcode jobs that use frame rate conversion, provide the
  /// value as a decimal number for Framerate. In this example, specify 23.976.
  final int? framerateNumerator;

  /// Optional. Choose the scan line type for this output. If you don't specify a
  /// value, MediaConvert will create a progressive output.
  final Vc3InterlaceMode? interlaceMode;

  /// Use this setting for interlaced outputs, when your output frame rate is half
  /// of your input frame rate. In this situation, choose Optimized interlacing to
  /// create a better quality interlaced output. In this case, each progressive
  /// frame from the input corresponds to an interlaced field in the output. Keep
  /// the default value, Basic interlacing, for all other output frame rates. With
  /// basic interlacing, MediaConvert performs any frame rate conversion first and
  /// then interlaces the frames. When you choose Optimized interlacing and you
  /// set your output frame rate to a value that isn't suitable for optimized
  /// interlacing, MediaConvert automatically falls back to basic interlacing.
  /// Required settings: To use optimized interlacing, you must set Telecine to
  /// None or Soft. You can't use optimized interlacing for hard telecine outputs.
  /// You must also set Interlace mode to a value other than Progressive.
  final Vc3ScanTypeConversionMode? scanTypeConversionMode;

  /// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
  /// second (fps). Enable slow PAL to create a 25 fps output by relabeling the
  /// video frames and resampling your audio. Note that enabling this setting will
  /// slightly reduce the duration of your video. Related settings: You must also
  /// set Framerate to 25.
  final Vc3SlowPal? slowPal;

  /// When you do frame rate conversion from 23.976 frames per second (fps) to
  /// 29.97 fps, and your output scan type is interlaced, you can optionally
  /// enable hard telecine to create a smoother picture. When you keep the default
  /// value, None, MediaConvert does a standard frame rate conversion to 29.97
  /// without doing anything with the field polarity to create a smoother picture.
  final Vc3Telecine? telecine;

  /// Specify the VC3 class to choose the quality characteristics for this output.
  /// VC3 class, together with the settings Framerate (framerateNumerator and
  /// framerateDenominator) and Resolution (height and width), determine your
  /// output bitrate. For example, say that your video resolution is 1920x1080 and
  /// your framerate is 29.97. Then Class 145 gives you an output with a bitrate
  /// of approximately 145 Mbps and Class 220 gives you and output with a bitrate
  /// of approximately 220 Mbps. VC3 class also specifies the color bit depth of
  /// your output.
  final Vc3Class? vc3Class;

  Vc3Settings({
    this.framerateControl,
    this.framerateConversionAlgorithm,
    this.framerateDenominator,
    this.framerateNumerator,
    this.interlaceMode,
    this.scanTypeConversionMode,
    this.slowPal,
    this.telecine,
    this.vc3Class,
  });

  factory Vc3Settings.fromJson(Map<String, dynamic> json) {
    return Vc3Settings(
      framerateControl: (json['framerateControl'] as String?)
          ?.let(Vc3FramerateControl.fromString),
      framerateConversionAlgorithm:
          (json['framerateConversionAlgorithm'] as String?)
              ?.let(Vc3FramerateConversionAlgorithm.fromString),
      framerateDenominator: json['framerateDenominator'] as int?,
      framerateNumerator: json['framerateNumerator'] as int?,
      interlaceMode:
          (json['interlaceMode'] as String?)?.let(Vc3InterlaceMode.fromString),
      scanTypeConversionMode: (json['scanTypeConversionMode'] as String?)
          ?.let(Vc3ScanTypeConversionMode.fromString),
      slowPal: (json['slowPal'] as String?)?.let(Vc3SlowPal.fromString),
      telecine: (json['telecine'] as String?)?.let(Vc3Telecine.fromString),
      vc3Class: (json['vc3Class'] as String?)?.let(Vc3Class.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final framerateControl = this.framerateControl;
    final framerateConversionAlgorithm = this.framerateConversionAlgorithm;
    final framerateDenominator = this.framerateDenominator;
    final framerateNumerator = this.framerateNumerator;
    final interlaceMode = this.interlaceMode;
    final scanTypeConversionMode = this.scanTypeConversionMode;
    final slowPal = this.slowPal;
    final telecine = this.telecine;
    final vc3Class = this.vc3Class;
    return {
      if (framerateControl != null) 'framerateControl': framerateControl.value,
      if (framerateConversionAlgorithm != null)
        'framerateConversionAlgorithm': framerateConversionAlgorithm.value,
      if (framerateDenominator != null)
        'framerateDenominator': framerateDenominator,
      if (framerateNumerator != null) 'framerateNumerator': framerateNumerator,
      if (interlaceMode != null) 'interlaceMode': interlaceMode.value,
      if (scanTypeConversionMode != null)
        'scanTypeConversionMode': scanTypeConversionMode.value,
      if (slowPal != null) 'slowPal': slowPal.value,
      if (telecine != null) 'telecine': telecine.value,
      if (vc3Class != null) 'vc3Class': vc3Class.value,
    };
  }
}

/// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
/// second (fps). Enable slow PAL to create a 25 fps output by relabeling the
/// video frames and resampling your audio. Note that enabling this setting will
/// slightly reduce the duration of your video. Related settings: You must also
/// set Framerate to 25.
enum Vc3SlowPal {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const Vc3SlowPal(this.value);

  static Vc3SlowPal fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum Vc3SlowPal'));
}

/// When you do frame rate conversion from 23.976 frames per second (fps) to
/// 29.97 fps, and your output scan type is interlaced, you can optionally
/// enable hard telecine to create a smoother picture. When you keep the default
/// value, None, MediaConvert does a standard frame rate conversion to 29.97
/// without doing anything with the field polarity to create a smoother picture.
enum Vc3Telecine {
  none('NONE'),
  hard('HARD'),
  ;

  final String value;

  const Vc3Telecine(this.value);

  static Vc3Telecine fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum Vc3Telecine'));
}

/// The action to take on content advisory XDS packets. If you select
/// PASSTHROUGH, packets will not be changed. If you select STRIP, any packets
/// will be removed in output captions.
enum VchipAction {
  passthrough('PASSTHROUGH'),
  strip('STRIP'),
  ;

  final String value;

  const VchipAction(this.value);

  static VchipAction fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum VchipAction'));
}

/// Type of video codec
enum VideoCodec {
  av1('AV1'),
  avcIntra('AVC_INTRA'),
  frameCapture('FRAME_CAPTURE'),
  h_264('H_264'),
  h_265('H_265'),
  mpeg2('MPEG2'),
  passthrough('PASSTHROUGH'),
  prores('PRORES'),
  uncompressed('UNCOMPRESSED'),
  vc3('VC3'),
  vp8('VP8'),
  vp9('VP9'),
  xavc('XAVC'),
  ;

  final String value;

  const VideoCodec(this.value);

  static VideoCodec fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum VideoCodec'));
}

/// Video codec settings contains the group of settings related to video
/// encoding. The settings in this group vary depending on the value that you
/// choose for Video codec. For each codec enum that you choose, define the
/// corresponding settings object. The following lists the codec enum, settings
/// object pairs. * AV1, Av1Settings * AVC_INTRA, AvcIntraSettings *
/// FRAME_CAPTURE, FrameCaptureSettings * H_264, H264Settings * H_265,
/// H265Settings * MPEG2, Mpeg2Settings * PRORES, ProresSettings * UNCOMPRESSED,
/// UncompressedSettings * VC3, Vc3Settings * VP8, Vp8Settings * VP9,
/// Vp9Settings * XAVC, XavcSettings
class VideoCodecSettings {
  /// Required when you set Codec, under VideoDescription>CodecSettings to the
  /// value AV1.
  final Av1Settings? av1Settings;

  /// Required when you choose AVC-Intra for your output video codec. For more
  /// information about the AVC-Intra settings, see the relevant specification.
  /// For detailed information about SD and HD in AVC-Intra, see
  /// https://ieeexplore.ieee.org/document/7290936. For information about 4K/2K in
  /// AVC-Intra, see
  /// https://pro-av.panasonic.net/en/avc-ultra/AVC-ULTRAoverview.pdf.
  final AvcIntraSettings? avcIntraSettings;

  /// Specifies the video codec. This must be equal to one of the enum values
  /// defined by the object VideoCodec. To passthrough the video stream of your
  /// input JPEG2000, VC-3, AVC-INTRA or Apple ProRes video without any video
  /// encoding: Choose Passthrough. If you have multiple input videos, note that
  /// they must have identical encoding attributes. When you choose Passthrough,
  /// your output container must be MXF or QuickTime MOV.
  final VideoCodec? codec;

  /// Required when you set Codec to the value FRAME_CAPTURE.
  final FrameCaptureSettings? frameCaptureSettings;

  /// Required when you set Codec to the value H_264.
  final H264Settings? h264Settings;

  /// Settings for H265 codec
  final H265Settings? h265Settings;

  /// Required when you set Codec to the value MPEG2.
  final Mpeg2Settings? mpeg2Settings;

  /// Required when you set Codec to the value PRORES.
  final ProresSettings? proresSettings;

  /// Required when you set Codec, under VideoDescription>CodecSettings to the
  /// value UNCOMPRESSED.
  final UncompressedSettings? uncompressedSettings;

  /// Required when you set Codec to the value VC3
  final Vc3Settings? vc3Settings;

  /// Required when you set Codec to the value VP8.
  final Vp8Settings? vp8Settings;

  /// Required when you set Codec to the value VP9.
  final Vp9Settings? vp9Settings;

  /// Required when you set Codec to the value XAVC.
  final XavcSettings? xavcSettings;

  VideoCodecSettings({
    this.av1Settings,
    this.avcIntraSettings,
    this.codec,
    this.frameCaptureSettings,
    this.h264Settings,
    this.h265Settings,
    this.mpeg2Settings,
    this.proresSettings,
    this.uncompressedSettings,
    this.vc3Settings,
    this.vp8Settings,
    this.vp9Settings,
    this.xavcSettings,
  });

  factory VideoCodecSettings.fromJson(Map<String, dynamic> json) {
    return VideoCodecSettings(
      av1Settings: json['av1Settings'] != null
          ? Av1Settings.fromJson(json['av1Settings'] as Map<String, dynamic>)
          : null,
      avcIntraSettings: json['avcIntraSettings'] != null
          ? AvcIntraSettings.fromJson(
              json['avcIntraSettings'] as Map<String, dynamic>)
          : null,
      codec: (json['codec'] as String?)?.let(VideoCodec.fromString),
      frameCaptureSettings: json['frameCaptureSettings'] != null
          ? FrameCaptureSettings.fromJson(
              json['frameCaptureSettings'] as Map<String, dynamic>)
          : null,
      h264Settings: json['h264Settings'] != null
          ? H264Settings.fromJson(json['h264Settings'] as Map<String, dynamic>)
          : null,
      h265Settings: json['h265Settings'] != null
          ? H265Settings.fromJson(json['h265Settings'] as Map<String, dynamic>)
          : null,
      mpeg2Settings: json['mpeg2Settings'] != null
          ? Mpeg2Settings.fromJson(
              json['mpeg2Settings'] as Map<String, dynamic>)
          : null,
      proresSettings: json['proresSettings'] != null
          ? ProresSettings.fromJson(
              json['proresSettings'] as Map<String, dynamic>)
          : null,
      uncompressedSettings: json['uncompressedSettings'] != null
          ? UncompressedSettings.fromJson(
              json['uncompressedSettings'] as Map<String, dynamic>)
          : null,
      vc3Settings: json['vc3Settings'] != null
          ? Vc3Settings.fromJson(json['vc3Settings'] as Map<String, dynamic>)
          : null,
      vp8Settings: json['vp8Settings'] != null
          ? Vp8Settings.fromJson(json['vp8Settings'] as Map<String, dynamic>)
          : null,
      vp9Settings: json['vp9Settings'] != null
          ? Vp9Settings.fromJson(json['vp9Settings'] as Map<String, dynamic>)
          : null,
      xavcSettings: json['xavcSettings'] != null
          ? XavcSettings.fromJson(json['xavcSettings'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final av1Settings = this.av1Settings;
    final avcIntraSettings = this.avcIntraSettings;
    final codec = this.codec;
    final frameCaptureSettings = this.frameCaptureSettings;
    final h264Settings = this.h264Settings;
    final h265Settings = this.h265Settings;
    final mpeg2Settings = this.mpeg2Settings;
    final proresSettings = this.proresSettings;
    final uncompressedSettings = this.uncompressedSettings;
    final vc3Settings = this.vc3Settings;
    final vp8Settings = this.vp8Settings;
    final vp9Settings = this.vp9Settings;
    final xavcSettings = this.xavcSettings;
    return {
      if (av1Settings != null) 'av1Settings': av1Settings,
      if (avcIntraSettings != null) 'avcIntraSettings': avcIntraSettings,
      if (codec != null) 'codec': codec.value,
      if (frameCaptureSettings != null)
        'frameCaptureSettings': frameCaptureSettings,
      if (h264Settings != null) 'h264Settings': h264Settings,
      if (h265Settings != null) 'h265Settings': h265Settings,
      if (mpeg2Settings != null) 'mpeg2Settings': mpeg2Settings,
      if (proresSettings != null) 'proresSettings': proresSettings,
      if (uncompressedSettings != null)
        'uncompressedSettings': uncompressedSettings,
      if (vc3Settings != null) 'vc3Settings': vc3Settings,
      if (vp8Settings != null) 'vp8Settings': vp8Settings,
      if (vp9Settings != null) 'vp9Settings': vp9Settings,
      if (xavcSettings != null) 'xavcSettings': xavcSettings,
    };
  }
}

/// Settings related to video encoding of your output. The specific video
/// settings depend on the video codec that you choose.
class VideoDescription {
  /// This setting only applies to H.264, H.265, and MPEG2 outputs. Use Insert AFD
  /// signaling to specify whether the service includes AFD values in the output
  /// video data and what those values are. * Choose None to remove all AFD values
  /// from this output. * Choose Fixed to ignore input AFD values and instead
  /// encode the value specified in the job. * Choose Auto to calculate output AFD
  /// values based on the input AFD scaler data.
  final AfdSignaling? afdSignaling;

  /// The anti-alias filter is automatically applied to all outputs. The service
  /// no longer accepts the value DISABLED for AntiAlias. If you specify that in
  /// your job, the service will ignore the setting.
  final AntiAlias? antiAlias;

  /// Video codec settings contains the group of settings related to video
  /// encoding. The settings in this group vary depending on the value that you
  /// choose for Video codec. For each codec enum that you choose, define the
  /// corresponding settings object. The following lists the codec enum, settings
  /// object pairs. * AV1, Av1Settings * AVC_INTRA, AvcIntraSettings *
  /// FRAME_CAPTURE, FrameCaptureSettings * H_264, H264Settings * H_265,
  /// H265Settings * MPEG2, Mpeg2Settings * PRORES, ProresSettings * UNCOMPRESSED,
  /// UncompressedSettings * VC3, Vc3Settings * VP8, Vp8Settings * VP9,
  /// Vp9Settings * XAVC, XavcSettings
  final VideoCodecSettings? codecSettings;

  /// Choose Insert for this setting to include color metadata in this output.
  /// Choose Ignore to exclude color metadata from this output. If you don't
  /// specify a value, the service sets this to Insert by default.
  final ColorMetadata? colorMetadata;

  /// Use Cropping selection to specify the video area that the service will
  /// include in the output video frame.
  final Rectangle? crop;

  /// Applies only to 29.97 fps outputs. When this feature is enabled, the service
  /// will use drop-frame timecode on outputs. If it is not possible to use
  /// drop-frame timecode, the system will fall back to non-drop-frame. This
  /// setting is enabled by default when Timecode insertion is enabled.
  final DropFrameTimecode? dropFrameTimecode;

  /// Applies only if you set AFD Signaling to Fixed. Use Fixed to specify a
  /// four-bit AFD value which the service will write on all frames of this video
  /// output.
  final int? fixedAfd;

  /// Use Height to define the video resolution height, in pixels, for this
  /// output. To use the same resolution as your input: Leave both Width and
  /// Height blank. To evenly scale from your input resolution: Leave Height blank
  /// and enter a value for Width. For example, if your input is 1920x1080 and you
  /// set Width to 1280, your output will be 1280x720.
  final int? height;

  /// Use Selection placement to define the video area in your output frame. The
  /// area outside of the rectangle that you specify here is black.
  final Rectangle? position;

  /// Use Respond to AFD to specify how the service changes the video itself in
  /// response to AFD values in the input. * Choose Respond to clip the input
  /// video frame according to the AFD value, input display aspect ratio, and
  /// output display aspect ratio. * Choose Passthrough to include the input AFD
  /// values. Do not choose this when AfdSignaling is set to NONE. A preferred
  /// implementation of this workflow is to set RespondToAfd to and set
  /// AfdSignaling to AUTO. * Choose None to remove all input AFD values from this
  /// output.
  final RespondToAfd? respondToAfd;

  /// Specify the video Scaling behavior when your output has a different
  /// resolution than your input. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/video-scaling.html
  final ScalingBehavior? scalingBehavior;

  /// Use Sharpness setting to specify the strength of anti-aliasing. This setting
  /// changes the width of the anti-alias filter kernel used for scaling.
  /// Sharpness only applies if your output resolution is different from your
  /// input resolution. 0 is the softest setting, 100 the sharpest, and 50
  /// recommended for most content.
  final int? sharpness;

  /// Applies only to H.264, H.265, MPEG2, and ProRes outputs. Only enable
  /// Timecode insertion when the input frame rate is identical to the output
  /// frame rate. To include timecodes in this output, set Timecode insertion to
  /// PIC_TIMING_SEI. To leave them out, set it to DISABLED. Default is DISABLED.
  /// When the service inserts timecodes in an output, by default, it uses any
  /// embedded timecodes from the input. If none are present, the service will set
  /// the timecode for the first output frame to zero. To change this default
  /// behavior, adjust the settings under Timecode configuration. In the console,
  /// these settings are located under Job > Job settings > Timecode
  /// configuration. Note - Timecode source under input settings does not affect
  /// the timecodes that are inserted in the output. Source under Job settings >
  /// Timecode configuration does.
  final VideoTimecodeInsertion? timecodeInsertion;

  /// Find additional transcoding features under Preprocessors. Enable the
  /// features at each output individually. These features are disabled by
  /// default.
  final VideoPreprocessor? videoPreprocessors;

  /// Use Width to define the video resolution width, in pixels, for this output.
  /// To use the same resolution as your input: Leave both Width and Height blank.
  /// To evenly scale from your input resolution: Leave Width blank and enter a
  /// value for Height. For example, if your input is 1920x1080 and you set Height
  /// to 720, your output will be 1280x720.
  final int? width;

  VideoDescription({
    this.afdSignaling,
    this.antiAlias,
    this.codecSettings,
    this.colorMetadata,
    this.crop,
    this.dropFrameTimecode,
    this.fixedAfd,
    this.height,
    this.position,
    this.respondToAfd,
    this.scalingBehavior,
    this.sharpness,
    this.timecodeInsertion,
    this.videoPreprocessors,
    this.width,
  });

  factory VideoDescription.fromJson(Map<String, dynamic> json) {
    return VideoDescription(
      afdSignaling:
          (json['afdSignaling'] as String?)?.let(AfdSignaling.fromString),
      antiAlias: (json['antiAlias'] as String?)?.let(AntiAlias.fromString),
      codecSettings: json['codecSettings'] != null
          ? VideoCodecSettings.fromJson(
              json['codecSettings'] as Map<String, dynamic>)
          : null,
      colorMetadata:
          (json['colorMetadata'] as String?)?.let(ColorMetadata.fromString),
      crop: json['crop'] != null
          ? Rectangle.fromJson(json['crop'] as Map<String, dynamic>)
          : null,
      dropFrameTimecode: (json['dropFrameTimecode'] as String?)
          ?.let(DropFrameTimecode.fromString),
      fixedAfd: json['fixedAfd'] as int?,
      height: json['height'] as int?,
      position: json['position'] != null
          ? Rectangle.fromJson(json['position'] as Map<String, dynamic>)
          : null,
      respondToAfd:
          (json['respondToAfd'] as String?)?.let(RespondToAfd.fromString),
      scalingBehavior:
          (json['scalingBehavior'] as String?)?.let(ScalingBehavior.fromString),
      sharpness: json['sharpness'] as int?,
      timecodeInsertion: (json['timecodeInsertion'] as String?)
          ?.let(VideoTimecodeInsertion.fromString),
      videoPreprocessors: json['videoPreprocessors'] != null
          ? VideoPreprocessor.fromJson(
              json['videoPreprocessors'] as Map<String, dynamic>)
          : null,
      width: json['width'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final afdSignaling = this.afdSignaling;
    final antiAlias = this.antiAlias;
    final codecSettings = this.codecSettings;
    final colorMetadata = this.colorMetadata;
    final crop = this.crop;
    final dropFrameTimecode = this.dropFrameTimecode;
    final fixedAfd = this.fixedAfd;
    final height = this.height;
    final position = this.position;
    final respondToAfd = this.respondToAfd;
    final scalingBehavior = this.scalingBehavior;
    final sharpness = this.sharpness;
    final timecodeInsertion = this.timecodeInsertion;
    final videoPreprocessors = this.videoPreprocessors;
    final width = this.width;
    return {
      if (afdSignaling != null) 'afdSignaling': afdSignaling.value,
      if (antiAlias != null) 'antiAlias': antiAlias.value,
      if (codecSettings != null) 'codecSettings': codecSettings,
      if (colorMetadata != null) 'colorMetadata': colorMetadata.value,
      if (crop != null) 'crop': crop,
      if (dropFrameTimecode != null)
        'dropFrameTimecode': dropFrameTimecode.value,
      if (fixedAfd != null) 'fixedAfd': fixedAfd,
      if (height != null) 'height': height,
      if (position != null) 'position': position,
      if (respondToAfd != null) 'respondToAfd': respondToAfd.value,
      if (scalingBehavior != null) 'scalingBehavior': scalingBehavior.value,
      if (sharpness != null) 'sharpness': sharpness,
      if (timecodeInsertion != null)
        'timecodeInsertion': timecodeInsertion.value,
      if (videoPreprocessors != null) 'videoPreprocessors': videoPreprocessors,
      if (width != null) 'width': width,
    };
  }
}

/// Contains details about the output's video stream
class VideoDetail {
  /// Height in pixels for the output
  final int? heightInPx;

  /// Width in pixels for the output
  final int? widthInPx;

  VideoDetail({
    this.heightInPx,
    this.widthInPx,
  });

  factory VideoDetail.fromJson(Map<String, dynamic> json) {
    return VideoDetail(
      heightInPx: json['heightInPx'] as int?,
      widthInPx: json['widthInPx'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final heightInPx = this.heightInPx;
    final widthInPx = this.widthInPx;
    return {
      if (heightInPx != null) 'heightInPx': heightInPx,
      if (widthInPx != null) 'widthInPx': widthInPx,
    };
  }
}

/// Overlay one or more videos on top of your input video. For more information,
/// see https://docs.aws.amazon.com/mediaconvert/latest/ug/video-overlays.html
class VideoOverlay {
  /// Enter the end timecode in the underlying input video for this overlay. Your
  /// overlay will be active through this frame. To display your video overlay for
  /// the duration of the underlying video: Leave blank. Use the format
  /// HH:MM:SS:FF or HH:MM:SS;FF, where HH is the hour, MM is the minute, SS is
  /// the second, and FF is the frame number. When entering this value, take into
  /// account your choice for the underlying Input timecode source. For example,
  /// if you have embedded timecodes that start at 01:00:00:00 and you want your
  /// overlay to end ten minutes into the video, enter 01:10:00:00.
  final String? endTimecode;

  /// Input settings for Video overlay. You can include one or more video overlays
  /// in sequence at different times that you specify.
  final VideoOverlayInput? input;

  /// Enter the start timecode in the underlying input video for this overlay.
  /// Your overlay will be active starting with this frame. To display your video
  /// overlay starting at the beginning of the underlying video: Leave blank. Use
  /// the format HH:MM:SS:FF or HH:MM:SS;FF, where HH is the hour, MM is the
  /// minute, SS is the second, and FF is the frame number. When entering this
  /// value, take into account your choice for the underlying Input timecode
  /// source. For example, if you have embedded timecodes that start at
  /// 01:00:00:00 and you want your overlay to begin five minutes into the video,
  /// enter 01:05:00:00.
  final String? startTimecode;

  VideoOverlay({
    this.endTimecode,
    this.input,
    this.startTimecode,
  });

  factory VideoOverlay.fromJson(Map<String, dynamic> json) {
    return VideoOverlay(
      endTimecode: json['endTimecode'] as String?,
      input: json['input'] != null
          ? VideoOverlayInput.fromJson(json['input'] as Map<String, dynamic>)
          : null,
      startTimecode: json['startTimecode'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final endTimecode = this.endTimecode;
    final input = this.input;
    final startTimecode = this.startTimecode;
    return {
      if (endTimecode != null) 'endTimecode': endTimecode,
      if (input != null) 'input': input,
      if (startTimecode != null) 'startTimecode': startTimecode,
    };
  }
}

/// Input settings for Video overlay. You can include one or more video overlays
/// in sequence at different times that you specify.
class VideoOverlayInput {
  /// Specify the input file S3, HTTP, or HTTPS URI for your video overlay. For
  /// consistency in color and formatting in your output video image, we recommend
  /// that you specify a video with similar characteristics as the underlying
  /// input video.
  final String? fileInput;

  /// Specify one or more clips to use from your video overlay. When you include
  /// an input clip, you must also specify its start timecode, end timecode, or
  /// both start and end timecode.
  final List<VideoOverlayInputClipping>? inputClippings;

  /// Specify the timecode source for your video overlay input clips. To use the
  /// timecode present in your video overlay: Choose Embedded. To use a zerobased
  /// timecode: Choose Start at 0. To choose a timecode: Choose Specified start.
  /// When you do, enter the starting timecode in Start timecode. If you don't
  /// specify a value for Timecode source, MediaConvert uses Embedded by default.
  final InputTimecodeSource? timecodeSource;

  /// Specify the starting timecode for this video overlay. To use this setting,
  /// you must set Timecode source to Specified start.
  final String? timecodeStart;

  VideoOverlayInput({
    this.fileInput,
    this.inputClippings,
    this.timecodeSource,
    this.timecodeStart,
  });

  factory VideoOverlayInput.fromJson(Map<String, dynamic> json) {
    return VideoOverlayInput(
      fileInput: json['fileInput'] as String?,
      inputClippings: (json['inputClippings'] as List?)
          ?.nonNulls
          .map((e) =>
              VideoOverlayInputClipping.fromJson(e as Map<String, dynamic>))
          .toList(),
      timecodeSource: (json['timecodeSource'] as String?)
          ?.let(InputTimecodeSource.fromString),
      timecodeStart: json['timecodeStart'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final fileInput = this.fileInput;
    final inputClippings = this.inputClippings;
    final timecodeSource = this.timecodeSource;
    final timecodeStart = this.timecodeStart;
    return {
      if (fileInput != null) 'fileInput': fileInput,
      if (inputClippings != null) 'inputClippings': inputClippings,
      if (timecodeSource != null) 'timecodeSource': timecodeSource.value,
      if (timecodeStart != null) 'timecodeStart': timecodeStart,
    };
  }
}

/// To transcode only portions of your video overlay, include one input clip for
/// each part of your video overlay that you want in your output.
class VideoOverlayInputClipping {
  /// Specify the timecode of the last frame to include in your video overlay's
  /// clip. Use the format HH:MM:SS:FF or HH:MM:SS;FF, where HH is the hour, MM is
  /// the minute, SS is the second, and FF is the frame number. When entering this
  /// value, take into account your choice for Timecode source.
  final String? endTimecode;

  /// Specify the timecode of the first frame to include in your video overlay's
  /// clip. Use the format HH:MM:SS:FF or HH:MM:SS;FF, where HH is the hour, MM is
  /// the minute, SS is the second, and FF is the frame number. When entering this
  /// value, take into account your choice for Timecode source.
  final String? startTimecode;

  VideoOverlayInputClipping({
    this.endTimecode,
    this.startTimecode,
  });

  factory VideoOverlayInputClipping.fromJson(Map<String, dynamic> json) {
    return VideoOverlayInputClipping(
      endTimecode: json['endTimecode'] as String?,
      startTimecode: json['startTimecode'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final endTimecode = this.endTimecode;
    final startTimecode = this.startTimecode;
    return {
      if (endTimecode != null) 'endTimecode': endTimecode,
      if (startTimecode != null) 'startTimecode': startTimecode,
    };
  }
}

/// Find additional transcoding features under Preprocessors. Enable the
/// features at each output individually. These features are disabled by
/// default.
class VideoPreprocessor {
  /// Use these settings to convert the color space or to modify properties such
  /// as hue and contrast for this output. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/converting-the-color-space.html.
  final ColorCorrector? colorCorrector;

  /// Use the deinterlacer to produce smoother motion and a clearer picture. For
  /// more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/working-with-scan-type.html.
  final Deinterlacer? deinterlacer;

  /// Enable Dolby Vision feature to produce Dolby Vision compatible video output.
  final DolbyVision? dolbyVision;

  /// Enable HDR10+ analysis and metadata injection. Compatible with HEVC only.
  final Hdr10Plus? hdr10Plus;

  /// Enable the Image inserter feature to include a graphic overlay on your
  /// video. Enable or disable this feature for each output individually. This
  /// setting is disabled by default.
  final ImageInserter? imageInserter;

  /// Enable the Noise reducer feature to remove noise from your video output if
  /// necessary. Enable or disable this feature for each output individually. This
  /// setting is disabled by default. When you enable Noise reducer, you must also
  /// select a value for Noise reducer filter. For AVC outputs, when you include
  /// Noise reducer, you cannot include the Bandwidth reduction filter.
  final NoiseReducer? noiseReducer;

  /// If you work with a third party video watermarking partner, use the group of
  /// settings that correspond with your watermarking partner to include
  /// watermarks in your output.
  final PartnerWatermarking? partnerWatermarking;

  /// Settings for burning the output timecode and specified prefix into the
  /// output.
  final TimecodeBurnin? timecodeBurnin;

  VideoPreprocessor({
    this.colorCorrector,
    this.deinterlacer,
    this.dolbyVision,
    this.hdr10Plus,
    this.imageInserter,
    this.noiseReducer,
    this.partnerWatermarking,
    this.timecodeBurnin,
  });

  factory VideoPreprocessor.fromJson(Map<String, dynamic> json) {
    return VideoPreprocessor(
      colorCorrector: json['colorCorrector'] != null
          ? ColorCorrector.fromJson(
              json['colorCorrector'] as Map<String, dynamic>)
          : null,
      deinterlacer: json['deinterlacer'] != null
          ? Deinterlacer.fromJson(json['deinterlacer'] as Map<String, dynamic>)
          : null,
      dolbyVision: json['dolbyVision'] != null
          ? DolbyVision.fromJson(json['dolbyVision'] as Map<String, dynamic>)
          : null,
      hdr10Plus: json['hdr10Plus'] != null
          ? Hdr10Plus.fromJson(json['hdr10Plus'] as Map<String, dynamic>)
          : null,
      imageInserter: json['imageInserter'] != null
          ? ImageInserter.fromJson(
              json['imageInserter'] as Map<String, dynamic>)
          : null,
      noiseReducer: json['noiseReducer'] != null
          ? NoiseReducer.fromJson(json['noiseReducer'] as Map<String, dynamic>)
          : null,
      partnerWatermarking: json['partnerWatermarking'] != null
          ? PartnerWatermarking.fromJson(
              json['partnerWatermarking'] as Map<String, dynamic>)
          : null,
      timecodeBurnin: json['timecodeBurnin'] != null
          ? TimecodeBurnin.fromJson(
              json['timecodeBurnin'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final colorCorrector = this.colorCorrector;
    final deinterlacer = this.deinterlacer;
    final dolbyVision = this.dolbyVision;
    final hdr10Plus = this.hdr10Plus;
    final imageInserter = this.imageInserter;
    final noiseReducer = this.noiseReducer;
    final partnerWatermarking = this.partnerWatermarking;
    final timecodeBurnin = this.timecodeBurnin;
    return {
      if (colorCorrector != null) 'colorCorrector': colorCorrector,
      if (deinterlacer != null) 'deinterlacer': deinterlacer,
      if (dolbyVision != null) 'dolbyVision': dolbyVision,
      if (hdr10Plus != null) 'hdr10Plus': hdr10Plus,
      if (imageInserter != null) 'imageInserter': imageInserter,
      if (noiseReducer != null) 'noiseReducer': noiseReducer,
      if (partnerWatermarking != null)
        'partnerWatermarking': partnerWatermarking,
      if (timecodeBurnin != null) 'timecodeBurnin': timecodeBurnin,
    };
  }
}

/// Input video selectors contain the video settings for the input. Each of your
/// inputs can have up to one video selector.
class VideoSelector {
  /// Ignore this setting unless this input is a QuickTime animation with an alpha
  /// channel. Use this setting to create separate Key and Fill outputs. In each
  /// output, specify which part of the input MediaConvert uses. Leave this
  /// setting at the default value DISCARD to delete the alpha channel and
  /// preserve the video. Set it to REMAP_TO_LUMA to delete the video and map the
  /// alpha channel to the luma channel of your outputs.
  final AlphaBehavior? alphaBehavior;

  /// If your input video has accurate color space metadata, or if you don't know
  /// about color space: Keep the default value, Follow. MediaConvert will
  /// automatically detect your input color space. If your input video has
  /// metadata indicating the wrong color space, or has missing metadata: Specify
  /// the accurate color space here. If your input video is HDR 10 and the SMPTE
  /// ST 2086 Mastering Display Color Volume static metadata isn't present in your
  /// video stream, or if that metadata is present but not accurate: Choose Force
  /// HDR 10. Specify correct values in the input HDR 10 metadata settings. For
  /// more information about HDR jobs, see
  /// https://docs.aws.amazon.com/console/mediaconvert/hdr. When you specify an
  /// input color space, MediaConvert uses the following color space metadata,
  /// which includes color primaries, transfer characteristics, and matrix
  /// coefficients:
  /// * HDR 10: BT.2020, PQ, BT.2020 non-constant
  /// * HLG 2020: BT.2020, HLG, BT.2020 non-constant
  /// * P3DCI (Theater): DCIP3, SMPTE 428M, BT.709
  /// * P3D65 (SDR): Display P3, sRGB, BT.709
  /// * P3D65 (HDR): Display P3, PQ, BT.709
  final ColorSpace? colorSpace;

  /// There are two sources for color metadata, the input file and the job input
  /// settings Color space and HDR master display information settings. The Color
  /// space usage setting determines which takes precedence. Choose Force to use
  /// color metadata from the input job settings. If you don't specify values for
  /// those settings, the service defaults to using metadata from your input.
  /// FALLBACK - Choose Fallback to use color metadata from the source when it is
  /// present. If there's no color metadata in your input file, the service
  /// defaults to using values you specify in the input settings.
  final ColorSpaceUsage? colorSpaceUsage;

  /// Set Embedded timecode override to Use MDPM when your AVCHD input contains
  /// timecode tag data in the Modified Digital Video Pack Metadata. When you do,
  /// we recommend you also set Timecode source to Embedded. Leave Embedded
  /// timecode override blank, or set to None, when your input does not contain
  /// MDPM timecode.
  final EmbeddedTimecodeOverride? embeddedTimecodeOverride;

  /// Use these settings to provide HDR 10 metadata that is missing or inaccurate
  /// in your input video. Appropriate values vary depending on the input video
  /// and must be provided by a color grader. The color grader generates these
  /// values during the HDR 10 mastering process. The valid range for each of
  /// these settings is 0 to 50,000. Each increment represents 0.00002 in CIE1931
  /// color coordinate. Related settings - When you specify these values, you must
  /// also set Color space to HDR 10. To specify whether the the values you
  /// specify here take precedence over the values in the metadata of your input
  /// file, set Color space usage. To specify whether color metadata is included
  /// in an output, set Color metadata. For more information about MediaConvert
  /// HDR jobs, see https://docs.aws.amazon.com/console/mediaconvert/hdr.
  final Hdr10Metadata? hdr10Metadata;

  /// Specify the maximum mastering display luminance. Enter an integer from 0 to
  /// 2147483647, in units of 0.0001 nits. For example, enter 10000000 for 1000
  /// nits.
  final int? maxLuminance;

  /// Use this setting if your input has video and audio durations that don't
  /// align, and your output or player has strict alignment requirements.
  /// Examples: Input audio track has a delayed start. Input video track ends
  /// before audio ends. When you set Pad video to Black, MediaConvert generates
  /// black video frames so that output video and audio durations match. Black
  /// video frames are added at the beginning or end, depending on your input. To
  /// keep the default behavior and not generate black video, set Pad video to
  /// Disabled or leave blank.
  final PadVideo? padVideo;

  /// Use PID to select specific video data from an input file. Specify this value
  /// as an integer; the system automatically converts it to the hexidecimal
  /// value. For example, 257 selects PID 0x101. A PID, or packet identifier, is
  /// an identifier for a set of data in an MPEG-2 transport stream container.
  final int? pid;

  /// Selects a specific program from within a multi-program transport stream.
  /// Note that Quad 4K is not currently supported.
  final int? programNumber;

  /// Use Rotate to specify how the service rotates your video. You can choose
  /// automatic rotation or specify a rotation. You can specify a clockwise
  /// rotation of 0, 90, 180, or 270 degrees. If your input video container is
  /// .mov or .mp4 and your input has rotation metadata, you can choose Automatic
  /// to have the service rotate your video according to the rotation specified in
  /// the metadata. The rotation must be within one degree of 90, 180, or 270
  /// degrees. If the rotation metadata specifies any other rotation, the service
  /// will default to no rotation. By default, the service does no rotation, even
  /// if your input video has rotation metadata. The service doesn't pass through
  /// rotation metadata.
  final InputRotate? rotate;

  /// If the sample range metadata in your input video is accurate, or if you
  /// don't know about sample range, keep the default value, Follow, for this
  /// setting. When you do, the service automatically detects your input sample
  /// range. If your input video has metadata indicating the wrong sample range,
  /// specify the accurate sample range here. When you do, MediaConvert ignores
  /// any sample range information in the input metadata. Regardless of whether
  /// MediaConvert uses the input sample range or the sample range that you
  /// specify, MediaConvert uses the sample range for transcoding and also writes
  /// it to the output metadata.
  final InputSampleRange? sampleRange;

  VideoSelector({
    this.alphaBehavior,
    this.colorSpace,
    this.colorSpaceUsage,
    this.embeddedTimecodeOverride,
    this.hdr10Metadata,
    this.maxLuminance,
    this.padVideo,
    this.pid,
    this.programNumber,
    this.rotate,
    this.sampleRange,
  });

  factory VideoSelector.fromJson(Map<String, dynamic> json) {
    return VideoSelector(
      alphaBehavior:
          (json['alphaBehavior'] as String?)?.let(AlphaBehavior.fromString),
      colorSpace: (json['colorSpace'] as String?)?.let(ColorSpace.fromString),
      colorSpaceUsage:
          (json['colorSpaceUsage'] as String?)?.let(ColorSpaceUsage.fromString),
      embeddedTimecodeOverride: (json['embeddedTimecodeOverride'] as String?)
          ?.let(EmbeddedTimecodeOverride.fromString),
      hdr10Metadata: json['hdr10Metadata'] != null
          ? Hdr10Metadata.fromJson(
              json['hdr10Metadata'] as Map<String, dynamic>)
          : null,
      maxLuminance: json['maxLuminance'] as int?,
      padVideo: (json['padVideo'] as String?)?.let(PadVideo.fromString),
      pid: json['pid'] as int?,
      programNumber: json['programNumber'] as int?,
      rotate: (json['rotate'] as String?)?.let(InputRotate.fromString),
      sampleRange:
          (json['sampleRange'] as String?)?.let(InputSampleRange.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final alphaBehavior = this.alphaBehavior;
    final colorSpace = this.colorSpace;
    final colorSpaceUsage = this.colorSpaceUsage;
    final embeddedTimecodeOverride = this.embeddedTimecodeOverride;
    final hdr10Metadata = this.hdr10Metadata;
    final maxLuminance = this.maxLuminance;
    final padVideo = this.padVideo;
    final pid = this.pid;
    final programNumber = this.programNumber;
    final rotate = this.rotate;
    final sampleRange = this.sampleRange;
    return {
      if (alphaBehavior != null) 'alphaBehavior': alphaBehavior.value,
      if (colorSpace != null) 'colorSpace': colorSpace.value,
      if (colorSpaceUsage != null) 'colorSpaceUsage': colorSpaceUsage.value,
      if (embeddedTimecodeOverride != null)
        'embeddedTimecodeOverride': embeddedTimecodeOverride.value,
      if (hdr10Metadata != null) 'hdr10Metadata': hdr10Metadata,
      if (maxLuminance != null) 'maxLuminance': maxLuminance,
      if (padVideo != null) 'padVideo': padVideo.value,
      if (pid != null) 'pid': pid,
      if (programNumber != null) 'programNumber': programNumber,
      if (rotate != null) 'rotate': rotate.value,
      if (sampleRange != null) 'sampleRange': sampleRange.value,
    };
  }
}

/// Applies only to H.264, H.265, MPEG2, and ProRes outputs. Only enable
/// Timecode insertion when the input frame rate is identical to the output
/// frame rate. To include timecodes in this output, set Timecode insertion to
/// PIC_TIMING_SEI. To leave them out, set it to DISABLED. Default is DISABLED.
/// When the service inserts timecodes in an output, by default, it uses any
/// embedded timecodes from the input. If none are present, the service will set
/// the timecode for the first output frame to zero. To change this default
/// behavior, adjust the settings under Timecode configuration. In the console,
/// these settings are located under Job > Job settings > Timecode
/// configuration. Note - Timecode source under input settings does not affect
/// the timecodes that are inserted in the output. Source under Job settings >
/// Timecode configuration does.
enum VideoTimecodeInsertion {
  disabled('DISABLED'),
  picTimingSei('PIC_TIMING_SEI'),
  ;

  final String value;

  const VideoTimecodeInsertion(this.value);

  static VideoTimecodeInsertion fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum VideoTimecodeInsertion'));
}

/// Required when you set Codec, under AudioDescriptions>CodecSettings, to the
/// value Vorbis.
class VorbisSettings {
  /// Optional. Specify the number of channels in this output audio track.
  /// Choosing Mono on the console gives you 1 output channel; choosing Stereo
  /// gives you 2. In the API, valid values are 1 and 2. The default value is 2.
  final int? channels;

  /// Optional. Specify the audio sample rate in Hz. Valid values are 22050,
  /// 32000, 44100, and 48000. The default value is 48000.
  final int? sampleRate;

  /// Optional. Specify the variable audio quality of this Vorbis output from -1
  /// (lowest quality, ~45 kbit/s) to 10 (highest quality, ~500 kbit/s). The
  /// default value is 4 (~128 kbit/s). Values 5 and 6 are approximately 160 and
  /// 192 kbit/s, respectively.
  final int? vbrQuality;

  VorbisSettings({
    this.channels,
    this.sampleRate,
    this.vbrQuality,
  });

  factory VorbisSettings.fromJson(Map<String, dynamic> json) {
    return VorbisSettings(
      channels: json['channels'] as int?,
      sampleRate: json['sampleRate'] as int?,
      vbrQuality: json['vbrQuality'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final channels = this.channels;
    final sampleRate = this.sampleRate;
    final vbrQuality = this.vbrQuality;
    return {
      if (channels != null) 'channels': channels,
      if (sampleRate != null) 'sampleRate': sampleRate,
      if (vbrQuality != null) 'vbrQuality': vbrQuality,
    };
  }
}

/// If you are using the console, use the Framerate setting to specify the frame
/// rate for this output. If you want to keep the same frame rate as the input
/// video, choose Follow source. If you want to do frame rate conversion, choose
/// a frame rate from the dropdown list or choose Custom. The framerates shown
/// in the dropdown list are decimal approximations of fractions. If you choose
/// Custom, specify your frame rate as a fraction.
enum Vp8FramerateControl {
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  specified('SPECIFIED'),
  ;

  final String value;

  const Vp8FramerateControl(this.value);

  static Vp8FramerateControl fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Vp8FramerateControl'));
}

/// Choose the method that you want MediaConvert to use when increasing or
/// decreasing the frame rate. For numerically simple conversions, such as 60
/// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
/// For numerically complex conversions, to avoid stutter: Choose Interpolate.
/// This results in a smooth picture, but might introduce undesirable video
/// artifacts. For complex frame rate conversions, especially if your source
/// video has already been converted from its original cadence: Choose
/// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
/// best conversion method frame by frame. Note that using FrameFormer increases
/// the transcoding time and incurs a significant add-on cost. When you choose
/// FrameFormer, your input video resolution must be at least 128x96.
enum Vp8FramerateConversionAlgorithm {
  duplicateDrop('DUPLICATE_DROP'),
  interpolate('INTERPOLATE'),
  frameformer('FRAMEFORMER'),
  ;

  final String value;

  const Vp8FramerateConversionAlgorithm(this.value);

  static Vp8FramerateConversionAlgorithm fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Vp8FramerateConversionAlgorithm'));
}

/// Optional. Specify how the service determines the pixel aspect ratio (PAR)
/// for this output. The default behavior, Follow source, uses the PAR from your
/// input video for your output. To specify a different PAR in the console,
/// choose any value other than Follow source. When you choose SPECIFIED for
/// this setting, you must also specify values for the parNumerator and
/// parDenominator settings.
enum Vp8ParControl {
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  specified('SPECIFIED'),
  ;

  final String value;

  const Vp8ParControl(this.value);

  static Vp8ParControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Vp8ParControl'));
}

/// Optional. Use Quality tuning level to choose how you want to trade off
/// encoding speed for output video quality. The default behavior is faster,
/// lower quality, multi-pass encoding.
enum Vp8QualityTuningLevel {
  multiPass('MULTI_PASS'),
  multiPassHq('MULTI_PASS_HQ'),
  ;

  final String value;

  const Vp8QualityTuningLevel(this.value);

  static Vp8QualityTuningLevel fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Vp8QualityTuningLevel'));
}

/// With the VP8 codec, you can use only the variable bitrate (VBR) rate control
/// mode.
enum Vp8RateControlMode {
  vbr('VBR'),
  ;

  final String value;

  const Vp8RateControlMode(this.value);

  static Vp8RateControlMode fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Vp8RateControlMode'));
}

/// Required when you set Codec to the value VP8.
class Vp8Settings {
  /// Target bitrate in bits/second. For example, enter five megabits per second
  /// as 5000000.
  final int? bitrate;

  /// If you are using the console, use the Framerate setting to specify the frame
  /// rate for this output. If you want to keep the same frame rate as the input
  /// video, choose Follow source. If you want to do frame rate conversion, choose
  /// a frame rate from the dropdown list or choose Custom. The framerates shown
  /// in the dropdown list are decimal approximations of fractions. If you choose
  /// Custom, specify your frame rate as a fraction.
  final Vp8FramerateControl? framerateControl;

  /// Choose the method that you want MediaConvert to use when increasing or
  /// decreasing the frame rate. For numerically simple conversions, such as 60
  /// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
  /// For numerically complex conversions, to avoid stutter: Choose Interpolate.
  /// This results in a smooth picture, but might introduce undesirable video
  /// artifacts. For complex frame rate conversions, especially if your source
  /// video has already been converted from its original cadence: Choose
  /// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
  /// best conversion method frame by frame. Note that using FrameFormer increases
  /// the transcoding time and incurs a significant add-on cost. When you choose
  /// FrameFormer, your input video resolution must be at least 128x96.
  final Vp8FramerateConversionAlgorithm? framerateConversionAlgorithm;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateDenominator to specify the denominator of this fraction.
  /// In this example, use 1001 for the value of FramerateDenominator. When you
  /// use the console for transcode jobs that use frame rate conversion, provide
  /// the value as a decimal number for Framerate. In this example, specify
  /// 23.976.
  final int? framerateDenominator;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateNumerator to specify the numerator of this fraction. In
  /// this example, use 24000 for the value of FramerateNumerator. When you use
  /// the console for transcode jobs that use frame rate conversion, provide the
  /// value as a decimal number for Framerate. In this example, specify 23.976.
  final int? framerateNumerator;

  /// GOP Length (keyframe interval) in frames. Must be greater than zero.
  final double? gopSize;

  /// Optional. Size of buffer (HRD buffer model) in bits. For example, enter five
  /// megabits as 5000000.
  final int? hrdBufferSize;

  /// Ignore this setting unless you set qualityTuningLevel to MULTI_PASS.
  /// Optional. Specify the maximum bitrate in bits/second. For example, enter
  /// five megabits per second as 5000000. The default behavior uses twice the
  /// target bitrate as the maximum bitrate.
  final int? maxBitrate;

  /// Optional. Specify how the service determines the pixel aspect ratio (PAR)
  /// for this output. The default behavior, Follow source, uses the PAR from your
  /// input video for your output. To specify a different PAR in the console,
  /// choose any value other than Follow source. When you choose SPECIFIED for
  /// this setting, you must also specify values for the parNumerator and
  /// parDenominator settings.
  final Vp8ParControl? parControl;

  /// Required when you set Pixel aspect ratio to SPECIFIED. On the console, this
  /// corresponds to any value other than Follow source. When you specify an
  /// output pixel aspect ratio (PAR) that is different from your input video PAR,
  /// provide your output PAR as a ratio. For example, for D1/DV NTSC widescreen,
  /// you would specify the ratio 40:33. In this example, the value for
  /// parDenominator is 33.
  final int? parDenominator;

  /// Required when you set Pixel aspect ratio to SPECIFIED. On the console, this
  /// corresponds to any value other than Follow source. When you specify an
  /// output pixel aspect ratio (PAR) that is different from your input video PAR,
  /// provide your output PAR as a ratio. For example, for D1/DV NTSC widescreen,
  /// you would specify the ratio 40:33. In this example, the value for
  /// parNumerator is 40.
  final int? parNumerator;

  /// Optional. Use Quality tuning level to choose how you want to trade off
  /// encoding speed for output video quality. The default behavior is faster,
  /// lower quality, multi-pass encoding.
  final Vp8QualityTuningLevel? qualityTuningLevel;

  /// With the VP8 codec, you can use only the variable bitrate (VBR) rate control
  /// mode.
  final Vp8RateControlMode? rateControlMode;

  Vp8Settings({
    this.bitrate,
    this.framerateControl,
    this.framerateConversionAlgorithm,
    this.framerateDenominator,
    this.framerateNumerator,
    this.gopSize,
    this.hrdBufferSize,
    this.maxBitrate,
    this.parControl,
    this.parDenominator,
    this.parNumerator,
    this.qualityTuningLevel,
    this.rateControlMode,
  });

  factory Vp8Settings.fromJson(Map<String, dynamic> json) {
    return Vp8Settings(
      bitrate: json['bitrate'] as int?,
      framerateControl: (json['framerateControl'] as String?)
          ?.let(Vp8FramerateControl.fromString),
      framerateConversionAlgorithm:
          (json['framerateConversionAlgorithm'] as String?)
              ?.let(Vp8FramerateConversionAlgorithm.fromString),
      framerateDenominator: json['framerateDenominator'] as int?,
      framerateNumerator: json['framerateNumerator'] as int?,
      gopSize: json['gopSize'] as double?,
      hrdBufferSize: json['hrdBufferSize'] as int?,
      maxBitrate: json['maxBitrate'] as int?,
      parControl:
          (json['parControl'] as String?)?.let(Vp8ParControl.fromString),
      parDenominator: json['parDenominator'] as int?,
      parNumerator: json['parNumerator'] as int?,
      qualityTuningLevel: (json['qualityTuningLevel'] as String?)
          ?.let(Vp8QualityTuningLevel.fromString),
      rateControlMode: (json['rateControlMode'] as String?)
          ?.let(Vp8RateControlMode.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final bitrate = this.bitrate;
    final framerateControl = this.framerateControl;
    final framerateConversionAlgorithm = this.framerateConversionAlgorithm;
    final framerateDenominator = this.framerateDenominator;
    final framerateNumerator = this.framerateNumerator;
    final gopSize = this.gopSize;
    final hrdBufferSize = this.hrdBufferSize;
    final maxBitrate = this.maxBitrate;
    final parControl = this.parControl;
    final parDenominator = this.parDenominator;
    final parNumerator = this.parNumerator;
    final qualityTuningLevel = this.qualityTuningLevel;
    final rateControlMode = this.rateControlMode;
    return {
      if (bitrate != null) 'bitrate': bitrate,
      if (framerateControl != null) 'framerateControl': framerateControl.value,
      if (framerateConversionAlgorithm != null)
        'framerateConversionAlgorithm': framerateConversionAlgorithm.value,
      if (framerateDenominator != null)
        'framerateDenominator': framerateDenominator,
      if (framerateNumerator != null) 'framerateNumerator': framerateNumerator,
      if (gopSize != null) 'gopSize': gopSize,
      if (hrdBufferSize != null) 'hrdBufferSize': hrdBufferSize,
      if (maxBitrate != null) 'maxBitrate': maxBitrate,
      if (parControl != null) 'parControl': parControl.value,
      if (parDenominator != null) 'parDenominator': parDenominator,
      if (parNumerator != null) 'parNumerator': parNumerator,
      if (qualityTuningLevel != null)
        'qualityTuningLevel': qualityTuningLevel.value,
      if (rateControlMode != null) 'rateControlMode': rateControlMode.value,
    };
  }
}

/// If you are using the console, use the Framerate setting to specify the frame
/// rate for this output. If you want to keep the same frame rate as the input
/// video, choose Follow source. If you want to do frame rate conversion, choose
/// a frame rate from the dropdown list or choose Custom. The framerates shown
/// in the dropdown list are decimal approximations of fractions. If you choose
/// Custom, specify your frame rate as a fraction.
enum Vp9FramerateControl {
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  specified('SPECIFIED'),
  ;

  final String value;

  const Vp9FramerateControl(this.value);

  static Vp9FramerateControl fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Vp9FramerateControl'));
}

/// Choose the method that you want MediaConvert to use when increasing or
/// decreasing the frame rate. For numerically simple conversions, such as 60
/// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
/// For numerically complex conversions, to avoid stutter: Choose Interpolate.
/// This results in a smooth picture, but might introduce undesirable video
/// artifacts. For complex frame rate conversions, especially if your source
/// video has already been converted from its original cadence: Choose
/// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
/// best conversion method frame by frame. Note that using FrameFormer increases
/// the transcoding time and incurs a significant add-on cost. When you choose
/// FrameFormer, your input video resolution must be at least 128x96.
enum Vp9FramerateConversionAlgorithm {
  duplicateDrop('DUPLICATE_DROP'),
  interpolate('INTERPOLATE'),
  frameformer('FRAMEFORMER'),
  ;

  final String value;

  const Vp9FramerateConversionAlgorithm(this.value);

  static Vp9FramerateConversionAlgorithm fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Vp9FramerateConversionAlgorithm'));
}

/// Optional. Specify how the service determines the pixel aspect ratio (PAR)
/// for this output. The default behavior, Follow source, uses the PAR from your
/// input video for your output. To specify a different PAR in the console,
/// choose any value other than Follow source. When you choose SPECIFIED for
/// this setting, you must also specify values for the parNumerator and
/// parDenominator settings.
enum Vp9ParControl {
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  specified('SPECIFIED'),
  ;

  final String value;

  const Vp9ParControl(this.value);

  static Vp9ParControl fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum Vp9ParControl'));
}

/// Optional. Use Quality tuning level to choose how you want to trade off
/// encoding speed for output video quality. The default behavior is faster,
/// lower quality, multi-pass encoding.
enum Vp9QualityTuningLevel {
  multiPass('MULTI_PASS'),
  multiPassHq('MULTI_PASS_HQ'),
  ;

  final String value;

  const Vp9QualityTuningLevel(this.value);

  static Vp9QualityTuningLevel fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Vp9QualityTuningLevel'));
}

/// With the VP9 codec, you can use only the variable bitrate (VBR) rate control
/// mode.
enum Vp9RateControlMode {
  vbr('VBR'),
  ;

  final String value;

  const Vp9RateControlMode(this.value);

  static Vp9RateControlMode fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum Vp9RateControlMode'));
}

/// Required when you set Codec to the value VP9.
class Vp9Settings {
  /// Target bitrate in bits/second. For example, enter five megabits per second
  /// as 5000000.
  final int? bitrate;

  /// If you are using the console, use the Framerate setting to specify the frame
  /// rate for this output. If you want to keep the same frame rate as the input
  /// video, choose Follow source. If you want to do frame rate conversion, choose
  /// a frame rate from the dropdown list or choose Custom. The framerates shown
  /// in the dropdown list are decimal approximations of fractions. If you choose
  /// Custom, specify your frame rate as a fraction.
  final Vp9FramerateControl? framerateControl;

  /// Choose the method that you want MediaConvert to use when increasing or
  /// decreasing the frame rate. For numerically simple conversions, such as 60
  /// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
  /// For numerically complex conversions, to avoid stutter: Choose Interpolate.
  /// This results in a smooth picture, but might introduce undesirable video
  /// artifacts. For complex frame rate conversions, especially if your source
  /// video has already been converted from its original cadence: Choose
  /// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
  /// best conversion method frame by frame. Note that using FrameFormer increases
  /// the transcoding time and incurs a significant add-on cost. When you choose
  /// FrameFormer, your input video resolution must be at least 128x96.
  final Vp9FramerateConversionAlgorithm? framerateConversionAlgorithm;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateDenominator to specify the denominator of this fraction.
  /// In this example, use 1001 for the value of FramerateDenominator. When you
  /// use the console for transcode jobs that use frame rate conversion, provide
  /// the value as a decimal number for Framerate. In this example, specify
  /// 23.976.
  final int? framerateDenominator;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateNumerator to specify the numerator of this fraction. In
  /// this example, use 24000 for the value of FramerateNumerator. When you use
  /// the console for transcode jobs that use frame rate conversion, provide the
  /// value as a decimal number for Framerate. In this example, specify 23.976.
  final int? framerateNumerator;

  /// GOP Length (keyframe interval) in frames. Must be greater than zero.
  final double? gopSize;

  /// Size of buffer (HRD buffer model) in bits. For example, enter five megabits
  /// as 5000000.
  final int? hrdBufferSize;

  /// Ignore this setting unless you set qualityTuningLevel to MULTI_PASS.
  /// Optional. Specify the maximum bitrate in bits/second. For example, enter
  /// five megabits per second as 5000000. The default behavior uses twice the
  /// target bitrate as the maximum bitrate.
  final int? maxBitrate;

  /// Optional. Specify how the service determines the pixel aspect ratio for this
  /// output. The default behavior is to use the same pixel aspect ratio as your
  /// input video.
  final Vp9ParControl? parControl;

  /// Required when you set Pixel aspect ratio to SPECIFIED. On the console, this
  /// corresponds to any value other than Follow source. When you specify an
  /// output pixel aspect ratio (PAR) that is different from your input video PAR,
  /// provide your output PAR as a ratio. For example, for D1/DV NTSC widescreen,
  /// you would specify the ratio 40:33. In this example, the value for
  /// parDenominator is 33.
  final int? parDenominator;

  /// Required when you set Pixel aspect ratio to SPECIFIED. On the console, this
  /// corresponds to any value other than Follow source. When you specify an
  /// output pixel aspect ratio (PAR) that is different from your input video PAR,
  /// provide your output PAR as a ratio. For example, for D1/DV NTSC widescreen,
  /// you would specify the ratio 40:33. In this example, the value for
  /// parNumerator is 40.
  final int? parNumerator;

  /// Optional. Use Quality tuning level to choose how you want to trade off
  /// encoding speed for output video quality. The default behavior is faster,
  /// lower quality, multi-pass encoding.
  final Vp9QualityTuningLevel? qualityTuningLevel;

  /// With the VP9 codec, you can use only the variable bitrate (VBR) rate control
  /// mode.
  final Vp9RateControlMode? rateControlMode;

  Vp9Settings({
    this.bitrate,
    this.framerateControl,
    this.framerateConversionAlgorithm,
    this.framerateDenominator,
    this.framerateNumerator,
    this.gopSize,
    this.hrdBufferSize,
    this.maxBitrate,
    this.parControl,
    this.parDenominator,
    this.parNumerator,
    this.qualityTuningLevel,
    this.rateControlMode,
  });

  factory Vp9Settings.fromJson(Map<String, dynamic> json) {
    return Vp9Settings(
      bitrate: json['bitrate'] as int?,
      framerateControl: (json['framerateControl'] as String?)
          ?.let(Vp9FramerateControl.fromString),
      framerateConversionAlgorithm:
          (json['framerateConversionAlgorithm'] as String?)
              ?.let(Vp9FramerateConversionAlgorithm.fromString),
      framerateDenominator: json['framerateDenominator'] as int?,
      framerateNumerator: json['framerateNumerator'] as int?,
      gopSize: json['gopSize'] as double?,
      hrdBufferSize: json['hrdBufferSize'] as int?,
      maxBitrate: json['maxBitrate'] as int?,
      parControl:
          (json['parControl'] as String?)?.let(Vp9ParControl.fromString),
      parDenominator: json['parDenominator'] as int?,
      parNumerator: json['parNumerator'] as int?,
      qualityTuningLevel: (json['qualityTuningLevel'] as String?)
          ?.let(Vp9QualityTuningLevel.fromString),
      rateControlMode: (json['rateControlMode'] as String?)
          ?.let(Vp9RateControlMode.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final bitrate = this.bitrate;
    final framerateControl = this.framerateControl;
    final framerateConversionAlgorithm = this.framerateConversionAlgorithm;
    final framerateDenominator = this.framerateDenominator;
    final framerateNumerator = this.framerateNumerator;
    final gopSize = this.gopSize;
    final hrdBufferSize = this.hrdBufferSize;
    final maxBitrate = this.maxBitrate;
    final parControl = this.parControl;
    final parDenominator = this.parDenominator;
    final parNumerator = this.parNumerator;
    final qualityTuningLevel = this.qualityTuningLevel;
    final rateControlMode = this.rateControlMode;
    return {
      if (bitrate != null) 'bitrate': bitrate,
      if (framerateControl != null) 'framerateControl': framerateControl.value,
      if (framerateConversionAlgorithm != null)
        'framerateConversionAlgorithm': framerateConversionAlgorithm.value,
      if (framerateDenominator != null)
        'framerateDenominator': framerateDenominator,
      if (framerateNumerator != null) 'framerateNumerator': framerateNumerator,
      if (gopSize != null) 'gopSize': gopSize,
      if (hrdBufferSize != null) 'hrdBufferSize': hrdBufferSize,
      if (maxBitrate != null) 'maxBitrate': maxBitrate,
      if (parControl != null) 'parControl': parControl.value,
      if (parDenominator != null) 'parDenominator': parDenominator,
      if (parNumerator != null) 'parNumerator': parNumerator,
      if (qualityTuningLevel != null)
        'qualityTuningLevel': qualityTuningLevel.value,
      if (rateControlMode != null) 'rateControlMode': rateControlMode.value,
    };
  }
}

/// Contains any warning codes and their count for the job.
class WarningGroup {
  /// Warning code that identifies a specific warning in the job. For more
  /// information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/warning_codes.html
  final int code;

  /// The number of times this warning occurred in the job.
  final int count;

  WarningGroup({
    required this.code,
    required this.count,
  });

  factory WarningGroup.fromJson(Map<String, dynamic> json) {
    return WarningGroup(
      code: json['code'] as int,
      count: json['count'] as int,
    );
  }

  Map<String, dynamic> toJson() {
    final code = this.code;
    final count = this.count;
    return {
      'code': code,
      'count': count,
    };
  }
}

/// Optional. Ignore this setting unless Nagra support directs you to specify a
/// value. When you don't specify a value here, the Nagra NexGuard library uses
/// its default value.
enum WatermarkingStrength {
  lightest('LIGHTEST'),
  lighter('LIGHTER'),
  $default('DEFAULT'),
  stronger('STRONGER'),
  strongest('STRONGEST'),
  ;

  final String value;

  const WatermarkingStrength(this.value);

  static WatermarkingStrength fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum WatermarkingStrength'));
}

/// The service defaults to using RIFF for WAV outputs. If your output audio is
/// likely to exceed 4 GB in file size, or if you otherwise need the extended
/// support of the RF64 format, set your output WAV file format to RF64.
enum WavFormat {
  riff('RIFF'),
  rf64('RF64'),
  ;

  final String value;

  const WavFormat(this.value);

  static WavFormat fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum WavFormat'));
}

/// Required when you set Codec to the value WAV.
class WavSettings {
  /// Specify Bit depth, in bits per sample, to choose the encoding quality for
  /// this audio track.
  final int? bitDepth;

  /// Specify the number of channels in this output audio track. Valid values are
  /// 1 and even numbers up to 64. For example, 1, 2, 4, 6, and so on, up to 64.
  final int? channels;

  /// The service defaults to using RIFF for WAV outputs. If your output audio is
  /// likely to exceed 4 GB in file size, or if you otherwise need the extended
  /// support of the RF64 format, set your output WAV file format to RF64.
  final WavFormat? format;

  /// Sample rate in Hz.
  final int? sampleRate;

  WavSettings({
    this.bitDepth,
    this.channels,
    this.format,
    this.sampleRate,
  });

  factory WavSettings.fromJson(Map<String, dynamic> json) {
    return WavSettings(
      bitDepth: json['bitDepth'] as int?,
      channels: json['channels'] as int?,
      format: (json['format'] as String?)?.let(WavFormat.fromString),
      sampleRate: json['sampleRate'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final bitDepth = this.bitDepth;
    final channels = this.channels;
    final format = this.format;
    final sampleRate = this.sampleRate;
    return {
      if (bitDepth != null) 'bitDepth': bitDepth,
      if (channels != null) 'channels': channels,
      if (format != null) 'format': format.value,
      if (sampleRate != null) 'sampleRate': sampleRate,
    };
  }
}

/// If the WebVTT captions track is intended to provide accessibility for people
/// who are deaf or hard of hearing: Set Accessibility subtitles to Enabled.
/// When you do, MediaConvert adds accessibility attributes to your output HLS
/// or DASH manifest. For HLS manifests, MediaConvert adds the following
/// accessibility attributes under EXT-X-MEDIA for this track:
/// CHARACTERISTICS="public.accessibility.describes-spoken-dialog,public.accessibility.describes-music-and-sound"
/// and AUTOSELECT="YES". For DASH manifests, MediaConvert adds the following in
/// the adaptation set for this track: <Accessibility
/// schemeIdUri="urn:mpeg:dash:role:2011" value="caption"/>. If the captions
/// track is not intended to provide such accessibility: Keep the default value,
/// Disabled. When you do, for DASH manifests, MediaConvert instead adds the
/// following in the adaptation set for this track: <Role
/// schemeIDUri="urn:mpeg:dash:role:2011" value="subtitle"/>.
enum WebvttAccessibilitySubs {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const WebvttAccessibilitySubs(this.value);

  static WebvttAccessibilitySubs fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum WebvttAccessibilitySubs'));
}

/// Settings related to WebVTT captions. WebVTT is a sidecar format that holds
/// captions in a file that is separate from the video container. Set up sidecar
/// captions in the same output group, but different output from your video. For
/// more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/ttml-and-webvtt-output-captions.html.
class WebvttDestinationSettings {
  /// If the WebVTT captions track is intended to provide accessibility for people
  /// who are deaf or hard of hearing: Set Accessibility subtitles to Enabled.
  /// When you do, MediaConvert adds accessibility attributes to your output HLS
  /// or DASH manifest. For HLS manifests, MediaConvert adds the following
  /// accessibility attributes under EXT-X-MEDIA for this track:
  /// CHARACTERISTICS="public.accessibility.describes-spoken-dialog,public.accessibility.describes-music-and-sound"
  /// and AUTOSELECT="YES". For DASH manifests, MediaConvert adds the following in
  /// the adaptation set for this track: <Accessibility
  /// schemeIdUri="urn:mpeg:dash:role:2011" value="caption"/>. If the captions
  /// track is not intended to provide such accessibility: Keep the default value,
  /// Disabled. When you do, for DASH manifests, MediaConvert instead adds the
  /// following in the adaptation set for this track: <Role
  /// schemeIDUri="urn:mpeg:dash:role:2011" value="subtitle"/>.
  final WebvttAccessibilitySubs? accessibility;

  /// To use the available style, color, and position information from your input
  /// captions: Set Style passthrough to Enabled. MediaConvert uses default
  /// settings when style and position information is missing from your input
  /// captions. To recreate the input captions exactly: Set Style passthrough to
  /// Strict. MediaConvert automatically applies timing adjustments, including
  /// adjustments for frame rate conversion, ad avails, and input clipping. Your
  /// input captions format must be WebVTT. To ignore the style and position
  /// information from your input captions and use simplified output captions: Set
  /// Style passthrough to Disabled, or leave blank.
  final WebvttStylePassthrough? stylePassthrough;

  WebvttDestinationSettings({
    this.accessibility,
    this.stylePassthrough,
  });

  factory WebvttDestinationSettings.fromJson(Map<String, dynamic> json) {
    return WebvttDestinationSettings(
      accessibility: (json['accessibility'] as String?)
          ?.let(WebvttAccessibilitySubs.fromString),
      stylePassthrough: (json['stylePassthrough'] as String?)
          ?.let(WebvttStylePassthrough.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final accessibility = this.accessibility;
    final stylePassthrough = this.stylePassthrough;
    return {
      if (accessibility != null) 'accessibility': accessibility.value,
      if (stylePassthrough != null) 'stylePassthrough': stylePassthrough.value,
    };
  }
}

/// Settings specific to WebVTT sources in HLS alternative rendition group.
/// Specify the properties (renditionGroupId, renditionName or
/// renditionLanguageCode) to identify the unique subtitle track among the
/// alternative rendition groups present in the HLS manifest. If no unique track
/// is found, or multiple tracks match the specified properties, the job fails.
/// If there is only one subtitle track in the rendition group, the settings can
/// be left empty and the default subtitle track will be chosen. If your caption
/// source is a sidecar file, use FileSourceSettings instead of
/// WebvttHlsSourceSettings.
class WebvttHlsSourceSettings {
  /// Optional. Specify alternative group ID
  final String? renditionGroupId;

  /// Optional. Specify ISO 639-2 or ISO 639-3 code in the language property
  final LanguageCode? renditionLanguageCode;

  /// Optional. Specify media name
  final String? renditionName;

  WebvttHlsSourceSettings({
    this.renditionGroupId,
    this.renditionLanguageCode,
    this.renditionName,
  });

  factory WebvttHlsSourceSettings.fromJson(Map<String, dynamic> json) {
    return WebvttHlsSourceSettings(
      renditionGroupId: json['renditionGroupId'] as String?,
      renditionLanguageCode: (json['renditionLanguageCode'] as String?)
          ?.let(LanguageCode.fromString),
      renditionName: json['renditionName'] as String?,
    );
  }

  Map<String, dynamic> toJson() {
    final renditionGroupId = this.renditionGroupId;
    final renditionLanguageCode = this.renditionLanguageCode;
    final renditionName = this.renditionName;
    return {
      if (renditionGroupId != null) 'renditionGroupId': renditionGroupId,
      if (renditionLanguageCode != null)
        'renditionLanguageCode': renditionLanguageCode.value,
      if (renditionName != null) 'renditionName': renditionName,
    };
  }
}

/// To use the available style, color, and position information from your input
/// captions: Set Style passthrough to Enabled. MediaConvert uses default
/// settings when style and position information is missing from your input
/// captions. To recreate the input captions exactly: Set Style passthrough to
/// Strict. MediaConvert automatically applies timing adjustments, including
/// adjustments for frame rate conversion, ad avails, and input clipping. Your
/// input captions format must be WebVTT. To ignore the style and position
/// information from your input captions and use simplified output captions: Set
/// Style passthrough to Disabled, or leave blank.
enum WebvttStylePassthrough {
  enabled('ENABLED'),
  disabled('DISABLED'),
  strict('STRICT'),
  ;

  final String value;

  const WebvttStylePassthrough(this.value);

  static WebvttStylePassthrough fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum WebvttStylePassthrough'));
}

/// Specify the XAVC Intra 4k (CBG) Class to set the bitrate of your output.
/// Outputs of the same class have similar image quality over the operating
/// points that are valid for that class.
enum Xavc4kIntraCbgProfileClass {
  class_100('CLASS_100'),
  class_300('CLASS_300'),
  class_480('CLASS_480'),
  ;

  final String value;

  const Xavc4kIntraCbgProfileClass(this.value);

  static Xavc4kIntraCbgProfileClass fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Xavc4kIntraCbgProfileClass'));
}

/// Required when you set Profile to the value XAVC_4K_INTRA_CBG.
class Xavc4kIntraCbgProfileSettings {
  /// Specify the XAVC Intra 4k (CBG) Class to set the bitrate of your output.
  /// Outputs of the same class have similar image quality over the operating
  /// points that are valid for that class.
  final Xavc4kIntraCbgProfileClass? xavcClass;

  Xavc4kIntraCbgProfileSettings({
    this.xavcClass,
  });

  factory Xavc4kIntraCbgProfileSettings.fromJson(Map<String, dynamic> json) {
    return Xavc4kIntraCbgProfileSettings(
      xavcClass: (json['xavcClass'] as String?)
          ?.let(Xavc4kIntraCbgProfileClass.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final xavcClass = this.xavcClass;
    return {
      if (xavcClass != null) 'xavcClass': xavcClass.value,
    };
  }
}

/// Specify the XAVC Intra 4k (VBR) Class to set the bitrate of your output.
/// Outputs of the same class have similar image quality over the operating
/// points that are valid for that class.
enum Xavc4kIntraVbrProfileClass {
  class_100('CLASS_100'),
  class_300('CLASS_300'),
  class_480('CLASS_480'),
  ;

  final String value;

  const Xavc4kIntraVbrProfileClass(this.value);

  static Xavc4kIntraVbrProfileClass fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Xavc4kIntraVbrProfileClass'));
}

/// Required when you set Profile to the value XAVC_4K_INTRA_VBR.
class Xavc4kIntraVbrProfileSettings {
  /// Specify the XAVC Intra 4k (VBR) Class to set the bitrate of your output.
  /// Outputs of the same class have similar image quality over the operating
  /// points that are valid for that class.
  final Xavc4kIntraVbrProfileClass? xavcClass;

  Xavc4kIntraVbrProfileSettings({
    this.xavcClass,
  });

  factory Xavc4kIntraVbrProfileSettings.fromJson(Map<String, dynamic> json) {
    return Xavc4kIntraVbrProfileSettings(
      xavcClass: (json['xavcClass'] as String?)
          ?.let(Xavc4kIntraVbrProfileClass.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final xavcClass = this.xavcClass;
    return {
      if (xavcClass != null) 'xavcClass': xavcClass.value,
    };
  }
}

/// Specify the XAVC 4k (Long GOP) Bitrate Class to set the bitrate of your
/// output. Outputs of the same class have similar image quality over the
/// operating points that are valid for that class.
enum Xavc4kProfileBitrateClass {
  bitrateClass_100('BITRATE_CLASS_100'),
  bitrateClass_140('BITRATE_CLASS_140'),
  bitrateClass_200('BITRATE_CLASS_200'),
  ;

  final String value;

  const Xavc4kProfileBitrateClass(this.value);

  static Xavc4kProfileBitrateClass fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Xavc4kProfileBitrateClass'));
}

/// Specify the codec profile for this output. Choose High, 8-bit, 4:2:0 (HIGH)
/// or High, 10-bit, 4:2:2 (HIGH_422). These profiles are specified in ITU-T
/// H.264.
enum Xavc4kProfileCodecProfile {
  high('HIGH'),
  high_422('HIGH_422'),
  ;

  final String value;

  const Xavc4kProfileCodecProfile(this.value);

  static Xavc4kProfileCodecProfile fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Xavc4kProfileCodecProfile'));
}

/// Optional. Use Quality tuning level to choose how you want to trade off
/// encoding speed for output video quality. The default behavior is faster,
/// lower quality, single-pass encoding.
enum Xavc4kProfileQualityTuningLevel {
  singlePass('SINGLE_PASS'),
  singlePassHq('SINGLE_PASS_HQ'),
  multiPassHq('MULTI_PASS_HQ'),
  ;

  final String value;

  const Xavc4kProfileQualityTuningLevel(this.value);

  static Xavc4kProfileQualityTuningLevel fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum Xavc4kProfileQualityTuningLevel'));
}

/// Required when you set Profile to the value XAVC_4K.
class Xavc4kProfileSettings {
  /// Specify the XAVC 4k (Long GOP) Bitrate Class to set the bitrate of your
  /// output. Outputs of the same class have similar image quality over the
  /// operating points that are valid for that class.
  final Xavc4kProfileBitrateClass? bitrateClass;

  /// Specify the codec profile for this output. Choose High, 8-bit, 4:2:0 (HIGH)
  /// or High, 10-bit, 4:2:2 (HIGH_422). These profiles are specified in ITU-T
  /// H.264.
  final Xavc4kProfileCodecProfile? codecProfile;

  /// The best way to set up adaptive quantization is to keep the default value,
  /// Auto, for the setting Adaptive quantization. When you do so, MediaConvert
  /// automatically applies the best types of quantization for your video content.
  /// Include this setting in your JSON job specification only when you choose to
  /// change the default value for Adaptive quantization. Enable this setting to
  /// have the encoder reduce I-frame pop. I-frame pop appears as a visual flicker
  /// that can arise when the encoder saves bits by copying some macroblocks many
  /// times from frame to frame, and then refreshes them at the I-frame. When you
  /// enable this setting, the encoder updates these macroblocks slightly more
  /// often to smooth out the flicker. This setting is disabled by default.
  /// Related setting: In addition to enabling this setting, you must also set
  /// Adaptive quantization to a value other than Off or Auto. Use Adaptive
  /// quantization to adjust the degree of smoothing that Flicker adaptive
  /// quantization provides.
  final XavcFlickerAdaptiveQuantization? flickerAdaptiveQuantization;

  /// Specify whether the encoder uses B-frames as reference frames for other
  /// pictures in the same GOP. Choose Allow to allow the encoder to use B-frames
  /// as reference frames. Choose Don't allow to prevent the encoder from using
  /// B-frames as reference frames.
  final XavcGopBReference? gopBReference;

  /// Frequency of closed GOPs. In streaming applications, it is recommended that
  /// this be set to 1 so a decoder joining mid-stream will receive an IDR frame
  /// as quickly as possible. Setting this value to 0 will break output
  /// segmenting.
  final int? gopClosedCadence;

  /// Specify the size of the buffer that MediaConvert uses in the HRD buffer
  /// model for this output. Specify this value in bits; for example, enter five
  /// megabits as 5000000. When you don't set this value, or you set it to zero,
  /// MediaConvert calculates the default by doubling the bitrate of this output
  /// point.
  final int? hrdBufferSize;

  /// Optional. Use Quality tuning level to choose how you want to trade off
  /// encoding speed for output video quality. The default behavior is faster,
  /// lower quality, single-pass encoding.
  final Xavc4kProfileQualityTuningLevel? qualityTuningLevel;

  /// Number of slices per picture. Must be less than or equal to the number of
  /// macroblock rows for progressive pictures, and less than or equal to half the
  /// number of macroblock rows for interlaced pictures.
  final int? slices;

  Xavc4kProfileSettings({
    this.bitrateClass,
    this.codecProfile,
    this.flickerAdaptiveQuantization,
    this.gopBReference,
    this.gopClosedCadence,
    this.hrdBufferSize,
    this.qualityTuningLevel,
    this.slices,
  });

  factory Xavc4kProfileSettings.fromJson(Map<String, dynamic> json) {
    return Xavc4kProfileSettings(
      bitrateClass: (json['bitrateClass'] as String?)
          ?.let(Xavc4kProfileBitrateClass.fromString),
      codecProfile: (json['codecProfile'] as String?)
          ?.let(Xavc4kProfileCodecProfile.fromString),
      flickerAdaptiveQuantization:
          (json['flickerAdaptiveQuantization'] as String?)
              ?.let(XavcFlickerAdaptiveQuantization.fromString),
      gopBReference:
          (json['gopBReference'] as String?)?.let(XavcGopBReference.fromString),
      gopClosedCadence: json['gopClosedCadence'] as int?,
      hrdBufferSize: json['hrdBufferSize'] as int?,
      qualityTuningLevel: (json['qualityTuningLevel'] as String?)
          ?.let(Xavc4kProfileQualityTuningLevel.fromString),
      slices: json['slices'] as int?,
    );
  }

  Map<String, dynamic> toJson() {
    final bitrateClass = this.bitrateClass;
    final codecProfile = this.codecProfile;
    final flickerAdaptiveQuantization = this.flickerAdaptiveQuantization;
    final gopBReference = this.gopBReference;
    final gopClosedCadence = this.gopClosedCadence;
    final hrdBufferSize = this.hrdBufferSize;
    final qualityTuningLevel = this.qualityTuningLevel;
    final slices = this.slices;
    return {
      if (bitrateClass != null) 'bitrateClass': bitrateClass.value,
      if (codecProfile != null) 'codecProfile': codecProfile.value,
      if (flickerAdaptiveQuantization != null)
        'flickerAdaptiveQuantization': flickerAdaptiveQuantization.value,
      if (gopBReference != null) 'gopBReference': gopBReference.value,
      if (gopClosedCadence != null) 'gopClosedCadence': gopClosedCadence,
      if (hrdBufferSize != null) 'hrdBufferSize': hrdBufferSize,
      if (qualityTuningLevel != null)
        'qualityTuningLevel': qualityTuningLevel.value,
      if (slices != null) 'slices': slices,
    };
  }
}

/// Keep the default value, Auto, for this setting to have MediaConvert
/// automatically apply the best types of quantization for your video content.
/// When you want to apply your quantization settings manually, you must set
/// Adaptive quantization to a value other than Auto. Use this setting to
/// specify the strength of any adaptive quantization filters that you enable.
/// If you don't want MediaConvert to do any adaptive quantization in this
/// transcode, set Adaptive quantization to Off. Related settings: The value
/// that you choose here applies to the following settings: Flicker adaptive
/// quantization (flickerAdaptiveQuantization), Spatial adaptive quantization,
/// and Temporal adaptive quantization.
enum XavcAdaptiveQuantization {
  off('OFF'),
  auto('AUTO'),
  low('LOW'),
  medium('MEDIUM'),
  high('HIGH'),
  higher('HIGHER'),
  max('MAX'),
  ;

  final String value;

  const XavcAdaptiveQuantization(this.value);

  static XavcAdaptiveQuantization fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum XavcAdaptiveQuantization'));
}

/// Optional. Choose a specific entropy encoding mode only when you want to
/// override XAVC recommendations. If you choose the value auto, MediaConvert
/// uses the mode that the XAVC file format specifies given this output's
/// operating point.
enum XavcEntropyEncoding {
  auto('AUTO'),
  cabac('CABAC'),
  cavlc('CAVLC'),
  ;

  final String value;

  const XavcEntropyEncoding(this.value);

  static XavcEntropyEncoding fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum XavcEntropyEncoding'));
}

/// The best way to set up adaptive quantization is to keep the default value,
/// Auto, for the setting Adaptive quantization. When you do so, MediaConvert
/// automatically applies the best types of quantization for your video content.
/// Include this setting in your JSON job specification only when you choose to
/// change the default value for Adaptive quantization. Enable this setting to
/// have the encoder reduce I-frame pop. I-frame pop appears as a visual flicker
/// that can arise when the encoder saves bits by copying some macroblocks many
/// times from frame to frame, and then refreshes them at the I-frame. When you
/// enable this setting, the encoder updates these macroblocks slightly more
/// often to smooth out the flicker. This setting is disabled by default.
/// Related setting: In addition to enabling this setting, you must also set
/// Adaptive quantization to a value other than Off or Auto. Use Adaptive
/// quantization to adjust the degree of smoothing that Flicker adaptive
/// quantization provides.
enum XavcFlickerAdaptiveQuantization {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const XavcFlickerAdaptiveQuantization(this.value);

  static XavcFlickerAdaptiveQuantization fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum XavcFlickerAdaptiveQuantization'));
}

/// If you are using the console, use the Frame rate setting to specify the
/// frame rate for this output. If you want to keep the same frame rate as the
/// input video, choose Follow source. If you want to do frame rate conversion,
/// choose a frame rate from the dropdown list. The framerates shown in the
/// dropdown list are decimal approximations of fractions.
enum XavcFramerateControl {
  initializeFromSource('INITIALIZE_FROM_SOURCE'),
  specified('SPECIFIED'),
  ;

  final String value;

  const XavcFramerateControl(this.value);

  static XavcFramerateControl fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum XavcFramerateControl'));
}

/// Choose the method that you want MediaConvert to use when increasing or
/// decreasing the frame rate. For numerically simple conversions, such as 60
/// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
/// For numerically complex conversions, to avoid stutter: Choose Interpolate.
/// This results in a smooth picture, but might introduce undesirable video
/// artifacts. For complex frame rate conversions, especially if your source
/// video has already been converted from its original cadence: Choose
/// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
/// best conversion method frame by frame. Note that using FrameFormer increases
/// the transcoding time and incurs a significant add-on cost. When you choose
/// FrameFormer, your input video resolution must be at least 128x96.
enum XavcFramerateConversionAlgorithm {
  duplicateDrop('DUPLICATE_DROP'),
  interpolate('INTERPOLATE'),
  frameformer('FRAMEFORMER'),
  ;

  final String value;

  const XavcFramerateConversionAlgorithm(this.value);

  static XavcFramerateConversionAlgorithm fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum XavcFramerateConversionAlgorithm'));
}

/// Specify whether the encoder uses B-frames as reference frames for other
/// pictures in the same GOP. Choose Allow to allow the encoder to use B-frames
/// as reference frames. Choose Don't allow to prevent the encoder from using
/// B-frames as reference frames.
enum XavcGopBReference {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const XavcGopBReference(this.value);

  static XavcGopBReference fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum XavcGopBReference'));
}

/// Specify the XAVC Intra HD (CBG) Class to set the bitrate of your output.
/// Outputs of the same class have similar image quality over the operating
/// points that are valid for that class.
enum XavcHdIntraCbgProfileClass {
  class_50('CLASS_50'),
  class_100('CLASS_100'),
  class_200('CLASS_200'),
  ;

  final String value;

  const XavcHdIntraCbgProfileClass(this.value);

  static XavcHdIntraCbgProfileClass fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum XavcHdIntraCbgProfileClass'));
}

/// Required when you set Profile to the value XAVC_HD_INTRA_CBG.
class XavcHdIntraCbgProfileSettings {
  /// Specify the XAVC Intra HD (CBG) Class to set the bitrate of your output.
  /// Outputs of the same class have similar image quality over the operating
  /// points that are valid for that class.
  final XavcHdIntraCbgProfileClass? xavcClass;

  XavcHdIntraCbgProfileSettings({
    this.xavcClass,
  });

  factory XavcHdIntraCbgProfileSettings.fromJson(Map<String, dynamic> json) {
    return XavcHdIntraCbgProfileSettings(
      xavcClass: (json['xavcClass'] as String?)
          ?.let(XavcHdIntraCbgProfileClass.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final xavcClass = this.xavcClass;
    return {
      if (xavcClass != null) 'xavcClass': xavcClass.value,
    };
  }
}

/// Specify the XAVC HD (Long GOP) Bitrate Class to set the bitrate of your
/// output. Outputs of the same class have similar image quality over the
/// operating points that are valid for that class.
enum XavcHdProfileBitrateClass {
  bitrateClass_25('BITRATE_CLASS_25'),
  bitrateClass_35('BITRATE_CLASS_35'),
  bitrateClass_50('BITRATE_CLASS_50'),
  ;

  final String value;

  const XavcHdProfileBitrateClass(this.value);

  static XavcHdProfileBitrateClass fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum XavcHdProfileBitrateClass'));
}

/// Optional. Use Quality tuning level to choose how you want to trade off
/// encoding speed for output video quality. The default behavior is faster,
/// lower quality, single-pass encoding.
enum XavcHdProfileQualityTuningLevel {
  singlePass('SINGLE_PASS'),
  singlePassHq('SINGLE_PASS_HQ'),
  multiPassHq('MULTI_PASS_HQ'),
  ;

  final String value;

  const XavcHdProfileQualityTuningLevel(this.value);

  static XavcHdProfileQualityTuningLevel fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum XavcHdProfileQualityTuningLevel'));
}

/// Required when you set Profile to the value XAVC_HD.
class XavcHdProfileSettings {
  /// Specify the XAVC HD (Long GOP) Bitrate Class to set the bitrate of your
  /// output. Outputs of the same class have similar image quality over the
  /// operating points that are valid for that class.
  final XavcHdProfileBitrateClass? bitrateClass;

  /// The best way to set up adaptive quantization is to keep the default value,
  /// Auto, for the setting Adaptive quantization. When you do so, MediaConvert
  /// automatically applies the best types of quantization for your video content.
  /// Include this setting in your JSON job specification only when you choose to
  /// change the default value for Adaptive quantization. Enable this setting to
  /// have the encoder reduce I-frame pop. I-frame pop appears as a visual flicker
  /// that can arise when the encoder saves bits by copying some macroblocks many
  /// times from frame to frame, and then refreshes them at the I-frame. When you
  /// enable this setting, the encoder updates these macroblocks slightly more
  /// often to smooth out the flicker. This setting is disabled by default.
  /// Related setting: In addition to enabling this setting, you must also set
  /// Adaptive quantization to a value other than Off or Auto. Use Adaptive
  /// quantization to adjust the degree of smoothing that Flicker adaptive
  /// quantization provides.
  final XavcFlickerAdaptiveQuantization? flickerAdaptiveQuantization;

  /// Specify whether the encoder uses B-frames as reference frames for other
  /// pictures in the same GOP. Choose Allow to allow the encoder to use B-frames
  /// as reference frames. Choose Don't allow to prevent the encoder from using
  /// B-frames as reference frames.
  final XavcGopBReference? gopBReference;

  /// Frequency of closed GOPs. In streaming applications, it is recommended that
  /// this be set to 1 so a decoder joining mid-stream will receive an IDR frame
  /// as quickly as possible. Setting this value to 0 will break output
  /// segmenting.
  final int? gopClosedCadence;

  /// Specify the size of the buffer that MediaConvert uses in the HRD buffer
  /// model for this output. Specify this value in bits; for example, enter five
  /// megabits as 5000000. When you don't set this value, or you set it to zero,
  /// MediaConvert calculates the default by doubling the bitrate of this output
  /// point.
  final int? hrdBufferSize;

  /// Choose the scan line type for the output. Keep the default value,
  /// Progressive to create a progressive output, regardless of the scan type of
  /// your input. Use Top field first or Bottom field first to create an output
  /// that's interlaced with the same field polarity throughout. Use Follow,
  /// default top or Follow, default bottom to produce outputs with the same field
  /// polarity as the source. For jobs that have multiple inputs, the output field
  /// polarity might change over the course of the output. Follow behavior depends
  /// on the input scan type. If the source is interlaced, the output will be
  /// interlaced with the same polarity as the source. If the source is
  /// progressive, the output will be interlaced with top field bottom field
  /// first, depending on which of the Follow options you choose.
  final XavcInterlaceMode? interlaceMode;

  /// Optional. Use Quality tuning level to choose how you want to trade off
  /// encoding speed for output video quality. The default behavior is faster,
  /// lower quality, single-pass encoding.
  final XavcHdProfileQualityTuningLevel? qualityTuningLevel;

  /// Number of slices per picture. Must be less than or equal to the number of
  /// macroblock rows for progressive pictures, and less than or equal to half the
  /// number of macroblock rows for interlaced pictures.
  final int? slices;

  /// Ignore this setting unless you set Frame rate (framerateNumerator divided by
  /// framerateDenominator) to 29.970. If your input framerate is 23.976, choose
  /// Hard. Otherwise, keep the default value None. For more information, see
  /// https://docs.aws.amazon.com/mediaconvert/latest/ug/working-with-telecine-and-inverse-telecine.html.
  final XavcHdProfileTelecine? telecine;

  XavcHdProfileSettings({
    this.bitrateClass,
    this.flickerAdaptiveQuantization,
    this.gopBReference,
    this.gopClosedCadence,
    this.hrdBufferSize,
    this.interlaceMode,
    this.qualityTuningLevel,
    this.slices,
    this.telecine,
  });

  factory XavcHdProfileSettings.fromJson(Map<String, dynamic> json) {
    return XavcHdProfileSettings(
      bitrateClass: (json['bitrateClass'] as String?)
          ?.let(XavcHdProfileBitrateClass.fromString),
      flickerAdaptiveQuantization:
          (json['flickerAdaptiveQuantization'] as String?)
              ?.let(XavcFlickerAdaptiveQuantization.fromString),
      gopBReference:
          (json['gopBReference'] as String?)?.let(XavcGopBReference.fromString),
      gopClosedCadence: json['gopClosedCadence'] as int?,
      hrdBufferSize: json['hrdBufferSize'] as int?,
      interlaceMode:
          (json['interlaceMode'] as String?)?.let(XavcInterlaceMode.fromString),
      qualityTuningLevel: (json['qualityTuningLevel'] as String?)
          ?.let(XavcHdProfileQualityTuningLevel.fromString),
      slices: json['slices'] as int?,
      telecine:
          (json['telecine'] as String?)?.let(XavcHdProfileTelecine.fromString),
    );
  }

  Map<String, dynamic> toJson() {
    final bitrateClass = this.bitrateClass;
    final flickerAdaptiveQuantization = this.flickerAdaptiveQuantization;
    final gopBReference = this.gopBReference;
    final gopClosedCadence = this.gopClosedCadence;
    final hrdBufferSize = this.hrdBufferSize;
    final interlaceMode = this.interlaceMode;
    final qualityTuningLevel = this.qualityTuningLevel;
    final slices = this.slices;
    final telecine = this.telecine;
    return {
      if (bitrateClass != null) 'bitrateClass': bitrateClass.value,
      if (flickerAdaptiveQuantization != null)
        'flickerAdaptiveQuantization': flickerAdaptiveQuantization.value,
      if (gopBReference != null) 'gopBReference': gopBReference.value,
      if (gopClosedCadence != null) 'gopClosedCadence': gopClosedCadence,
      if (hrdBufferSize != null) 'hrdBufferSize': hrdBufferSize,
      if (interlaceMode != null) 'interlaceMode': interlaceMode.value,
      if (qualityTuningLevel != null)
        'qualityTuningLevel': qualityTuningLevel.value,
      if (slices != null) 'slices': slices,
      if (telecine != null) 'telecine': telecine.value,
    };
  }
}

/// Ignore this setting unless you set Frame rate (framerateNumerator divided by
/// framerateDenominator) to 29.970. If your input framerate is 23.976, choose
/// Hard. Otherwise, keep the default value None. For more information, see
/// https://docs.aws.amazon.com/mediaconvert/latest/ug/working-with-telecine-and-inverse-telecine.html.
enum XavcHdProfileTelecine {
  none('NONE'),
  hard('HARD'),
  ;

  final String value;

  const XavcHdProfileTelecine(this.value);

  static XavcHdProfileTelecine fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () =>
          throw Exception('$value is not known in enum XavcHdProfileTelecine'));
}

/// Choose the scan line type for the output. Keep the default value,
/// Progressive to create a progressive output, regardless of the scan type of
/// your input. Use Top field first or Bottom field first to create an output
/// that's interlaced with the same field polarity throughout. Use Follow,
/// default top or Follow, default bottom to produce outputs with the same field
/// polarity as the source. For jobs that have multiple inputs, the output field
/// polarity might change over the course of the output. Follow behavior depends
/// on the input scan type. If the source is interlaced, the output will be
/// interlaced with the same polarity as the source. If the source is
/// progressive, the output will be interlaced with top field bottom field
/// first, depending on which of the Follow options you choose.
enum XavcInterlaceMode {
  progressive('PROGRESSIVE'),
  topField('TOP_FIELD'),
  bottomField('BOTTOM_FIELD'),
  followTopField('FOLLOW_TOP_FIELD'),
  followBottomField('FOLLOW_BOTTOM_FIELD'),
  ;

  final String value;

  const XavcInterlaceMode(this.value);

  static XavcInterlaceMode fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () =>
              throw Exception('$value is not known in enum XavcInterlaceMode'));
}

/// Specify the XAVC profile for this output. For more information, see the Sony
/// documentation at https://www.xavc-info.org/. Note that MediaConvert doesn't
/// support the interlaced video XAVC operating points for XAVC_HD_INTRA_CBG. To
/// create an interlaced XAVC output, choose the profile XAVC_HD.
enum XavcProfile {
  xavcHdIntraCbg('XAVC_HD_INTRA_CBG'),
  xavc_4kIntraCbg('XAVC_4K_INTRA_CBG'),
  xavc_4kIntraVbr('XAVC_4K_INTRA_VBR'),
  xavcHd('XAVC_HD'),
  xavc_4k('XAVC_4K'),
  ;

  final String value;

  const XavcProfile(this.value);

  static XavcProfile fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum XavcProfile'));
}

/// Required when you set Codec to the value XAVC.
class XavcSettings {
  /// Keep the default value, Auto, for this setting to have MediaConvert
  /// automatically apply the best types of quantization for your video content.
  /// When you want to apply your quantization settings manually, you must set
  /// Adaptive quantization to a value other than Auto. Use this setting to
  /// specify the strength of any adaptive quantization filters that you enable.
  /// If you don't want MediaConvert to do any adaptive quantization in this
  /// transcode, set Adaptive quantization to Off. Related settings: The value
  /// that you choose here applies to the following settings: Flicker adaptive
  /// quantization (flickerAdaptiveQuantization), Spatial adaptive quantization,
  /// and Temporal adaptive quantization.
  final XavcAdaptiveQuantization? adaptiveQuantization;

  /// Optional. Choose a specific entropy encoding mode only when you want to
  /// override XAVC recommendations. If you choose the value auto, MediaConvert
  /// uses the mode that the XAVC file format specifies given this output's
  /// operating point.
  final XavcEntropyEncoding? entropyEncoding;

  /// If you are using the console, use the Frame rate setting to specify the
  /// frame rate for this output. If you want to keep the same frame rate as the
  /// input video, choose Follow source. If you want to do frame rate conversion,
  /// choose a frame rate from the dropdown list. The framerates shown in the
  /// dropdown list are decimal approximations of fractions.
  final XavcFramerateControl? framerateControl;

  /// Choose the method that you want MediaConvert to use when increasing or
  /// decreasing the frame rate. For numerically simple conversions, such as 60
  /// fps to 30 fps: We recommend that you keep the default value, Drop duplicate.
  /// For numerically complex conversions, to avoid stutter: Choose Interpolate.
  /// This results in a smooth picture, but might introduce undesirable video
  /// artifacts. For complex frame rate conversions, especially if your source
  /// video has already been converted from its original cadence: Choose
  /// FrameFormer to do motion-compensated interpolation. FrameFormer uses the
  /// best conversion method frame by frame. Note that using FrameFormer increases
  /// the transcoding time and incurs a significant add-on cost. When you choose
  /// FrameFormer, your input video resolution must be at least 128x96.
  final XavcFramerateConversionAlgorithm? framerateConversionAlgorithm;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateDenominator to specify the denominator of this fraction.
  /// In this example, use 1001 for the value of FramerateDenominator. When you
  /// use the console for transcode jobs that use frame rate conversion, provide
  /// the value as a decimal number for Frame rate. In this example, specify
  /// 23.976.
  final int? framerateDenominator;

  /// When you use the API for transcode jobs that use frame rate conversion,
  /// specify the frame rate as a fraction. For example, 24000 / 1001 = 23.976
  /// fps. Use FramerateNumerator to specify the numerator of this fraction. In
  /// this example, use 24000 for the value of FramerateNumerator. When you use
  /// the console for transcode jobs that use frame rate conversion, provide the
  /// value as a decimal number for Framerate. In this example, specify 23.976.
  final int? framerateNumerator;

  /// Specify the XAVC profile for this output. For more information, see the Sony
  /// documentation at https://www.xavc-info.org/. Note that MediaConvert doesn't
  /// support the interlaced video XAVC operating points for XAVC_HD_INTRA_CBG. To
  /// create an interlaced XAVC output, choose the profile XAVC_HD.
  final XavcProfile? profile;

  /// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
  /// second (fps). Enable slow PAL to create a 25 fps output by relabeling the
  /// video frames and resampling your audio. Note that enabling this setting will
  /// slightly reduce the duration of your video. Related settings: You must also
  /// set Frame rate to 25.
  final XavcSlowPal? slowPal;

  /// Ignore this setting unless your downstream workflow requires that you
  /// specify it explicitly. Otherwise, we recommend that you adjust the softness
  /// of your output by using a lower value for the setting Sharpness or by
  /// enabling a noise reducer filter. The Softness setting specifies the
  /// quantization matrices that the encoder uses. Keep the default value, 0, for
  /// flat quantization. Choose the value 1 or 16 to use the default JVT softening
  /// quantization matricies from the H.264 specification. Choose a value from 17
  /// to 128 to use planar interpolation. Increasing values from 17 to 128 result
  /// in increasing reduction of high-frequency data. The value 128 results in the
  /// softest video.
  final int? softness;

  /// The best way to set up adaptive quantization is to keep the default value,
  /// Auto, for the setting Adaptive quantization. When you do so, MediaConvert
  /// automatically applies the best types of quantization for your video content.
  /// Include this setting in your JSON job specification only when you choose to
  /// change the default value for Adaptive quantization. For this setting, keep
  /// the default value, Enabled, to adjust quantization within each frame based
  /// on spatial variation of content complexity. When you enable this feature,
  /// the encoder uses fewer bits on areas that can sustain more distortion with
  /// no noticeable visual degradation and uses more bits on areas where any small
  /// distortion will be noticeable. For example, complex textured blocks are
  /// encoded with fewer bits and smooth textured blocks are encoded with more
  /// bits. Enabling this feature will almost always improve your video quality.
  /// Note, though, that this feature doesn't take into account where the viewer's
  /// attention is likely to be. If viewers are likely to be focusing their
  /// attention on a part of the screen with a lot of complex texture, you might
  /// choose to disable this feature. Related setting: When you enable spatial
  /// adaptive quantization, set the value for Adaptive quantization depending on
  /// your content. For homogeneous content, such as cartoons and video games, set
  /// it to Low. For content with a wider variety of textures, set it to High or
  /// Higher.
  final XavcSpatialAdaptiveQuantization? spatialAdaptiveQuantization;

  /// The best way to set up adaptive quantization is to keep the default value,
  /// Auto, for the setting Adaptive quantization. When you do so, MediaConvert
  /// automatically applies the best types of quantization for your video content.
  /// Include this setting in your JSON job specification only when you choose to
  /// change the default value for Adaptive quantization. For this setting, keep
  /// the default value, Enabled, to adjust quantization within each frame based
  /// on temporal variation of content complexity. When you enable this feature,
  /// the encoder uses fewer bits on areas of the frame that aren't moving and
  /// uses more bits on complex objects with sharp edges that move a lot. For
  /// example, this feature improves the readability of text tickers on newscasts
  /// and scoreboards on sports matches. Enabling this feature will almost always
  /// improve your video quality. Note, though, that this feature doesn't take
  /// into account where the viewer's attention is likely to be. If viewers are
  /// likely to be focusing their attention on a part of the screen that doesn't
  /// have moving objects with sharp edges, such as sports athletes' faces, you
  /// might choose to disable this feature. Related setting: When you enable
  /// temporal adaptive quantization, adjust the strength of the filter with the
  /// setting Adaptive quantization.
  final XavcTemporalAdaptiveQuantization? temporalAdaptiveQuantization;

  /// Required when you set Profile to the value XAVC_4K_INTRA_CBG.
  final Xavc4kIntraCbgProfileSettings? xavc4kIntraCbgProfileSettings;

  /// Required when you set Profile to the value XAVC_4K_INTRA_VBR.
  final Xavc4kIntraVbrProfileSettings? xavc4kIntraVbrProfileSettings;

  /// Required when you set Profile to the value XAVC_4K.
  final Xavc4kProfileSettings? xavc4kProfileSettings;

  /// Required when you set Profile to the value XAVC_HD_INTRA_CBG.
  final XavcHdIntraCbgProfileSettings? xavcHdIntraCbgProfileSettings;

  /// Required when you set Profile to the value XAVC_HD.
  final XavcHdProfileSettings? xavcHdProfileSettings;

  XavcSettings({
    this.adaptiveQuantization,
    this.entropyEncoding,
    this.framerateControl,
    this.framerateConversionAlgorithm,
    this.framerateDenominator,
    this.framerateNumerator,
    this.profile,
    this.slowPal,
    this.softness,
    this.spatialAdaptiveQuantization,
    this.temporalAdaptiveQuantization,
    this.xavc4kIntraCbgProfileSettings,
    this.xavc4kIntraVbrProfileSettings,
    this.xavc4kProfileSettings,
    this.xavcHdIntraCbgProfileSettings,
    this.xavcHdProfileSettings,
  });

  factory XavcSettings.fromJson(Map<String, dynamic> json) {
    return XavcSettings(
      adaptiveQuantization: (json['adaptiveQuantization'] as String?)
          ?.let(XavcAdaptiveQuantization.fromString),
      entropyEncoding: (json['entropyEncoding'] as String?)
          ?.let(XavcEntropyEncoding.fromString),
      framerateControl: (json['framerateControl'] as String?)
          ?.let(XavcFramerateControl.fromString),
      framerateConversionAlgorithm:
          (json['framerateConversionAlgorithm'] as String?)
              ?.let(XavcFramerateConversionAlgorithm.fromString),
      framerateDenominator: json['framerateDenominator'] as int?,
      framerateNumerator: json['framerateNumerator'] as int?,
      profile: (json['profile'] as String?)?.let(XavcProfile.fromString),
      slowPal: (json['slowPal'] as String?)?.let(XavcSlowPal.fromString),
      softness: json['softness'] as int?,
      spatialAdaptiveQuantization:
          (json['spatialAdaptiveQuantization'] as String?)
              ?.let(XavcSpatialAdaptiveQuantization.fromString),
      temporalAdaptiveQuantization:
          (json['temporalAdaptiveQuantization'] as String?)
              ?.let(XavcTemporalAdaptiveQuantization.fromString),
      xavc4kIntraCbgProfileSettings:
          json['xavc4kIntraCbgProfileSettings'] != null
              ? Xavc4kIntraCbgProfileSettings.fromJson(
                  json['xavc4kIntraCbgProfileSettings'] as Map<String, dynamic>)
              : null,
      xavc4kIntraVbrProfileSettings:
          json['xavc4kIntraVbrProfileSettings'] != null
              ? Xavc4kIntraVbrProfileSettings.fromJson(
                  json['xavc4kIntraVbrProfileSettings'] as Map<String, dynamic>)
              : null,
      xavc4kProfileSettings: json['xavc4kProfileSettings'] != null
          ? Xavc4kProfileSettings.fromJson(
              json['xavc4kProfileSettings'] as Map<String, dynamic>)
          : null,
      xavcHdIntraCbgProfileSettings:
          json['xavcHdIntraCbgProfileSettings'] != null
              ? XavcHdIntraCbgProfileSettings.fromJson(
                  json['xavcHdIntraCbgProfileSettings'] as Map<String, dynamic>)
              : null,
      xavcHdProfileSettings: json['xavcHdProfileSettings'] != null
          ? XavcHdProfileSettings.fromJson(
              json['xavcHdProfileSettings'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() {
    final adaptiveQuantization = this.adaptiveQuantization;
    final entropyEncoding = this.entropyEncoding;
    final framerateControl = this.framerateControl;
    final framerateConversionAlgorithm = this.framerateConversionAlgorithm;
    final framerateDenominator = this.framerateDenominator;
    final framerateNumerator = this.framerateNumerator;
    final profile = this.profile;
    final slowPal = this.slowPal;
    final softness = this.softness;
    final spatialAdaptiveQuantization = this.spatialAdaptiveQuantization;
    final temporalAdaptiveQuantization = this.temporalAdaptiveQuantization;
    final xavc4kIntraCbgProfileSettings = this.xavc4kIntraCbgProfileSettings;
    final xavc4kIntraVbrProfileSettings = this.xavc4kIntraVbrProfileSettings;
    final xavc4kProfileSettings = this.xavc4kProfileSettings;
    final xavcHdIntraCbgProfileSettings = this.xavcHdIntraCbgProfileSettings;
    final xavcHdProfileSettings = this.xavcHdProfileSettings;
    return {
      if (adaptiveQuantization != null)
        'adaptiveQuantization': adaptiveQuantization.value,
      if (entropyEncoding != null) 'entropyEncoding': entropyEncoding.value,
      if (framerateControl != null) 'framerateControl': framerateControl.value,
      if (framerateConversionAlgorithm != null)
        'framerateConversionAlgorithm': framerateConversionAlgorithm.value,
      if (framerateDenominator != null)
        'framerateDenominator': framerateDenominator,
      if (framerateNumerator != null) 'framerateNumerator': framerateNumerator,
      if (profile != null) 'profile': profile.value,
      if (slowPal != null) 'slowPal': slowPal.value,
      if (softness != null) 'softness': softness,
      if (spatialAdaptiveQuantization != null)
        'spatialAdaptiveQuantization': spatialAdaptiveQuantization.value,
      if (temporalAdaptiveQuantization != null)
        'temporalAdaptiveQuantization': temporalAdaptiveQuantization.value,
      if (xavc4kIntraCbgProfileSettings != null)
        'xavc4kIntraCbgProfileSettings': xavc4kIntraCbgProfileSettings,
      if (xavc4kIntraVbrProfileSettings != null)
        'xavc4kIntraVbrProfileSettings': xavc4kIntraVbrProfileSettings,
      if (xavc4kProfileSettings != null)
        'xavc4kProfileSettings': xavc4kProfileSettings,
      if (xavcHdIntraCbgProfileSettings != null)
        'xavcHdIntraCbgProfileSettings': xavcHdIntraCbgProfileSettings,
      if (xavcHdProfileSettings != null)
        'xavcHdProfileSettings': xavcHdProfileSettings,
    };
  }
}

/// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
/// second (fps). Enable slow PAL to create a 25 fps output by relabeling the
/// video frames and resampling your audio. Note that enabling this setting will
/// slightly reduce the duration of your video. Related settings: You must also
/// set Frame rate to 25.
enum XavcSlowPal {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const XavcSlowPal(this.value);

  static XavcSlowPal fromString(String value) => values.firstWhere(
      (e) => e.value == value,
      orElse: () => throw Exception('$value is not known in enum XavcSlowPal'));
}

/// The best way to set up adaptive quantization is to keep the default value,
/// Auto, for the setting Adaptive quantization. When you do so, MediaConvert
/// automatically applies the best types of quantization for your video content.
/// Include this setting in your JSON job specification only when you choose to
/// change the default value for Adaptive quantization. For this setting, keep
/// the default value, Enabled, to adjust quantization within each frame based
/// on spatial variation of content complexity. When you enable this feature,
/// the encoder uses fewer bits on areas that can sustain more distortion with
/// no noticeable visual degradation and uses more bits on areas where any small
/// distortion will be noticeable. For example, complex textured blocks are
/// encoded with fewer bits and smooth textured blocks are encoded with more
/// bits. Enabling this feature will almost always improve your video quality.
/// Note, though, that this feature doesn't take into account where the viewer's
/// attention is likely to be. If viewers are likely to be focusing their
/// attention on a part of the screen with a lot of complex texture, you might
/// choose to disable this feature. Related setting: When you enable spatial
/// adaptive quantization, set the value for Adaptive quantization depending on
/// your content. For homogeneous content, such as cartoons and video games, set
/// it to Low. For content with a wider variety of textures, set it to High or
/// Higher.
enum XavcSpatialAdaptiveQuantization {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const XavcSpatialAdaptiveQuantization(this.value);

  static XavcSpatialAdaptiveQuantization fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum XavcSpatialAdaptiveQuantization'));
}

/// The best way to set up adaptive quantization is to keep the default value,
/// Auto, for the setting Adaptive quantization. When you do so, MediaConvert
/// automatically applies the best types of quantization for your video content.
/// Include this setting in your JSON job specification only when you choose to
/// change the default value for Adaptive quantization. For this setting, keep
/// the default value, Enabled, to adjust quantization within each frame based
/// on temporal variation of content complexity. When you enable this feature,
/// the encoder uses fewer bits on areas of the frame that aren't moving and
/// uses more bits on complex objects with sharp edges that move a lot. For
/// example, this feature improves the readability of text tickers on newscasts
/// and scoreboards on sports matches. Enabling this feature will almost always
/// improve your video quality. Note, though, that this feature doesn't take
/// into account where the viewer's attention is likely to be. If viewers are
/// likely to be focusing their attention on a part of the screen that doesn't
/// have moving objects with sharp edges, such as sports athletes' faces, you
/// might choose to disable this feature. Related setting: When you enable
/// temporal adaptive quantization, adjust the strength of the filter with the
/// setting Adaptive quantization.
enum XavcTemporalAdaptiveQuantization {
  disabled('DISABLED'),
  enabled('ENABLED'),
  ;

  final String value;

  const XavcTemporalAdaptiveQuantization(this.value);

  static XavcTemporalAdaptiveQuantization fromString(String value) =>
      values.firstWhere((e) => e.value == value,
          orElse: () => throw Exception(
              '$value is not known in enum XavcTemporalAdaptiveQuantization'));
}

class BadRequestException extends _s.GenericAwsException {
  BadRequestException({String? type, String? message})
      : super(type: type, code: 'BadRequestException', message: message);
}

class ConflictException extends _s.GenericAwsException {
  ConflictException({String? type, String? message})
      : super(type: type, code: 'ConflictException', message: message);
}

class ForbiddenException extends _s.GenericAwsException {
  ForbiddenException({String? type, String? message})
      : super(type: type, code: 'ForbiddenException', message: message);
}

class InternalServerErrorException extends _s.GenericAwsException {
  InternalServerErrorException({String? type, String? message})
      : super(
            type: type, code: 'InternalServerErrorException', message: message);
}

class NotFoundException extends _s.GenericAwsException {
  NotFoundException({String? type, String? message})
      : super(type: type, code: 'NotFoundException', message: message);
}

class TooManyRequestsException extends _s.GenericAwsException {
  TooManyRequestsException({String? type, String? message})
      : super(type: type, code: 'TooManyRequestsException', message: message);
}

final _exceptionFns = <String, _s.AwsExceptionFn>{
  'BadRequestException': (type, message) =>
      BadRequestException(type: type, message: message),
  'ConflictException': (type, message) =>
      ConflictException(type: type, message: message),
  'ForbiddenException': (type, message) =>
      ForbiddenException(type: type, message: message),
  'InternalServerErrorException': (type, message) =>
      InternalServerErrorException(type: type, message: message),
  'NotFoundException': (type, message) =>
      NotFoundException(type: type, message: message),
  'TooManyRequestsException': (type, message) =>
      TooManyRequestsException(type: type, message: message),
};
